{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Simple and Multiple Linear Regression from Scratch\n",
    "\n",
    "_Authors: Kiefer Katovich (SF) and Matt Brems (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Code a simple linear regression from scratch using a simple housing price data set.\n",
    "- Understand and code the loss function (mean squared error) MSE in regression.\n",
    "- Write functions to calculate the R^2 metric.\n",
    "- Understand what R^2 represents.\n",
    "- Plot the regression line and predictions against the true values.\n",
    "- Understand the difference between multiple linear regression (MLR) and simple linear regression.\n",
    "- Derive the beta coefficients in MLR using linear algebra.\n",
    "- Construct an MLR, calculate the coefficients manually, and evaluate the R^2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Load the Real Estate Data](#load-data)\n",
    "- [Build a Simple Linear Regression (SLR)](#build-slr)\n",
    "    - [Define the Target and Predictor Variables](#target-predictor)\n",
    "    - [Code Prediction Function](#pred-func)\n",
    "    - [Code Regression Plotting Function](#plot-regline)\n",
    "    - [Code Function to Calculate Residuals](#calc-resids)\n",
    "    - [Code Function to Calculate SSE](#calc-sse)\n",
    "    - [Minimizing the SSE](#minimize-sse)    \n",
    "- [R2: \"The Coefficient of Determination\"](#r2)\n",
    "- [From SLR to Multiple Linear Regression (MLR)](#slr-to-mlr)\n",
    "- [Assumptions of MLR](#assumptions)\n",
    "- [Fitting an MLR](#fit-mlr)\n",
    "    - [Deriving the MLR Coefficients with Linear Algebra](#mlr-beta-derivation)\n",
    "    - [Code the MLR](#code-mlr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load-data'></a>\n",
    "\n",
    "## Load the Real Estate Data\n",
    "\n",
    "---\n",
    "\n",
    "Over the course of this lesson we will be constructing a simple linear regression (SLR) and then extending this to a multiple linear regression (MLR). Included in the `datasets` folder is a very simple data set on real estate prices.\n",
    "\n",
    "**Load the data using Pandas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "house_csv = './datasets/housing-data.csv'\n",
    "\n",
    "# Load data with Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are:\n",
    "\n",
    "    sqft: The size of the house in square feet.\n",
    "    bdrms: Number of bedrooms.\n",
    "    age: Age in years of house.\n",
    "    price: The price of the house.\n",
    "    \n",
    "**Convert `price` to units of 1000 (thousands of dollars).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform price to new units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='build-slr'></a>\n",
    "\n",
    "## Build an SLR: Estimating `price` with `sqft`\n",
    "\n",
    "---\n",
    "\n",
    "We will start by constructing the simple linear regression. Below is the formulation for the SLR and our specific model of interest:\n",
    "\n",
    "### $$ y = \\beta_0 + \\beta_1 x + \\epsilon \\\\\n",
    "\\text{price} = \\beta_0 + \\beta_1 \\text{sqft} + \\epsilon$$\n",
    "\n",
    "> $\\beta_0$: The intercept\n",
    "\n",
    "Without the intercept term, the regression line would always have to pass through the origin, which is rarely an optimal way to represent the relationship between our target and predictor variables.\n",
    "\n",
    "> $\\beta_1$: The coefficient on $x$ \n",
    "\n",
    "We intend to estimate the values of $y$ from $x$. Each value of $x$ is multiplied by the same coefficient. This is why linear regression models model a _linear_ relationship between our predictor and target variables.\n",
    "\n",
    "Recall that a 1-unit increase in $x$ will correspond to a $\\beta_1$ unit increase in $y$ according to our model.\n",
    "\n",
    "> $\\epsilon_1$: The error (residuals)\n",
    "\n",
    "This is the difference between the predicted and true values that are unexplained by $x$ in the regression.\n",
    "\n",
    "---\n",
    "\n",
    "<a id='target-predictor'></a>\n",
    "\n",
    "### Define the Target and Predictor Variables\n",
    "\n",
    "Extract the target variable and predictor variable from our Pandas DataFrame. Classically, target and predictor are referred to as dependent and independent variables, respectively. There are many different terms for what $x$ and $y$ represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define predictor and target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pred-func'></a>\n",
    "\n",
    "### Build a Function to Predict $\\hat{y}$ Given $x$\n",
    "\n",
    "Build a function to represent the formula below:\n",
    "\n",
    "### $$\\hat{y} = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "**Note:** We have removed the error term from the equation. Obviously, we do not know the error or we would be able to model $y$ perfectly. We assume that our prediction $\\hat{y}$ is an imperfect estimation of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate y-hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot-regline'></a>\n",
    "\n",
    "### Write a Function to Plot a Regression Line\n",
    "\n",
    "Your function should:\n",
    "- Accept $\\beta_0$, $\\beta_1$, $x$, and $y$ as arguments.\n",
    "- Calculate the predicted values $\\hat{y}$ given $x$ (using the function you wrote above).\n",
    "- Plot the original points.\n",
    "- Plot the predicted points (in a different color).\n",
    "- Plot the regression line defined by the slope and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to plot regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use your function with $\\beta_0 = 0$ and $\\beta_1 = 1$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='calc-resids'></a>\n",
    "\n",
    "### Write a Function to Calculate Residuals\n",
    "\n",
    "Recall that the residuals are simply the error of the model:\n",
    "\n",
    "### $$ \\text{residual}_i = y_i - \\hat{y}_i$$\n",
    "\n",
    "Where $y_i$ is the true value of our target at this observation $i$, $\\hat{y}_i$ is the predicted value of our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='calc-sse'></a>\n",
    "\n",
    "### Write a Function to Calculate the Sum of Squared Errors (SSE)\n",
    "\n",
    "Simple linear regression can use the \"ordinary least squares\" method for identifying linear relations between variables. Here the term [\"least squares\"](https://www.mathworks.com/help/optim/ug/least-squares-model-fitting-algorithms.html) means that it _minimizes the sum of the squared residuals._\n",
    "\n",
    "\n",
    "> **Aside:** Why use the squared residuals instead of just the absolute value of the residuals? Well, both can be used, but absolute value of residuals is typically used when there are large outliers or other abnormalities in variables. [Solving for the least absolute deviations (LAD)](https://en.wikipedia.org/wiki/Least_absolute_deviations) is a type of \"robust\" regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the sum of squared errors from your initial regression with $\\beta_0 = 0$ and $\\beta_1 = 1$ using the functions we defined earlier.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate SSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose a new $\\beta_0$ and $\\beta_1$ you think might be better, and calculate the SSE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot new regression and calculate new SSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='minimize-sse'></a>\n",
    "\n",
    "### Minimizing the Sum of Squared Errors\n",
    "\n",
    "In simple linear regression, we can use calculus to derive the equation that minimizes the sum of squared errors. [See here](http://web.cocc.edu/srule/MTH244/other/LRJ.PDF) or [here](https://en.wikipedia.org/wiki/Simple_linear_regression) for descriptions of the derivation.\n",
    "\n",
    "For those familiar with calculus, **set the derivative of the loss function to 0 and solve for $\\beta_0$ and $\\beta_1$.** The loss function is \"convex\" and therefore it is at its minimum where the derivative is 0. Solving involves taking the partial derivatives for $\\beta_0$ and $\\beta_1$. \n",
    "\n",
    "The equations for the $\\beta_0$ and $\\beta_1$ that minimize the sum of squares are:\n",
    "\n",
    "### $$ \\beta_1 = \\frac{\\sum_{i=1}^n (y_i - \\bar{y} ) (x_i - \\bar{x} )}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $$\n",
    "\n",
    "and\n",
    "\n",
    "### $$ \\beta_0 = \\bar{y} - \\beta_1\\bar{x} $$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the sample means of $x$ and $y$, respectively.\n",
    "\n",
    "#### Write Functions Below to Calculate $\\beta_0$ and $\\beta_1$ Based on These Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to calculate betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the optimal $\\beta_1$ and $\\beta_0$ using your functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the regression with the optimal betas and calculate the SSE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot best fit regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='r2'></a>\n",
    "\n",
    "## $R^2$: The \"Coefficient of Determination\"\n",
    "\n",
    "---\n",
    "\n",
    "> **$R^2$ is the amount of variance explained above baseline in your target $y$ by predictor $x$**.\n",
    "\n",
    "It is composed of two parts: the **total sum of squares** and the **residual sum of squares**.\n",
    "\n",
    "The total sum of squares is defined as:\n",
    "\n",
    "### $$ SS_{tot} = \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2 $$\n",
    "\n",
    "You are already familiar with the residual sum of squares. It is defined as:\n",
    "\n",
    "### $$ SS_{res} = \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2 $$\n",
    "\n",
    "$R^2$ is then calculated with:\n",
    "\n",
    "### $$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} $$\n",
    "\n",
    "The total sum of squares is the **baseline model**: the amount of variance in $y$ we would explain if we were to predict each point of $y$ using just the mean of $y$, $\\bar{y}$.\n",
    "\n",
    "This is equivalent to estimating $y$ by fitting a regression with nothing but the intercept term $\\beta_0$, which becomes the mean of $y$ (the best possible estimator of $y$ using a single value):\n",
    "\n",
    "### $$ \\hat{y} = \\beta_0 = \\bar{y} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the quotient of the the $SS_{res}$ and $SS_{tot}$ decreases, the $R^2$ value gets closer to 1. While the maximum $R^2$ is 1, an $R^2$ can be infinitely negative as well. Having a negative $R^2$ indicates that your predictive equation has greater error than the baseline mode.  \n",
    "\n",
    "_In other words, your equation is worse at representing the relationship than a horizontal line through the Y intercept._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot your regression again, with a new regression line representing the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot regression with baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the SSE for the baseline model and for the model with predictor `sqft`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the SSE for the model and baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to calculate $R^2$. Print out the $R^2$ of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate R^2 for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='slr-to-mlr'></a>\n",
    "\n",
    "## From Simple Linear Regression (SLR) to Multiple Linear Regression (MLR)\n",
    "\n",
    "---\n",
    "\n",
    "The TL;DR of multiple linear regression:\n",
    "\n",
    "> Instead of using just one predictor to estimate a continuous target, we build a model with multiple predictor variables. You will be using MLR much more frequently than SLR going forward.\n",
    "\n",
    "These variables will be represented as columns in a matrix (often a Pandas DataFrame).\n",
    "\n",
    "**Brainstorm some examples of real-world scenarios where multiple predictors would be beneficial. Can you think of cases where it might be detrimental?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assumptions'></a>\n",
    "\n",
    "## Assumptions of MLR\n",
    "\n",
    "---\n",
    "\n",
    "Like SLR, there are assumptions associated with MLR. Luckily, they're quite similar to the SLR assumptions:\n",
    "\n",
    "1) **Linearity:** $Y$ must have an approximately linear relationship with each independent $X_i$.\n",
    "\n",
    "2) **Independence:** Errors (residuals) $\\epsilon_i$ and $\\epsilon_j$ must be independent of one another for any $i \\ne j$.\n",
    "\n",
    "3) **Normality:** The errors (residuals) follow a normal distribution.\n",
    "\n",
    "4) **Equality of Variances**: The errors (residuals) should have a roughly consistent pattern, regardless of the value of the $X_i$ predictors. (There should be no discernable relationship between the $X$ predictors and the residuals.)\n",
    "\n",
    "5) **Independence of Predictors**: The independent variables $X_i$ and $X_j$ must be independent of one another for any $i \\ne j$.\n",
    "\n",
    "The mnemonic LINEI is a useful way to remember these five assumptions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fit-mlr'></a>\n",
    "\n",
    "## Fitting a Multiple Linear Regression\n",
    "\n",
    "---\n",
    "\n",
    "The formula for computing the $\\beta$ values in multiple regression is best done using linear algebra. We will cover the derivation, but for more details  [these slides are a great resource](http://statweb.stanford.edu/~nzhang/191_web/lecture4_handout.pdf).\n",
    "\n",
    "$X$ is now a _matrix_ of predictors $x_1$ through $x_i$ (with each column a predictor), and $y$ is the target vector we are seeking to estimate. There is still only one *estimated* variable!\n",
    "\n",
    "### $$ \\hat{y} = \\beta X$$\n",
    "\n",
    "**Note:** $\\beta$ in the formula above is a *vector* of coefficients now, rather than a single value.\n",
    "\n",
    "In different notation we could write $\\hat{y}$ calculated with:\n",
    "\n",
    "### $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$\n",
    "\n",
    "---\n",
    "\n",
    "<a id='mlr-beta-derivation'></a>\n",
    "\n",
    "### Deriving the $\\beta$ coefficients with linear algebra\n",
    "\n",
    "$\\beta$ is solved with the linear algebra formula:\n",
    "\n",
    "### $$ \\beta = (X'X)^{-1}X'y $$\n",
    "\n",
    "Where $X'$ is the transposed matrix of original matrix $X$ and $(X'X)^-1$ is the inverted matrix of $X'X$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation using true $y$ is:\n",
    "\n",
    "### $$ y = \\beta X + \\epsilon $$\n",
    "\n",
    "Again, $\\epsilon$ is our vector of errors, or residuals.\n",
    "\n",
    "We can equivalently formulate this in terms of the residuals as:\n",
    "\n",
    "### $$ \\epsilon = \\beta X - y $$\n",
    "\n",
    "Our goal is to minimize the sum of squared residuals. The sum of squared residuals is equivalent to the dot product of the vector of residuals:\n",
    "\n",
    "### $$ \\sum_{i=1}^n \\epsilon_i^2 = \n",
    "\\left[\\begin{array}{cc}\n",
    "\\epsilon_1 \\cdots \\epsilon_n\n",
    "\\end{array}\\right] \n",
    "\\left[\\begin{array}{cc}\n",
    "\\epsilon_1 \\\\ \\cdots \\\\ \\epsilon_n\n",
    "\\end{array}\\right] = \\epsilon' \\epsilon\n",
    "$$\n",
    "\n",
    "Therefore we can write the sum of squared residuals as:\n",
    "\n",
    "### $$ \\epsilon' \\epsilon = (\\beta X - y)' (\\beta X - y) $$\n",
    "\n",
    "Which becomes:\n",
    "\n",
    "### $$ \\epsilon' \\epsilon = y'y - y'X\\beta - \\beta' X' y + \\beta' X' X \\beta $$\n",
    "\n",
    "Now take the derivative with respect to $\\beta$:\n",
    "\n",
    "### $$ \\frac{\\partial \\epsilon' \\epsilon}{\\partial \\beta} = \n",
    "-2X'y + 2X'X\\beta$$\n",
    "\n",
    "We want to minimize the sum of squared errors, and so we set the derivative to 0 and solve for the beta coefficient vector:\n",
    "\n",
    "### $$ 0 = -2X'y + 2X'X\\beta \\\\\n",
    "X'X\\beta = X'y \\\\\n",
    "\\beta = (X'X)^{-1}X'y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code-mlr'></a>\n",
    "\n",
    "### Code an MLR\n",
    "\n",
    "**First, we need to create the \"design matrix\" of our predictors.**\n",
    "\n",
    "The first column will be a column of all 1s (the intercept) and the other columns will be `sqft`, `bdrms`, and `age`.\n",
    "\n",
    "This is easiest to do with Pandas. Add a column for the intercept first, then extract the matrix using `.values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the X matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve for the Beta Coefficients\n",
    "\n",
    "We are still predicting `price`. Implement the linear algebra equation to solve for the beta coefficients. \n",
    "\n",
    "### $$ \\beta = (X'X)^{-1}X'y $$\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "The transpose of a matrix is calculated by appending `.T` to the matrix:\n",
    "\n",
    "    X.T\n",
    "\n",
    "Matrices multiplied in the formula should be done with the \"dot product:\"\n",
    "\n",
    "    np.dot(mat1, mat2)\n",
    "\n",
    "Inverting a matrix is done using:\n",
    "\n",
    "    np.linalg.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the beta vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confirm that these betas are the same as the ones using `sklearn.linear_model.LinearRegression`**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression(fit_intercept=False)\n",
    "linreg.fit(X, price)\n",
    "\n",
    "print linreg.coef_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validate that the beta vector is the same as scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate predicted $\\hat{y}$ with your $X$ predictor matrix and $\\beta$ coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the $R^2$ of the multiple regression model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate MLR R^2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "[Maximum-Likelihood Estimation](https://onlinecourses.science.psu.edu/stat504/node/28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
