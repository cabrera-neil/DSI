{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Build a (Basic) Random Forest From Scratch\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "> **Note**: This lab is intended to be completed in a group or as a code-along with the instructor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Random Forest?\n",
    "\n",
    "---\n",
    "\n",
    "Random forests are some of the most widely used classifiers. They perform well and are relatively simple to use, as they require very few parameters. \n",
    "\n",
    "As we have seen, decision trees are powerful machine learning models, but they also have some critical limitations. In particular, trees that are grown deep tend to learn highly irregular patterns and therefore overfit their training sets. Bagging helps mitigate this problem by exposing different trees to different subsamples of the whole training set.\n",
    "\n",
    "Random forests average multiple deep decision trees trained on different parts of a training set further, with the goal of reducing the variance. This does come with a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.\n",
    "\n",
    "### Feature Bagging\n",
    "\n",
    "Random forests differ from decision tree bagging in only one way: They use a modified tree-learning algorithm that selects a random subset of the features at each candidate split in the learning process. This process is sometimes called feature bagging. \n",
    "\n",
    "Using random forests helps combat the correlation of trees in an ordinary bootstrap sample. Normally, for example, if one or a few features are strong predictors for the response variable (target output), these features will be the ones selected in many of the bagging base trees. This causes these features to become correlated. By selecting a random subset of features at each split, we avoid this correlation between base trees and strengthen the overall model.\n",
    "\n",
    "**For a problem with `p` features, it is typical to use:**\n",
    "\n",
    "- `p^(1/2)` features in each split for a classification problem (rounded down).\n",
    "- `p/3` with a minimum node size of five as the default for a regression problem (rounded down)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Instructions\n",
    "\n",
    "---\n",
    "\n",
    "**A random forest classifier satisfying these conditions is built:**\n",
    "\n",
    "1) Multiple internal decision tree classifiers will be built as the base models.\n",
    "- For each base model, the data will be resampled with replacement data (this is what is referred to as bootstrapping).\n",
    "- Each decision tree will be fit on one of the bootstrapped samples of the original data.\n",
    "- Each internal base model will then be passed the new data and make its predictions. The final output will be the result of a vote across the base models for the class.\n",
    "\n",
    "**Your custom random forest classifier must:**\n",
    "\n",
    "1) Accept the hyperparameters `max_features`, `n_estimators`, and `max_depth`.\n",
    "2) Implement a `fit` method.\n",
    "3) Implement a `predict` method.\n",
    "4) Implement a `score` method.\n",
    "5) Satisfy the conditions for random forest classifiers listed above.\n",
    "6) **for the sake of simplicity, you will not be implementing feature bagging in this lab.**\n",
    "\n",
    "**Test your random forest classifier on the pre-cleaned Titanic data set and compare the performance to a single decision tree classifier.**\n",
    "\n",
    "> *Note: You're allowed to use the `DecisionTreeClassifier` class in scikit-learn for the internal base estimators. This lab is about building the random forest ensemble estimator, not building a decision tree class!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('./datasets/titanic_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will be a custom, basic version of a random forest in the style of\n",
    "# scikit-learn's models.\n",
    "class RandomForest(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self):\n",
    "        pass\n",
    "    \n",
    "    def score(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
