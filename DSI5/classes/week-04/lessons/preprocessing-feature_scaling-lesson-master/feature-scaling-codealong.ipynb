{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Feature Scaling\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Joseph Nelson (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Explain the benefits of scaling data.\n",
    "- Identify situations in which scaling data is beneficial. \n",
    "- Scale data using Python and scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lesson Guide\n",
    "- [Introduction to Feature Scaling](#intro)\n",
    "- [Why Scale Data?](#why-scale)\n",
    "- [Centering](#centering)\n",
    "- [Standardization](#standardization)\n",
    "    - [Standardizing with scikit-learn's `StandardScaler`](#standard-scaler)\n",
    "- [Normalization](#normalization)\n",
    "    - [Normalizing With scikit-learn's `MinMaxScaler`](#minmax)\n",
    "- [Independent Practice Scaling the Wine Data Set](#independent-practice)\n",
    "- [Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction to Feature Scaling\n",
    "\n",
    "---\n",
    "\n",
    "Scaling data is the process of increasing or decreasing its magnitude according to a fixed ratio. In other words, you change the size but not the shape of the data (the shape of the distribution remains unchanged).\n",
    "\n",
    "Some data scaling methods often change the *location* of the data as well. For example, when \"centering\" we take a distribution and change its mean to zero by subtracting the mean of the distribution from each data point in the distribution. While this is not technically \"scaling,\" changing the location is often part of the process and preserves the shape of the data set (it just shifts it around).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='why-scale'></a>\n",
    "\n",
    "## Why Should We Scale Data?\n",
    "\n",
    "---\n",
    "\n",
    "**There are a number of good reasons to scale our data:**\n",
    "- To handle disparities in units.\n",
    "- To cut computational expense.\n",
    "- To improve model performance (especially for machine learning).\n",
    "- We scale for models to prevent the steps on different axes from varying widely.\n",
    "\n",
    "**It’s rarely a bad idea to scale your data.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='centering'></a>\n",
    "\n",
    "## Centering: Changing the Location of Data\n",
    "\n",
    "---\n",
    "\n",
    "Let's start with the simplest transformation example — centering. If we have a distribution of values ($X$), then to center our data to a new distribution ($X_c$) we apply the following formula:\n",
    "\n",
    "### $$ X_c = X - \\bar{X} $$\n",
    "\n",
    "### Benefits of Centering Data\n",
    "\n",
    "In linear modeling, the primary benefit of centering your predictor data is that **the intercept now represents the estimate of the target when all predictors are at their mean value.**\n",
    "\n",
    "If we don't center, the intercept is the estimate of our model when all predictors are at value 0. When you center your predictors, it often makes the intercept much more interpretable.\n",
    "\n",
    "### Centering Example: Baseball Player Height and Weight\n",
    "\n",
    "Load in the data set containing the heights, weights, and ages of baseball players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseball = pd.read_csv('./datasets/baseball_height_weight.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the distribution of the heights and weights below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct a linear regression predicting weight from height. Interpret the value of the intercept and the coefficient from this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Center the height variable and rerun the regression with the centered height. Interpret the new intercept and coefficient.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standardization'></a>\n",
    "\n",
    "## Standardization\n",
    "\n",
    "---\n",
    "\n",
    "The most common method of scaling is standardization. In standardization, we first center the data and then divide by the standard devation to enforce that the standard deviation of the variable is one:\n",
    "\n",
    "### $$ X_{std} = \\frac{X - \\bar{X}}{s_{X}} $$\n",
    "\n",
    "### Benefits of Standardizing Data\n",
    "\n",
    "There are many benefits to standardizing our data, especially when we have more than one predictor:\n",
    "- Intercepts are interpreted as the estimate when all predictors are at their mean value.\n",
    "- Coefficients are in units of standard deviations of the original predictors. This allows for direct comparison of the magnitude of impact between different predictors.\n",
    "- Optimization methods (minimizing loss functions) are faster and more stable.\n",
    "- It is required for regularization penalties where the magnitude of coefficients for different predictors must have the same meaning.\n",
    "- In K-nearest neighbors (KNN) methods, it is necessary if you want features to contribute equally, as these models use the distance between observations calculated from the features.\n",
    "- K-means clustering is affected by the scale of the data, so standardizing the features will prevent variables from dominating simply based on their scale.\n",
    "- In logistic regression, neural networks, and support vector machines, unscaled data can result in a disproportionate effect of some data points over others.\n",
    "\n",
    "> **Note:** In ordinary linear regression, centering and scaling your variables does *not* impact the amount of variance for which you can account. This is because we are only moving and adjusting the magnitude of the distribution; the shape of the distribution does not change.\n",
    "\n",
    "### Standardization Example\n",
    "\n",
    "First, plot the original `height` variable against the `weight` variable. Use seaborn's `sns.jointplot`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create standardized versions of the `height` and `weight` variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the standardized `weight` against the `height`. Notice the distribution shapes and relationship between the variables is unchanged.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standard-scaler'></a>\n",
    "### Standardizing with scikit-learn's `StandardScaler`\n",
    "\n",
    "Scikit-learn comes packaged with a class, `StandardScaler`, that will perform the standardization on a matrix for you. \n",
    "\n",
    "Load in the package like so:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "Once instantiated, the `StandardScaler` object has three primary built-in methods:\n",
    "- `.fit(X)` will calculate the mean and standard deviations for each column of X.\n",
    "- `.transform(X)` will take X and return a transformed version of X where each column is standardized according to their means and standard deviations (you must have run `.fit()` first).\n",
    "- `.fit_transform(X)` combines the `.fit()` method and the `.transform()` method.\n",
    "\n",
    "**Use `StandardScaler` to standardize a predictor matrix containing heights and weights from the baseball data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a linear regression predicting age from the standardized height and weight data. Interpret the coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='normalization'></a>\n",
    "\n",
    "## Normalization\n",
    "\n",
    "---\n",
    "\n",
    "Normalization most often refers to the process of \"normalizing\" a variable to exist between 0 and 1. Think of it as squishing the variable to restrict it to a specific range.\n",
    "\n",
    "### $$ X_{norm} = \\frac{X - min(X)}{max(X) - min(X)} $$\n",
    "\n",
    "This type of normalization is typically referred to as \"min-max scaling.\" \n",
    "\n",
    "### Benefits of Normalization\n",
    "\n",
    "Typically, standardization is preferred to min-max normalization. However, there are some applications where min-max scaling is preferable:\n",
    "- In neural networks, for example, which often require their inputs to be bounded between 0 and 1. \n",
    "- In images where pixels can only take on a specific range of RGB values.\n",
    "\n",
    "<a id='minmax'></a>\n",
    "### Normalization with `MinMaxScaler`\n",
    "\n",
    "Scikit-learn also has a class for normalization called `MinMaxScaler`:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "```\n",
    "\n",
    "The `MinMaxScaler` has the same `fit()`, `transform()`, and `fit_transform()` methods.\n",
    "\n",
    "**Normalize the `age`, `height`, and `weight` variables using `MinMaxScaler.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at the min and max ranges for the normalized matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the normalized `height` against the normalized `weight`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='independent-practice'></a>\n",
    "\n",
    "## Independent Practice: Scaling the Wine Data Set\n",
    "\n",
    "---\n",
    "\n",
    "Below you'll load in the wine quality data set. This data set contains a variety of features for different types/brands of wine. \n",
    "\n",
    "**You should:**\n",
    "1) Load and examine the data.\n",
    "2) Create a target variable for wine quality.\n",
    "3) Create a predictor matrix with the variables of your choice.\n",
    "4) Create standardized and normalized versions of your predictor matrix.\n",
    "5) Employing cross-validation, calculate the average $R^2$ score for wine quality using the original predictors, the standardized predictors, and the normalized predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Load and examine the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Create a target variable for wine quality.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Create a predictor matrix with variables of your choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Create a standardized and normalized version of your predictor matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Employing cross-validation, calculate the average $R^2$ score for wine quality using the original predictors, the standardized predictors, and the normalized predictors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "[About Feature Scaling and Normalization](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
