{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Classification with K-Nearest Neighbors\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Alexander Barriga (SF), Joseph Nelson (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the difference between classification and regression models.\n",
    "- Explore the k-nearest neighbors (KNN) algorithm visually and in pseudocode.\n",
    "- Explain the differences between distance metrics and explore the two most common kinds.\n",
    "- Apply KNN classification to the Wisconsin Breast Cancer data set.\n",
    "- Practice manually performing stratified cross-validation.\n",
    "- Visually examine the effect of k neighbors on the decision boundary.\n",
    "- Explain the effect of choosing k on the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction: Classification vs. Regression](#intro)\n",
    "- [K-nearest Neighbors, Visually](#knn-visual-intro)\n",
    "- [The K-Nearest Neighbors (KNN) Algorithm](#knn)\n",
    "    - [A Note on Parametric vs. Nonparametric Methods](#nonparametric)\n",
    "- [The KNN Distance Metric](#distance)\n",
    "    - [Euclidean Distance](#euclidean)\n",
    "    - [Manhattan Distance](#manhattan)\n",
    "- [Load the Wisconsin Breast Cancer Data Set](#wisconsin)\n",
    "    - [Rename Columns and Subset the Data](#rename-subset)\n",
    "    - [Encode the Target as a Binary Class](#target)\n",
    "- [Examine the Correlation Structure of the Data Set](#correlations)\n",
    "    - [Use a Heatmap](#heatmap)\n",
    "    - [Use a Pairplot](#pairplot)\n",
    "- [Using Scikit-Learn's `KNeighborsClassifier` and `StratifiedKFold`](#kneighborsclassifier)\n",
    "    - [Create the Target and Predictors](#target-predictors)\n",
    "    - [Standardize the Predictor Matrix](#standardize)\n",
    "    - [Write a Function to Manually Perform the Cross-Validation Procedure](#manual-cv)\n",
    "    - [Calculate the \"Baseline\" Accuracy](#baseline)\n",
    "    - [Cross-Validate the Mean Accuracy With Five Neighbors](#cv-knn5)\n",
    "    - [Cross-Validate the Mean Accuracy with One Neighbor](#cv-knn5)\n",
    "- [Visualize the KNN Decision Boundary](#visualize-knn)\n",
    "- [How is Bias-Variance Affected by the Number of Neighbors?](#bias-variance)\n",
    "- [Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction: Regression vs. Classification\n",
    "\n",
    "We've discussed the difference between continuous and discrete numbers. We've predicted continuous numbers using regression. But what about discrete numbers?\n",
    "\n",
    "Think back to the wine quality data set we've used in the past. We used linear regression to predict the quality on a zero–10 scale. What if we just wanted to predict whether wine was good or bad? Red or white? \n",
    "\n",
    "Classification algorithms do just that — they predict categories or classes. They split the data into groups and place new data into those groups. \n",
    "\n",
    "![](http://ipython-books.github.io/images/ml.png \"Best Split vs Best Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn-visual-intro'></a>\n",
    "\n",
    "---\n",
    "\n",
    "### K-Nearest Neighbors (KNN), Visually\n",
    "\n",
    "**KNN works in a similar way to how humans might approach classification. Below we have some red and blue dots:**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_reds_and_blues.png \"Some Dots\")\n",
    "\n",
    "**A new dot appears without a color. We need to decide which color it's most likely going to be.**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point.png \"A New Dot Appears\")\n",
    "\n",
    "**We compare it to its three nearest neighbors — its neighbors are more often red, so we label it red.**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point_pred.png \"3 Nearest Neighbors\")\n",
    "\n",
    "**What if we increase the number of neighbors to consider to five?**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point_pred_blue.png \"5 Nearest Neighbors\")\n",
    "\n",
    "**This is, in essence, the k-nearest neighbors (KNN) algorithm. The k represents the number of \"neighbors\" you consider.**\n",
    "\n",
    "> ***Images above credited to the [yhat blog](http://blog.yhat.com/).***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>\n",
    "\n",
    "## The KNN Algorithm\n",
    "\n",
    "---\n",
    "\n",
    "K-nearest neighbors takes a different approach to modeling than we've been practicing with linear models. In order to estimate a value (regression) or class membership (classification), the algorithm finds the observations in its training data that are \"nearest\" to the observation it has to predict. It then averages or takes a vote of those training observations' target values to estimate the value for the new data point.\n",
    "\n",
    "Distance is usually calculated using the Euclidean distance. The \"k\" in KNN refers to the number of nearest neighbors contributing to the prediction. \n",
    "\n",
    "Today we'll only be looking at KNN in the context of classification.\n",
    "\n",
    "**The KNN can be concisely represented with pseudocode:**\n",
    "\n",
    "```\n",
    "for unclassified_point in sample:\n",
    "    for known_point in known_class_points:\n",
    "        calculate distances (euclidean or other) between known_point and unclassified_point\n",
    "    for k in range of specified_neighbors_number:\n",
    "        find k_nearest_points in known_class_points to unclassified_point\n",
    "    assign class to unclassified_point using \"votes\" from k_nearest_points\n",
    "```\n",
    "\n",
    "> **Note**: In the case of ties, scikit-learn's `KNeighborsClassifier()` will choose the first class (when weights are uniform). If this is unappealing, you can change the weights keyword argument to `'distance'`. More on this later.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='nonparametric'></a>\n",
    "\n",
    "### A Note on Parametric vs. Nonparametric Methods\n",
    "\n",
    "So far, all of our tests and methods have been **parametric**, meaning we've assumed that our data falls into a certain distribution. In linear regression, our parameters are the coefficients in our model and our estimate of the target is calculated from these parameters.\n",
    "\n",
    "There are alternatives for cases in which we can't assume a particular distribution for our data — or we choose not to. When we make no assumptions about the distribution for our data, we call our data **nonparametric**. For nearly every parametric test, there's a nonparametric analog available. The KNN model is an example of a nonparametric model. You can see that there are no coefficients for the different predictors and our estimate is not represented by a formula of our predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='distance'></a>\n",
    "## The KNN Distance Metric\n",
    "\n",
    "---\n",
    "KNN typically uses one of two distance metrics: **Euclidean**('yoo-klid-ee-uhn') or **Manhattan**. Other distance metrics are possible, but rarer. Sometimes it makes sense to create your own distance function.\n",
    "\n",
    "<a id='euclidean'></a>\n",
    "### Euclidean Distance\n",
    "\n",
    "Recall the famous Pythagorean theorem:\n",
    "![Alt text](http://ncalculators.com/images/pythagoras-theorem.gif)\n",
    "\n",
    "We can apply the theorem to calculate the distance between two points. This is called Euclidean distance. \n",
    "\n",
    "![Alt text](http://rosalind.info/media/Euclidean_distance.png)\n",
    "\n",
    "### $$\\text{Euclidean  distance}=\\sqrt{(x_1-x_2)^2+(y_1-y_1)^2}$$\n",
    "\n",
    "There are many different distance metrics, but Euclidean is the most common (and the default in scikit-learn).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='manhattan'></a>\n",
    "### Manhattan Distance (Taxicab Distance)\n",
    "\n",
    "Another way to measure the distance between two points is to take the sum of the absolute value of their differences. \n",
    "\n",
    "### $$ D = \\sum_{i=1}^n | x_i - y_i | $$\n",
    "\n",
    "The name \"Manhattan distance\" comes from the fact that taxis in Manhattan must drive from point A to point B on streets that force traffic to flow forward or backward and left or right — but never diagonally. \n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Manhattan_distance_bgiu.png/261px-Manhattan_distance_bgiu.png)\n",
    "![](https://pbs.twimg.com/media/CgIlqLTWEAAedKB.jpg)\n",
    "\n",
    "**Note that the Manhattan distance is a less common choice.**\n",
    "- Manhattan distance is more restrictive than Euclidean distance in terms of how distance is measured.\n",
    "- [Manhattan distance comes from $L_{p = 1}$ space and Euclidean distance comes from $L_{p = 2}$ space](https://en.wikipedia.org/wiki/Lp_space).\n",
    "- In practice, we can cross-validate KNN using both types of distances to see which performs best. \n",
    "\n",
    "![](http://www.improvedoutcomes.com/docs/WebSiteDocs/image/diagram_euclidean_manhattan_distance_metrics.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wisconsin'></a>\n",
    "\n",
    "## Load the Wisconsin Breast Cancer Data Set\n",
    "\n",
    "---\n",
    "\n",
    "Below we'll be testing out the KNN classification algorithm on the [Wisconsin Breast Cancer data set](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data).\n",
    "\n",
    "> **Note:** (The file as is saved as `.data` suffix, but is actually formatted as a `.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcw = pd.read_csv('./datasets/wdbc.data', header=None, index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='rename-subset'></a>\n",
    "### Rename Columns and Subset the Data\n",
    "\n",
    "The following attributes are the columns of the data set.\n",
    "\n",
    "The column names are taken from the data set's information file (check it out for more details):\n",
    "\n",
    "`./assets/datasets/wdbc.names`\n",
    "\n",
    "You can open it with a text editor of your choice.\n",
    "\n",
    "      Attribute                     \n",
    "    --------------------------------------------\n",
    "    1. Sample code number [subject ID].\n",
    "    2. Class\n",
    "    3. Cell nucleus mean radius.\n",
    "    4. Cell nucleus SE radius.\n",
    "    5. Cell nucleus worst radius.\n",
    "    6. Texture mean.\n",
    "    7. Texture SE.\n",
    "    8. Texture worst.\n",
    "    9. Perimeter mean.\n",
    "    10. Perimeter SE.\n",
    "    11. Perimeter worst.\n",
    "    12. Area mean.\n",
    "    13. Area SE.\n",
    "    14. Area worst.\n",
    "    15. Smoothness mean.\n",
    "    16. Smoothness SE.\n",
    "    17. Smoothness worst.\n",
    "    18. Compactness mean.\n",
    "    19. Compactness SE.\n",
    "    20. Compactness worst.\n",
    "    21. Concavity mean.\n",
    "    22. Concavity SE.\n",
    "    23. Concavity worst.\n",
    "    24. Concave points mean.\n",
    "    25. Concave points SE.\n",
    "    26. Concave points worst.\n",
    "    27. Symmetry mean.\n",
    "    28. Symmetry SE.\n",
    "    29. Symmetry worst.\n",
    "    30. Fractal dimension mean.\n",
    "    31. Fractal dimension SE.\n",
    "    32. Fractal dimension worst.\n",
    "   \n",
    "**Using the list provided, reassign the column names in the data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['id','malignant',\n",
    "                'nucleus_mean','nucleus_se','nucleus_worst',\n",
    "                'texture_mean','texture_se','texture_worst',\n",
    "                'perimeter_mean','perimeter_se','perimeter_worst',\n",
    "                'area_mean','area_se','area_worst',\n",
    "                'smoothness_mean','smoothness_se','smoothness_worst',\n",
    "                'compactness_mean','compactness_se','compactness_worst',\n",
    "                'concavity_mean','concavity_se','concavity_worst',\n",
    "                'concave_pts_mean','concave_pts_se','concave_pts_worst',\n",
    "                'symmetry_mean','symmetry_se','symmetry_worst',\n",
    "                'fractal_dim_mean','fractal_dim_se','fractal_dim_worst']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the columns pertaining to the standard deviation and \"worst\" measurements, leaving only the mean measurement columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='target'></a>\n",
    "### Encode the Target Class Variable `malignant` to be a Binary 0 vs. 1\n",
    "\n",
    "The `malignant` class target variable is coded as \"B\" for benign and \"M\" as malignant. \n",
    "\n",
    "We need to recode this to a binary integer for classification:\n",
    " - Encode `malignant` as 1.\n",
    " - Encode `benign` as 0.\n",
    " \n",
    "`malignant` is assigned to 1 because our goal is to predict malignant tumors using the data. In binary classification problems, the category we want to predict is typically encoded as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='correlations'></a>\n",
    "## Examine the Correlation Structure of the Data Set\n",
    "\n",
    "---\n",
    "\n",
    "You should exclude the `id` column, as it's just an indicator variable for the subject.\n",
    "\n",
    "<a id='heatmap'></a>\n",
    "### Method 1: Plot a Heatmap of the Correlation Matrix\n",
    "\n",
    "Plot a Seaborn heatmap of the correlation matrix to visually examine which variables are correlated and anti-correlated and to what degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pairplot'></a>\n",
    "### Method 2: Use Seaborn's `pairplot()` Function to Visualize Relationships Between Variables\n",
    "\n",
    "When you have a small number of predictor variables, Seaborn's `pairplot()` function will give you a more detailed, visual look at the relationships between them. The pairplot is similar to a correlation matrix but displays scatterplots of variable pairs. Along the diagonal line are histograms showing the distribution of each variable.\n",
    "\n",
    "One of the most appealing aspects of the `pairplot` function for classification tasks is that the scatterplots and histograms can be split along a `hue` variable. If we use the `malignant` target class as the hue, we're able to see how the classes are distributed across these variables as well.\n",
    "\n",
    "Plot data using Seaborn's `pairplot()` function. The hue will be the class variable \"malignant.\" The variables will be the other columns excluding, of course, the subject ID column. This function can take some time to run.\n",
    "\n",
    "> **Note:** Most of these predictors are highly correlated with the class variable. This is an indication that our classifier is likely to perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kneighborsclassifier'></a>\n",
    "\n",
    "## Using Scikit-Learn's `KNeighborsClassifier` and `StratifiedKFold`\n",
    "\n",
    "---\n",
    "\n",
    "Let's see how scikit-learn's KNN classifier performs on our data set, predicting the `malignant` target class using cross-validation.\n",
    "\n",
    "Load the KNN classifier:\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "```\n",
    "\n",
    "**We're going to set some arguments when instantiating the model:**\n",
    "1. **n_neighbors**: Specifies how many neighbors will vote on the class.\n",
    "2. **weights**: Uniform weights indicate that all neighbors have the same weight.\n",
    "3. **metric** and **p**: when distance is `minkowski` (the default) and `p == 2` (the default), _it's equivalent to the Euclidean distance metric_.\n",
    "\n",
    "Also load scikit-learn's `StratifiedKFold` from the `model_selection` module:\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "```\n",
    "\n",
    "The `StratifiedKFold` object will return cross-validation _indices_ that you can use to subset your data — for example, in a loop that runs the model and tests it. \n",
    "\n",
    "This is the **stratified** version of the `KFold` class. Stratification ensures that there are equal proportions of the predicted class in each train-test fold. This is a best practice in classification tasks.\n",
    "\n",
    "> **Note:** The `cross_val_score` can also stratify for you. However, you should get familiar with using indices for cross-validation on data. Being able perform cross-validation at a more \"manual\" level allows for a lot more power and customization. It also reinforces what's happening in your head during cross-validation, as you have to divide up the data yourself with the indices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='target-predictors'></a>\n",
    "### Create Your Target Vector and Predictor Matrix\n",
    "\n",
    "The target should be the binary `malignant` column — the predictors are up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standardize'></a>\n",
    "\n",
    "### Standardize the Predictor Matrix\n",
    "\n",
    "Standardization should be done for the predictors when using a KNN model. But why? \n",
    "\n",
    "Remember that KNN finds the nearest neighbors according to a distance metric. If the predictors are left unstandardized, then it's possible that some predictors will have an unfair impact on the distance measure simply because they're on a larger scale than other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-inds'></a>\n",
    "### Create the Cross-Validation Indices Using `StratifiedKFold`\n",
    "\n",
    "`StratifiedKFold` is instantiated with `n_splits` - number of train-test pairs desired.\n",
    "\n",
    "The built-in `.split()` function will take a predictor matrix, target arrays, and return the training and testing indices.\n",
    "\n",
    "> **Note:** The `split()` function will return a Python *generator*. This can be iterated but works differently from a list in that, once iterated, it'll be \"empty\". You can convert the output to a list using a list comprehension if you need to use the indices multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='manual-cv'></a>\n",
    "### Write a Function to Manually Perform Cross-Validation Using Your Stratified Indices\n",
    "\n",
    "Now that we have the indices — row indexes for our train/test splits — write a function that will:\n",
    "- Split the x and y into training and testing subsets.\n",
    "- Fit a KNN classifier on the training set.\n",
    "- Calculate the accuracy of the classifier on the testing set.\n",
    "- Store the accuracies for each fold and return them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseline'></a>\n",
    "### Calculate the \"Baseline\" Accuracy\n",
    "\n",
    "Before we can evaluate whether our classifier's accuracy is good or bad, we need to know the baseline accuracy.\n",
    "\n",
    "**The baseline accuracy is the proportion of the majority class.**\n",
    "\n",
    "For a binary classification, this means that the baseline accuracy is the percent of the data set that's labeled malignant or benign, depending on whether `malignant` or `benign` is greater. This can be calculated as:\n",
    "\n",
    "```python\n",
    "baseline = np.mean(y)  # if np.mean(y) is >= 0.5\n",
    "baseline = 1. - np.mean(y) # if np.mean(y) is < 0.5\n",
    "```\n",
    "\n",
    "**It is critical that you know your baseline accuracy,**\n",
    "\n",
    "Imagine your data set had, for example, 95 ones and 5 zeroes, and you got 95 percent accuracy using KNN. If you hadn't looked at your baseline accuracy, you may conclude that your classifier is doing well, when in fact it's doing no better than chance. The classifier could have guessed only 1s and gotten a 95 percent accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-knn5'></a>\n",
    "### Cross-Validate the Mean Accuracy for a KNN Model With Five Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-knn1'></a>\n",
    "### Cross-Validate the Mean Accuracy for a KNN Model With One Neighbor\n",
    "\n",
    "As you can see, the mean cross-validated accuracy is high with five neighbors. \n",
    "\n",
    "Let's see what it's like when we only use one neighbor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualize-knn'></a>\n",
    "\n",
    "## Visualize the KNN Decision Boundary\n",
    "\n",
    "---\n",
    "\n",
    "Even with one neighbor, we can do quite well predicting the malignant observations.\n",
    "\n",
    "Below you can load an interactive KNN visualization that shows how the decision boundary of a KNN classifier changes as the number of neighbors changes.\n",
    "\n",
    "The `KNNBoundaryPlotter` class has four required arguments:\n",
    "\n",
    "    KNNBoundaryPlotter (data, predictor1, predictor2, class_target)\n",
    "    \n",
    "By default, it will fit a visualization of the decision boundary across 1–100 nearest neighbors.\n",
    "\n",
    "The boundary is where the classifier will vote for `malignant` or `benign` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import imp\n",
    "# plotter = imp.load_source('plotter', './knn_plotter.py')\n",
    "# from plotter import KNNBoundaryPlotter\n",
    "\n",
    "# kbp = KNNBoundaryPlotter(bcw, 'area_mean', 'symmetry_mean', 'malignant', nn_range=range(1,101))\n",
    "\n",
    "# kbp.knn_mesh_runner()\n",
    "# kbp.knn_interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bias-variance'></a>\n",
    "### How Does Increasing the Number of Neighbors Impact the Bias and Variance of Your Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Scott Foreman-Roe's [breakdown](http://scott.fortmann-roe.com/docs/BiasVariance.html) of the bias-variance tradeoff, featuring a discussion of KNN (required reading).\n",
    "- A [detailed discussion](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/) of KNN.\n",
    "- A lengthy example of KNN applied to [image classification](http://cs231n.github.io/classification/ ).\n",
    "- If academic breakdowns are your thing, be sure to visit [this](http://me.seekingqed.com/files/intro_KNN.pdf) resource.\n",
    "- Read the scikit-learn [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) on implementing KNN.\n",
    "- Context on choosing the right [algorithm from scikit-learn](http://scikit-learn.org/stable/tutorial/machine_learning_map/).\n",
    "- A deeper dive into [Euclidian distance](http://www.econ.upf.edu/~michael/stanford/maeb4.pdf).\n",
    "- A classifier comparison from [scikit-learn](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) (this is also in our [repository](https://github.com/ga-students/DSI-DC-2/blob/master/curriculum/Week-04/4.01%20Intro%20to%20Classification/classification-methods.py))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
