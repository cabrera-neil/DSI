{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Practice Spark Lab\n",
    "\n",
    "_Authors: David Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "> *Note: This lab may be best administered as a walk through with the instructor.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, we will use Spark to dig into the Bay Area Bike Share data.**\n",
    "\n",
    "You'll need to run this lab on the virtual machine (VM) provided. As usual, connect to your VM using:\n",
    "\n",
    "    vagrant up\n",
    "    vagrant ssh\n",
    "\n",
    "Once inside, run:\n",
    "\n",
    "    spark_local_start.sh\n",
    "\n",
    "**Important:** If your machine is already running and you've started the Hadoop services with `bigdata_start.sh`, you may want to first run `bigdata_stop.sh` to stop all services and free up some memory.\n",
    "\n",
    "Once you've started Spark in local mode, you should be able to access Jupyter at this address:\n",
    "\n",
    "http://10.211.55.101:18888\n",
    "\n",
    "This is where we'll work.\n",
    "\n",
    "Our goal is to calculate the average number of trips per hour using the Caltrain Station as a starting point.\n",
    "\n",
    "**Check that your SparkContext is available.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load the Bay Area Bike Share trip data.\n",
    "\n",
    "> **Note:** The data have been pre-loaded onto your VM. `201408_babs_open_data/201408_trip_data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = sc.textFile('file:///home/vagrant/data/201408_babs_open_data/201408_trip_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) What kind of object are the data loaded as?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As a resilient distributed data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Split CSV lines.\n",
    "\n",
    "In Spark, we can build complex pipelines that are only executed when we ask to collect them.\n",
    "\n",
    "In a Python pipeline, the calculation is immediately executed, but, with Spark, the pipeline definition and execution are separate steps.\n",
    "\n",
    "In other words, we can define the pipeline with all of its steps and the data will only flow through it when we call `collect`. In order to get familiar with this new workflow, we'll start with small steps to build our pipeline.\n",
    "\n",
    "**Apply a map to trips that split each line at commas and save that to a resilient distributed data set (RDD).**\n",
    "\n",
    "> **Hint:** If you want to check that you're doing things correctly, you can collect the result and display the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = trips.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Filter for the Caltrain station.\n",
    "\n",
    "We can also create filters using the `filter()` method in Spark.\n",
    "\n",
    "**Select station number 70 by filtering on the fifth column.** \n",
    "\n",
    "We perform all of the following analysis on just this station, which corresponds to the most popular starting point. Save this to a variable called `station_70`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_70 = trips.filter(lambda x: x[4] == '70')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Trips by day: Hour (mapper).\n",
    "\n",
    "Let's analyze the trips by the hour. We can do this by performing a MapReduce job in Spark. First, we'll need to emit tuples with a count of one for each key (day and hour), then we'll sum the counts by key.\n",
    "\n",
    "**Emit a tuple of ((date, hour), 1), applying a map to `station_70` that extracts the relevant data from each line.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Emit a tuple of ((date, hour), 1).\n",
    "trips_by_day_hour = station_70.map(lambda x: ((x[2].split()[0], x[2].split()[1].split(':')[0]), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Trips by day: Hour (reducer).\n",
    "\n",
    "Use the `reduceByKey()` method to obtain the number of trips per day and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips_by_day_hour = trips_by_day_hour.reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Trips by hour (mapper).\n",
    "\n",
    "Let's further group the trips by hour. We'll do this with a second MapReduce job.\n",
    "\n",
    "First, we will discard the day and emit a tuple for hour and count. You can achieve this with a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Emit a tuple of (hour, count).\n",
    "trips_by_hour = trips_by_day_hour.map(lambda x: (int(x[0][1]), x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Trips by hour (reducer).\n",
    "\n",
    "Now, calculate the average number of trips by hour using the `combineByKey()` method.\n",
    "\n",
    "> You can find a suggestion for how to do this [here](http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_trips_by_hour = trips_by_hour.combineByKey( (lambda x: (x, 1)), \n",
    " (lambda x, y: (x[0] + y, x[1] + 1)), \n",
    " (lambda x, y: (x[0] + y[0], x[1] + y[1])) \n",
    " )\n",
    "avg_trips_by_hour = avg_trips_by_hour.mapValues(lambda v : v[0] / v[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) `collect()` the results.\n",
    "Finally, we can collect our results and sort them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_trips_sorted = sorted(avg_trips_by_hour.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) [Bonus] Using the Spark SQLContext.\n",
    "\n",
    "Besides the SparkContext, Spark also exposes a SQLContext that allows us to perform SQL queries on an RDD object.\n",
    "\n",
    "A SQLContext is also already created for you. Donâ€™t create another or unspecified behavior may occur. As you can see below, the SQLContext provided is a HiveContext.\n",
    "\n",
    "**Run a query using the SQLContext to obtain the average duration of a trip originating from the Caltrain station.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tripsSql = sqlContext.read.format('com.databricks.spark.csv').options(header='true',\n",
    "                                                                      inferschema='true').load('file:///home/vagrant/data/201408_babs_open_data/201408_trip_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tripsSql.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register this DataFrame as a table.\n",
    "tripsSql.registerTempTable(\"tripsSql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "SELECT\n",
    "    hour,\n",
    "    COUNT(1) AS c,\n",
    "    ROUND(AVG(duration) / 60) AS avg_duration\n",
    "FROM (\n",
    "    SELECT\n",
    "        CAST(SPLIT(SPLIT(t.startdate, ' ')[1], ':')[0] AS INT) AS hour,\n",
    "        t.duration AS duration\n",
    "    FROM \"tripsSql\" t\n",
    "    WHERE\n",
    "        t.startterminal = 70\n",
    "        AND\n",
    "        t.duration IS NOT NULL\n",
    "    ) r\n",
    "GROUP BY hour\n",
    "ORDER BY hour ASC;\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
