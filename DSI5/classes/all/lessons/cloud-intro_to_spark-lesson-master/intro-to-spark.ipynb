{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" href=\"//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" integrity=\"sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u\" crossorigin=\"anonymous\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" href=\"//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" integrity=\"sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u\" crossorigin=\"anonymous\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Intro to Spark\n",
    "\n",
    "_Authors: David Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/ieVW98.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Identify major data types used in Spark.\n",
    "- Identify basic operations for data munging in Spark.\n",
    "- Understand the Spark computation model.\n",
    "- Use Spark UI to view running \"jobs.\"\n",
    "- Build simple queries and transformations for Spark via Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Pre-Work\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Install and configure the latest version of Apache Spark.\n",
    "- Configure Jupyter Notebook with SparkContext.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [What is Spark?](#intro)\n",
    "    - [Spark Is a Distributed Framework for Parallelized Applications](#dist)\n",
    "    - [Spark Is an API](#api)\n",
    "    - [Spark Is Machine Learning](#ml)\n",
    "    - [Spark Is a Framework for Building High-Volume Stream Processors](#stream)\n",
    "    - [Spark Is a SQL Interface](#sql)\n",
    "    - [Spark Is Parallel Graph Processing](#graph)\n",
    "- [Programming With Spark](#prog)\n",
    "- [Spark UI](#sparkui)\n",
    "- [Using Spark via PySpark](#using)\n",
    "    - [Spark Installation Guide](#guide)\n",
    "    - [The SparkContext](#spark-context)\n",
    "- [RDDs vs. DataFrames](#rdd)\n",
    "- [Transformations and Actions](#ta)\n",
    "    - [Common Spark Transformations](#common-transformations)\n",
    "    - [Common Spark Actions](#common-actions)\n",
    "- [Spark Data Types](#dtypes)\n",
    "    - [Resilient Distributed Data Set](#rdds)\n",
    "    - [DataFrames](#df)\n",
    "- [Common DataFrame Operations and Characteristics](#common-df)\n",
    "- [Some Basic Statistics in Spark](#stats)\n",
    "- [Limiting Results](#limiting)\n",
    "- [More DataFrame and Series Operations](#more-ops)\n",
    "- [Activity: Check Out Another Data Set Using Spark](#activity)\n",
    "- [Practical Tips](#practical-tips)\n",
    "- [Additional Resources](#additional-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## What Is Spark?\n",
    "---\n",
    "\n",
    "**How is it different and similar to:**\n",
    "\n",
    "- Hadoop?\n",
    "- MapReduce?\n",
    "- Random forests?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is This \"Spark\" You Speak of?\n",
    "\n",
    "Apache Spark is an open-source cluster computing framework.\n",
    "\n",
    "Spark provides an interface for programming entire computer clusters with implicit data parallelism and fault tolerance.\n",
    "\n",
    "Spark has gained traction over the past few years because of its superior performance with respect to Hadoop-MapReduce.\n",
    "\n",
    "Spark relaxes the constraints of MapReduce by doing the following:\n",
    "\n",
    "- It generalizes computation from MapReduce by only graphing to arbitrary directed acyclic graphs (DAGs).\n",
    "- It removes a lot of the boilerplate code present in Hadoop.\n",
    "- It can \"tweak\" things that are not accessible in Hadoop (e.g., the sort algorithm).\n",
    "- It can load data in a cluster memory, greatly speeding up I/O. \n",
    "\n",
    "![](https://snag.gy/4oxeiA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Spark Core**: Contains the basic functionality of Spark; in particular, the APIs that define resilient distributed data sets (RDDs) and the operations and actions that can be undertaken upon them. The rest of Spark's libraries are built on top of the RDD and Spark Core.\n",
    "\n",
    "- **Spark SQL**: Provides APIs for interacting with Spark via the Apache Hive variant of SQL called Hive query language (HiveQL). Every database table is represented as an RDD, and Spark SQL queries are transformed into Spark operations. For those that are familiar with Hive and HiveQL, Spark can act as a drop-in replacement.\n",
    "\n",
    "- **Spark Streaming**: Enables the processing and manipulation of live streams of data in real time. Many streaming data libraries (such as Apache Storm) exist for handling real-time data. Spark Streaming enables programs to leverage this data similarly to how you would interact with a normal RDD as data are flowing in.\n",
    "\n",
    "- **MLlib**: A library of common machine learning algorithms implemented as Spark operations on RDDs. This library contains scalable learning algorithms, such as classifications and regressions, that require iterative operations across large data sets. The Mahout library — formerly the big data machine learning library of choice — will move to Spark for its implementations in the future.\n",
    "\n",
    "- **GraphX**: A collection of algorithms and tools for manipulating graphs and performing parallel graph operations and computations. GraphX extends the RDD API to include operations for manipulating graphs, creating sub-graphs, or accessing all vertices in a path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Use Spark?\n",
    "\n",
    "Imagine you have to run a linear regression over several terabytes of data. A few obvious problems arise:\n",
    "\n",
    "- It's hard to find a place to store all of that data.\n",
    "- It would take a painfully long time to compute the regression.\n",
    "\n",
    "Spark helps solve these problems by allowing us to distribute the data over several computers and use main memory to speed up the computation. Main memory runs about 1,000 times faster than hard disk, but real performance is more complex than just those two components.\n",
    "\n",
    "Spark is also often used for streaming data. Perhaps a company needs instant information about a large number of incoming tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Is Many Things  \n",
    "\n",
    "- It's a set of tools for developing applications.  \n",
    "- It's the parallel processing of applications in a distributed environment.\n",
    "\n",
    "![](https://snag.gy/c9b1Kx.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dist'></a>\n",
    "### Spark Is a Distributed Computer Framework for Parallelized Applications Like _Hadoop_\n",
    "\n",
    "_Spark can interact with Hadoop's HDFS to access large amounts of data using high-volume, distributed I/O._\n",
    "\n",
    "![](https://snag.gy/s8gSlG.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='api'></a>\n",
    "### Spark Is an API for Handling Large Data Transformations Like _MapReduce_\n",
    "\n",
    "_Spark is a data transformation and selection tool similar to **Pandas**. You can develop transformations through an API that allows you to chain operations together in a modular fashion._\n",
    "\n",
    "![](https://snag.gy/9G4gJO.jpg)\n",
    "\n",
    "> However, Spark delivers on the idea of data manipulation through the framework of **transformations** and **actions**. These transformations and actions are performed parallely using many different worker nodes (which may be distributed on multiple machines).\n",
    "\n",
    "> MapReduce is broken down into two main functions: map and reduce. Spark, on the other hand, runs through a set of operations determined through a **directed acyclic graph** (DAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ml'></a>\n",
    "### Spark Is Machine Learning Like _Scikit-Learn_\n",
    "\n",
    "![](https://snag.gy/RnuX6h.jpg)\n",
    "\n",
    "_Spark provides an interface to MLib via Scala, Java, Python, and R. The most common methods are provided, including regression, support vector machines, and random forests. However, not all evaluation metrics are available in Python yet. Spark is written in Scala, so features are prioritized to Scala first throughout the Spark ecosystem._\n",
    "\n",
    "> <i class=\"fa fa-question-circe\"></i> Does anyone remember specific limitations? (You might not if you haven't done MLlib with Spark yet.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stream'></a>\n",
    "### Spark Is a Framework for Building High-Volume Stream Processors\n",
    "![](https://snag.gy/RCikuU.jpg)\n",
    "\n",
    "With Spark Streaming, it's possible to build a process that can respond to data in real time using any of Spark's features — including MLib, GraphX, or any transformations you could do within the SparkContext. The streaming capabilities of Spark Core make it possible to produce real-time applications, such as ETL, analytics dashboards, data mining, or large-scale aggregations.\n",
    "\n",
    "In short, you can create a \"streaming context\" that listens to a specific port and tie it to any number of operations that can be programed with Spark.\n",
    "\n",
    ">```python\n",
    "># Save this as a file called \"network_streaming.py.\"\n",
    ">from pyspark import SparkContext\n",
    ">from pyspark.streaming import StreamingContext\n",
    ">\n",
    "># Create a local StreamingContext with two working threads and a batch interval of one second.\n",
    ">sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    ">ssc = StreamingContext(sc, 1)\n",
    ">\n",
    "># Create a DStream that will connect to hostname:port (i.e., localhost:9999).\n",
    ">lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    ">\n",
    "># Split each line into words.\n",
    ">words = lines.flatMap(lambda line: line.split(\" \"))\n",
    ">\n",
    "># Count each word in each batch.\n",
    ">pairs = words.map(lambda word: (word, 1))\n",
    ">wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    ">\n",
    "># Print the first 10 elements of each RDD generated in this DStream to the console.\n",
    ">wordCounts.pprint()\n",
    ">\n",
    ">ssc.start()             # Start the computation.\n",
    ">ssc.awaitTermination()  # Wait for the computation to terminate.\n",
    ">```\n",
    "\n",
    "Then, you can have Spark run this stream using the UNIX utility **netcat** to route your service once you run `spark-submit`.\n",
    "\n",
    ">```bash\n",
    ">$ nc -lk 9999\n",
    ">```\n",
    "\n",
    "Running `spark-submit` launches applications onto your Spark cluster.\n",
    "\n",
    ">```bash\n",
    ">./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sql'></a>\n",
    "### Spark Is a SQL Interface Into DataFrames Like _Hive_\n",
    "\n",
    "Spark isn't actually **Hive**, but it uses components from it. However, you can use Spark DataFrames more easily with temporary SQL views.\n",
    "\n",
    ">```python\n",
    "># Load a data set as a Spark DataFrame.\n",
    ">df = spark.read.csv(\"datasets/somedataset/hamburgers_eaten_per_hour.csv\")\n",
    ">df.createOrReplaceTempView(\"hamburgers\")\n",
    ">```\n",
    "\n",
    "\n",
    "\n",
    "Then, viola! You can slice and dice your DataFrame as SQL:\n",
    "\n",
    ">```python\n",
    ">spark.sql(\"SELECT * FROM hamburgers\").show()\n",
    ">\n",
    "># +------+---------+\n",
    "># | eaten|     name|\n",
    "># +------+---------+\n",
    "># |null  |     Jeff|\n",
    "># |  30  |   Kiefer|\n",
    "># |  19  |     Hang|\n",
    "># +------+---------+\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='graph'></a>\n",
    "### Spark Is Also Parallel Graph Processing\n",
    "\n",
    "![](https://snag.gy/0KvrCw.jpg)\n",
    "\n",
    "Graph processing provides the ability to process data in terms of relationships. Traditional relational database management systems (RDBMS) fail to calculate relationships between entities that don't have a predetermined depth between sets.  \n",
    "\n",
    "Using graphs, you can represent irregular relationship shapes within data and apply functions recursively and/or arbitrarily on any depth of relationships.\n",
    "\n",
    "Both RDBMS and graphs can be used to equate the other's feature, but each have their strengths and trade-offs, depending on the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prog'></a>\n",
    "## Programming With Spark\n",
    "---\n",
    "\n",
    "There are a variety of ways to run application builds with Spark, including:\n",
    "\n",
    "- PySpark\n",
    "- Spark-submit\n",
    "\n",
    "### PySpark\n",
    "PySpark is a real-time interpreter that communicates between Spark to Python and allows you to integrate operations with the Spark Python libraries. This is a great way to prototype applications much like we would with Jupyter Notebook. It's also possible to connect PySpark to Jupyter.\n",
    "\n",
    "### Spark-Submit\n",
    "Spark-submit, on the other hand, is a way to run a set of application instructions bundled in a single file. It's possible to write these in Python, Scala, or Java. Typically, it's convenient to prototype a set of operations that you want performed on a data set with PySpark, debug them to a high level of quality, and then run them with spark-submit.\n",
    "\n",
    "> With spark-submit, it's also possible to tune your application's parameters to a granular degree, including memory, number of total cores, and specific cluster modes to control test versus production deployments.\n",
    ">\n",
    "> [Read more about submitting applications](http://spark.apache.org/docs/latest/submitting-applications.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sparkui'></a>\n",
    "## Spark UI\n",
    "---\n",
    "\n",
    "Anytime a SparkContext is created, a corresponding Spark UI is launched. Whenever you launch a Spark standalone instance — PySpark — a Spark UI will be created. Then, through the web UI, you can monitor how your applications run. Anything that the SparkContext handles, even the one line operations from PySpark, can be observed as a separate job in Spark UI.\n",
    "\n",
    "### Spark Jobs\n",
    "![](https://snag.gy/JnuSKC.jpg)\n",
    "\n",
    "### Job Metrics\n",
    "![](https://snag.gy/JW2fOb.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='using'></a>\n",
    "## Using Spark via PySpark\n",
    "---\n",
    "\n",
    "If you haven't done so yet, it's nice to be able to install Spark manually to get familiar with how to use it locally. The advantage of this, as opposed to using a virtual machine, is that you should be able to run Spark without the overhead of running an additional virtual operating system.\n",
    "\n",
    "<a id='guide'></a>\n",
    "### Spark Installation Guide\n",
    "> This is a good time to review or install Spark using our guide:\n",
    ">\n",
    ">[Spark Installation Guide](./spark-installation-guide.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='spark-context'></a>\n",
    "### PySpark Is All About SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext, **`sc`**, represents your interface to a running Spark cluster manager. A SparkContext is defined as a preconfigured cluster with an application name connected to it. All **transformations** and **actions** performed by Spark are handled through the SparkContext.\n",
    "\n",
    ">**Context for standalone Spark apps:**\n",
    ">\n",
    ">If we were to write a standalone app for spark-submit, we would need to create the context manually.\n",
    "\n",
    ">```python\n",
    ">from pyspark import SparkContext\n",
    ">sc = SparkContext(\"local\", \"Simple App\")\n",
    ">```\n",
    "\n",
    ">Otherwise, it's already created for us as a result of running **`pyspark`** from the shell, as well as in PySpark and with a connected Jupyter Notebook.\n",
    "\n",
    "<img src=\"https://snag.gy/iCm4G1.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rdd'></a>\n",
    "## RDDs vs. DataFrames\n",
    "\n",
    "---\n",
    "\n",
    "The two main types of data objects in Spark are the **resilient distributed dataset (RDD)** and the **DataFrame**. Both types represent data in a distributed state. RDDs store data in a more primitive state, such as a list of pairs, integers, floats, or strings. DataFrames have a rich structure definition called a **schema**, much like a Pandas DataFrame.\n",
    "\n",
    "- You use **RDDs** to manage semi-structured data.\n",
    "- You use **DataFrames** to operate on a typed series.\n",
    "\n",
    "Both RDDs and DataFrames can contain multiple types of objects. **DataFrames** are much more constrained because data are illustrated by a two-dimensional tabular structure in which columns represent variables and rows represent observations. **RDDs** are much more flexible if your data require less structure than a DataFrame but you still need to be able to use _transformation_ methods (i.e., `map()`) and _action_ methods (i.e., `reduce()`).\n",
    "\n",
    ">_Distributed data in Spark:_\n",
    ">![](https://snag.gy/vxVhri.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta'></a>\n",
    "\n",
    "## _Transformations_ and _Actions_\n",
    "\n",
    "---\n",
    "\n",
    "<a id='common-transformations'></a>\n",
    "### <i class=\"fa fa-cogs\" aria-hidden=\"true\"></i> Common Spark Transformations\n",
    "\n",
    "\n",
    "Generally, **transformations** in Spark modify data as new copies of whichever data set is used as input. The default documents refer to _map_ as a **transformation** and _reduce_ as an **action**. **Transformations are lazy**, so no computations are performed until an **action** is requested.\n",
    "\n",
    "<style>\n",
    ".no-border:table, th, td {\n",
    "    border: none;\n",
    "    border-left: none;\n",
    "    margin: 10px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<table class=\"no-border\" style=\"border: none;\">\n",
    "<tbody class=\"no-border\" style=\"border: none;\"><tr class=\"no-border\" style=\" background: #000000; color: #FFFFFF;\"><th>Transformation</th><th style=\"border: none; text-align: center;\">Meaning</th></tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border: none;\"> <b>map</b>(<i>func</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Returns a new distributed data set formed by passing each element of the source through a function. <i>func</i>. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>filter</b>(<i>func</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Returns a new data set formed by selecting those elements of the source on which <i>func</i> returns true. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>sample</b>(<i>withReplacement</i>, <i>fraction</i>, <i>seed</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Samples a <i>fraction</i> of the data with or without replacement using a given random number generator seed. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>distinct</b>([<i>numTasks</i>])) </td>\n",
    "  <td style=\"border-right: none;\"> Returns a new data set that contains the distinct elements of the source data set.</td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>groupByKey</b>([<i>numTasks</i>]) <a name=\"GroupByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on a data set of key-value, (K, V), pairs, returns a data set of (K, Iterable&lt;V&gt;) pairs. <br>\n",
    "    <b>Note:</b> If you're grouping in order to perform an aggregation (such as a sum or\n",
    "      average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield a much better\n",
    "      performance.\n",
    "    <br>\n",
    "    <b>Note:</b> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.\n",
    "      You can pass an optional <code>numTasks</code> argument to set a different number of tasks.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>reduceByKey</b>(<i>func</i>, [<i>numTasks</i>]) <a name=\"ReduceByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on a data set of (K, V) pairs, returns a data set of (K, V) pairs where the values for each key are aggregated using the given reduce function <i>func</i>, which must be of type (V, V) =&gt; V. The number of reduce tasks is configurable through an optional second argument, as with <code>groupByKey</code>. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>aggregateByKey</b>(<i>zeroValue</i>)(<i>seqOp</i>, <i>combOp</i>, [<i>numTasks</i>]) <a name=\"AggregateByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on a data set of (K, V) pairs, returns a data set of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral zero value. Allows an aggregated value type that is different than the input value type while avoiding unnecessary allocations. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>sortByKey</b>([<i>ascending</i>], [<i>numTasks</i>]) <a name=\"SortByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on a data set of (K, V) pairs where K implements ordered, returns a data set of (K, V) pairs sorted by keys in ascending or descending order, as specified in the Boolean <code>ascending</code> argument.</td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>join</b>(<i>otherDataset</i>, [<i>numTasks</i>]) <a name=\"JoinLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on data sets of type (K, V) and (K, W), returns a data set of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "    Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>pipe</b>(<i>command</i>, <i>[envVars]</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Pipes each partition of the RDD through a shell command (e.g., a Perl or Bash script). RDD elements are written to the process' `stdin`, and lines that output to its `stdout` are returned as an RDD of strings. </td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "\n",
    "> <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> For more info on **transformations**, check out the [<i class=\"fa fa-external-link\" aria-hidden=\"true\"></i> Spark programming guide on #transformations](http://spark.apache.org/docs/latest/programming-guide.html#transformations).\n",
    "\n",
    "> A great visual guide for common Spark transformations is available from [<i class=\"fa fa-external-link\" aria-hidden=\"true\"></i> Jeff Thompson at Databricks](http://training.databricks.com/visualapi.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='common-actions'></a>\n",
    "### <i class=\"fa fa-wrench\" aria-hidden=\"true\"></i> Common Spark Actions\n",
    "\n",
    "Spark **actions** aren’t performed until they are called — sounds like common sense, right? However, be careful, because in _Pandas_ we're accustomed to getting results immediately after most transformations are performed. In Spark, even if we have a few operations programmed out, nothing will actually be transformed until we call an **action**.\n",
    "\n",
    "```python\n",
    "# 1) Read text file.\n",
    "#########\n",
    "# This line reads a text file from the file system. Each line becomes an element in an RDD.\n",
    "# No operations have been performed yet. We don't know if the text file actually exists or if it read any data.\n",
    "#############################################################################################################\n",
    "\n",
    "text_lines = sc.textFile(\"somefile.txt\")\n",
    "\n",
    "# 2) Map lengths of each line.\n",
    "#########\n",
    "# Once data are available, we simply count the length of lines (length of string) and create a new RDD,\n",
    "# which only has the length of each line as a new object.\n",
    "#############################################################################################################\n",
    "\n",
    "text_line_lengths = text_lines.map(lambda s: len(s))\n",
    "\n",
    "# 3) Reduce to find the total sum of lengths.\n",
    "#########\n",
    "# Once reduce is called, all prior operations are run, meaning we actually run sc.textFile() \n",
    "# and text_lines.Map() before finally running text_line_lengths.reduce().\n",
    "#############################################################################################################\n",
    "\n",
    "total_length = text_line_lengths.reduce(lambda a, b: a + b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table\" style=\"border-left: none; border-right: none;\">\n",
    "<tbody style=\"border-left: none;\"><tr style=\"background: black; color: white;\"><th>Action</th><th style=\"text-align: center;\">Meaning</th></tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>reduce</b>(<i>func</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Aggregates the elements of the data set using a function, <i>func</i> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>collect</b>() </td>\n",
    "  <td style=\"border-right: none;\"> Returns all of the elements of the data set as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>count</b>() </td>\n",
    "  <td style=\"border-right: none;\"> Returns the number of elements in the data set. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>first</b>() </td>\n",
    "  <td style=\"border-right: none;\"> Returns the first element of the data set (similar to `take(1)`). </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>take</b>(<i>n</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Returns an array with the first <i>n</i> elements of the data set. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>takeSample</b>(<i>withReplacement</i>, <i>num</i>, [<i>seed</i>]) </td>\n",
    "  <td style=\"border-right: none;\"> Returns an array with a random sample of <i>num</i> elements from the data set — with or without replacement — optionally pre-specifying a random number generator seed.</td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>takeOrdered</b>(<i>n</i>, <i>[ordering]</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Returns the first <i>n</i> elements of the RDD using either their natural order or a custom comparator. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>saveAsTextFile</b>(<i>path</i>) </td>\n",
    "  <td style=\"border-right: none; border-right: none;\"> Writes the elements of the data set as a text file (or set of text files) in a given directory in the local file system, HDFS, or any other Hadoop-supported file system. Spark will call `toString()` on each element to convert it to a line of text in the file. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>countByKey</b>() <a name=\"CountByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> Only available on RDDs of type (K, V). Returns a HashMap of (K, Int) pairs with the count of each key. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>foreach</b>(<i>func</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Runs a function <i>func</i> on each element of the data set. This is usually done for side effects such as updating an <a href=\"#accumulators\">accumulator</a> or interacting with external storage systems.\n",
    "  <br><b>Note</b>: Modifying variables other than accumulators outside of <code>foreach()</code> may result in undefined behavior. See <a href=\"#understanding-closures-a-nameclosureslinka\">understanding closures </a> for more details.</td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "\n",
    "> <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> For more information on **actions**, check out the [<i class=\"fa fa-external-link\" aria-hidden=\"true\"></i> Spark programming guide on #actions](http://spark.apache.org/docs/latest/programming-guide.html#actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <i class=\"fa fa-question-circle\" aria-hidden=\"true\"></i>   How Might You Approach Programming With Spark vs. Pandas? (~1 Minute)\n",
    "\n",
    "_Given that the operations are lazy?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dtypes'></a>\n",
    "## Spark Data Types\n",
    "---\n",
    "\n",
    "### RDDs\n",
    "\n",
    "It's best to think of RDDs as primitive, distributed objects. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes. The RDD type is the oldest type and has been around since the first version of Spark. \n",
    "\n",
    "> A handy trick: Because RDDs can be Python class types, it's possible to do things like run scikit-learn's `GridSearch` over an existing scikit-learn model and a parameter search over a particular amount of permutations in a clustered Spark set up.\n",
    "\n",
    "### DataFrames\n",
    "\n",
    "As part of the \"Tungsten Initiative,\" which sought to improve the performance of Spark, DataFrames entered the Spark code base in version 1.3. The big difference between RDDs and DataFrames is that DataFrames introduce the idea of a \"schema,\" much like Pandas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rdds'></a>\n",
    "<span style=\"font-size: 25pt; font-weight: light;\"><strong>R</strong>eslient <strong>D</strong>istributed <strong>D</strong>ata set</span>\n",
    "<hr>\n",
    "\n",
    "\n",
    "Everything in Spark revolves around the **RDD**.\n",
    "\n",
    "> \"...a fault-tolerant collection of elements that can be operated on in parallel...\"\n",
    "> _— Spark documentation_\n",
    "\n",
    "Most data sets that can be loaded externally (CSV, HDFS/Hadoop, text files, RDBMS/SQL, etc.) become an RDD and are operated in parallel within a distributed system (Spark). Otherwise, you can `sc.parallelize(my_data)` a data set through a SparkContext (sometimes called a driver program)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD: More examples with RDDs. We’ll explore DataFrames further for now, as they are more relatable.\n",
    "# RDDs are great for custom jobs and operations that require a more open format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check out this grid search with Spark package on [<i class=\"fa fa-external-link\" aria-hidden=\"true\"></i> GitHub: Spark-Scitkit-Learn](https://github.com/databricks/spark-sklearn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df'></a>\n",
    "### DataFrames\n",
    "\n",
    "Spark DataFrames serialize their data at a lower-level native to Java/Scala. So, when a DataFrame is passed between nodes, it's much more performant, requiring fewer processes to handle computations. Data can be processed faster when optimized to a common format (the schema) that Spark doesn't have to convert to in order to perform tasks.\n",
    "\n",
    "Outside of the performance optimizations introduced with a schema-based data structure, the **DataFrame API** provides a convenient set of selectors for transforming data, much like Pandas. Lastly, it's possible to create temporary views in which **DataFrames** can be queried with SQL (**SparkSQL**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = spark.read.csv(\n",
    "#     path        =   \"../../../datasets/sentiment_words/sentiment_words_simple.csv\",\n",
    "#     header      =   True, \n",
    "#     mode        =   \"DROPMALFORMED\",   # Poorly formed rows in CSV are dropped rather than causing an error for the entire operation.\n",
    "#     inferSchema =   True               # It's not always perfect, but it works well in most cases as of version 2.1+.\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='common-df'></a>\n",
    "## Common DataFrame Operations and Characteristics\n",
    "---\n",
    "\n",
    "Let's take a look at some familiar and new functions and properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Variable/Column Space of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "Inspect the schema programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain DataFrame\n",
    "Show details about DataFrame type, schema, and origin of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe\n",
    "Describe will look similar to the synonymous Pandas **describe** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Check the Pandas version here:\n",
    "# df.toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printSchema\n",
    "The schema is an important characteristic of a Spark DataFrame. It tells us what's possible in terms of transformation. It's also the reason DataFrames are so fast, as they're tied to a set number of classes that are serialized and optimized in Java/Scala behind the scenes.\n",
    "\n",
    "><i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i> The schema that we've been so excited to see is finally here to explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count\n",
    "Count with caveat: This will return the count of all rows, including _non-NaN_ values. Pandas will omit these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stats'></a>\n",
    "## Some Basic Statistics in Spark\n",
    "---\n",
    "\n",
    "### Covariance\n",
    "<span style=\"font-size: 20pt;\">\n",
    "$\\operatorname{cov}(X,Y) = \\operatorname{E}{\\big[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])\\big]}$\n",
    "</span>\n",
    "> \"Covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values — i.e., the variables tend to show similar behavior — the covariance is positive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.cov(\"pos_score\", \"neg_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Score\n",
    "<span style=\"font-size: 20pt;\">\n",
    "$\\rho_{X,Y}= \\frac{\\operatorname{cov}(X,Y)}{\\sigma_X \\sigma_Y} $\n",
    "</span>\n",
    "> The normalized version of the covariance — the correlation coefficient — however, shows by its magnitude the strength of the linear relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.corr(\"pos_score\", \"neg_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='limiting'></a>\n",
    "## Limiting Results\n",
    "\n",
    "---\n",
    "\n",
    "### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Limited Results\n",
    "\n",
    "With Pandas, we're used to the `df.head()` being a first step in exploring a data set. With Spark, this isn't exactly the case. You need to use the `df.show()` operation to explore data as a first step. While Pandas formats its DataFrame output for display in HTML tables with sensible defaults for output, when you're using Spark, you have to be a bit more specific about what you're looking at with `show()`.\n",
    "\n",
    "\n",
    "> The `truncate` parameter is helpful for truncating attributes for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i class=\"fa fa-question-circle\" aria-hidden=\"true\"></i> Why Do You Need to Be Careful When Displaying Data in Spark?\n",
    "\n",
    "Hopefully you can see why Pandas is so useful for EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='more-ops'></a>\n",
    "## More DataFrame and Series Operations\n",
    "---\n",
    "\n",
    "### Convert From a List of Row Objects to a List of Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting DataFrame Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting variables with Spark's **DataFrames API** works similarly to Pandas DataFrames. When selecting variables in Pandas, it uses a `list` object passed to a DataFrame object via `[]` brackets, like so:\n",
    "\n",
    ">```df[['col1', 'col2']]```\n",
    "\n",
    "The equivalent in Spark is using the `.select()` method, which takes flat parameters.\n",
    "\n",
    "> <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> DataFrames in Pandas are implemented with an overloaded brackets operator, so whenever we reference any Pandas DataFrames object with brackets, it gets passed down to a function that handles the column references as the input to a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select All Features/Variables/Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Specific Features/Variables/Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series Operations\n",
    "\n",
    "With Pandas, you can easily create a new series that's the sum of every row in **\"col1\"** with **\"col2”**, like so:\n",
    "\n",
    "> `df['col1'] + df['col2']`\n",
    "\n",
    "In Spark, we have to do this through the `select()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As an \"Alias\"\n",
    "Selections can become hard to read, as they are keyed by conditions. We can use an \"alias\" to abstract any selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating New Features/Variables/Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-question-circle\"></i> Have We Changed the Original DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Data\n",
    "In Pandas, we use masks through DataFrame object brackets in order to filter data.\n",
    ">`df[df['feature'] > 0]`\n",
    "\n",
    "In Spark, we use the `filter()` method to select different aspects of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Conditions\n",
    "This is actually the same in Spark as it is in Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter as an Expression\n",
    "Pandas has a similar function called \"where.\" However, with Spark `filter()`, we can filter using shorthand expressions when referencing column sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As SQL?\n",
    "\n",
    "\n",
    "Working with DataFrames as SQL is as easy as creating a **temporary view**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary Views Select DataFrames\n",
    "The same transformations we just learned can be applied to DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='activity'></a>\n",
    "## Activity: Check Out Another Data Set Using Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Load up the \"Pokemon\" basic Pokedex data set.\n",
    "First, try this without inferring the schema and without the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Check out the data set with the` inferSchema` parameter but without `header`.\n",
    "How does it work with/without the schema parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Create a temporary view with the Pokedex DataFrame called \"pokemon.\"\n",
    "Then \n",
    "```sql SELECT * FROM pokemon LIMIT 10```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.A) Which is the strongest Pokemon by `Type`?\n",
    "Research Spark's \"grouping\" functions using the Spark DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.B) Which is the strongest Pokemon by `Type`?\n",
    "Using the Spark SQL temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.A) Which Pokemon has the best combined attack and defense?\n",
    "Using Spark DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.B) Which Pokemon has the best combined attack and defense?\n",
    "Using the Spark SQL temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Create a new feature called \"Pokevalue\" that is the combined attack and defense scaled by .2 of the Pokemon HP.\n",
    "Use any means necessary to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practical-tips'></a>\n",
    "# <i class=\"fa fa-thumbs-up\" aria-hidden=\"true\"></i> Practical Tips\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling From Enormous Data Sets\n",
    "Undoubtedly, you may need to examine a larger data set. A common operation is to take a sample. To approximate the characteristics of your global distribution, you should try to adjust the size that best matches the metrics of central tendency or consider performing a power analysis to determine sample sizing.\n",
    "\n",
    "> The size of your sample generally depends on your application, be it A/B testing, EDA, machine learning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>\n",
    "## Additional Resources\n",
    "\n",
    "- [Qubole: Apache Spark Use Cases](https://www.qubole.com/blog/big-data/apache-spark-use-cases/)\n",
    "- [Spark Examples](http://spark.apache.org/examples.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
