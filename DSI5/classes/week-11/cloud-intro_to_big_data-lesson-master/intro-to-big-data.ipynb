{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> **Jupyter slideshow:** This notebook can be displayed as slides. To view it as a slideshow in your browser, type the following in the console:\n",
    "\n",
    "\n",
    "> `> ipython nbconvert [this_notebook.ipynb] --to slides --post serve`\n",
    "\n",
    "\n",
    "> To toggle off the slideshow cell formatting, click the `CellToolbar` button, then `View --> Cell Toolbar --> None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Intro to Big Data\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/SZOEv2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Student Pre-Work\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Run Python scripts from the UNIX shell.\n",
    "- Recall how the `cat` and `sort` UNIX commands work.\n",
    "- Download the VM link [here](https://www.dropbox.com/s/egzz6129w90okzf/GA%20DSI%20bigdata%200.9.ova?dl=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Objectives\n",
    "*By the end of this lesson, you will be able to:*\n",
    "- Recognize big data problems.\n",
    "- Explain how the MapReduce algorithm works.\n",
    "- Understand the difference between high performance computing and cloud computing.\n",
    "- Describe the divide and conquer strategy.\n",
    "- Perform a MapReduce on a single node using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Introduction](#intro)\n",
    "- [What is Big Data?](#big-data)\n",
    "- [High Performance Computing (HPC)](#hpc)\n",
    "- [Cloud Computing](#cloud)\n",
    "- [Parallelism](#parallelism)\n",
    "- [Divide and Conquer](#dc)\n",
    "- [MapReduce](#mapreduce)\n",
    "- [MapReduce: Key-Value Pairs](#kv-pairs)\n",
    "- [Guided Practice: Word Count on Paper](#guided-practice)\n",
    "    - [Simple MapReduce](#simple)\n",
    "- [Combiners](#combiners)\n",
    "- [MapReduce in Python](#python)\n",
    "    - [`mapper.py`](#mapper)\n",
    "    - [`reducer.py`](#reducer)\n",
    "    - [Running the Code in Terminal](#terminal)\n",
    "- [Independent Practice](#ind-practice)\n",
    "- [Conclusion](#conclusion)\n",
    "- [Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "This lesson identifies some major trends in the field of big data and data infrastructure, including common tools and problems you may encounter working as a data scientist. \n",
    "\n",
    "It's time to take the tools you've learned to a new level by increasing the size of the data sets you can tackle.\n",
    "\n",
    "\n",
    "<img src=\"https://snag.gy/mDzP4d.jpg\" style=\"height: 300px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Do You Think Big Data Is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **Big data is a hot topic. It refers to techniques and tools that allow us to store, process, and analyze large scale (multi-terabyte) data sets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can You Think of Any Data Sets That Would Be Considered Big Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Facebook social graphs.\n",
    "- Netflix movie preferences.\n",
    "- Large recommender systems.\n",
    "- The activity of visitors to a website.\n",
    "- Customer activity in a retail store (i.e., Target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Challenges Exist With Such Large Amounts of Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Processing time.\n",
    "- Cost\n",
    "- Architecture maintenance and set up.\n",
    "- Difficulty in visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"big-data\"></a>\n",
    "## What is Big Data?\n",
    "---\n",
    "\n",
    "Big data is a term used for data that exceed the processing capacity of typical databases. We need a big data analytics team when data sets are large and growing quickly and we want to uncover hidden patterns, unknown correlations, and build models. \n",
    "\n",
    "**There are three main features in big data (the three Vs):**\n",
    "- **Volume**: Large amounts of data.\n",
    "- **Variety**: Different types of structured, unstructured, and multi-structured data.\n",
    "- **Velocity**: The need to be analyzed quickly.\n",
    "\n",
    "**David Yerrington's Fourth V (An Unofficial Big Data Tenet):**\n",
    "- **Value**: It's important to assess the business value of predictions, and understanding the underpinnings of cost versus benefit is even more essential in the context of big data. It's easy to misunderstand the three Vs without looking at the bigger picture and connecting the value of the business cases involved.\n",
    "\n",
    "![3v](./assets/images/3vbigdata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='hpc'></a>\n",
    "## High Performance Computing (HPC)\n",
    "---\n",
    "\n",
    "Supercomputers, or HPCs, are very expensive, powerful calculators used by researchers to solve complicated math problems.\n",
    "\n",
    "![supercomputer](./assets/images/supercomputer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can You Think of Advantages and Disadvantages of HPC Configurations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Advantages:**\n",
    "- Can perform very complex calculations.\n",
    "- Centrally controlled.\n",
    "- Useful for research and complicated math problems.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Expensive.\n",
    "- Difficult to maintain (self-managed or managed hosting both incur operations overhead).\n",
    "- Scalability is bounded (before big data, this would be medium data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='cloud'></a>\n",
    "## Cloud Computing\n",
    "---\n",
    "\n",
    "Instead of using one huge machine, what if we bought a bunch of commodity machines?\n",
    "\n",
    "> *Note: Commodity hardware is a term used in operations to describe mixed-server hardware, but it can also refer to the basic machines you would use in an office.*\n",
    "\n",
    "![Commodity hardware](https://snag.gy/fNYgt0.jpg)<center>*Actual AWS Datacenter*</center>.\n",
    "\n",
    "**Can you think of advantages and disadvantages of this configuration?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Advantages:**\n",
    "- Relatively cheaper.\n",
    "- Easier to maintain (as a user of the cloud system).\n",
    "- Scalability is unbounded (just add more nodes to the cluster).\n",
    "- A variety of turnkey solutions are available through cloud providers.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Complex infrastructure. \n",
    "- Subject matter expertise required to leverage lower-level resources within the infrastructure.\n",
    "- Mainly tailored for parallelizable problems.\n",
    "- Relatively small CPU power at the lowest level.\n",
    "- More input/output between machines.\n",
    "\n",
    "The term big data refers to the cloud computing case in which commodity hardware with unlimited scalability is used to solve highly parallelizable problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How Do You Think Many Computers Process Data?\n",
    "\n",
    "**How does this contrast with how you perform analysis on your laptop?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='parallelism'></a>\n",
    "## Parallelism\n",
    "---\n",
    "\n",
    "The conceptual foundation of big data processing is the idea that a problem can be computed by multiple machines in pieces simultaneously. Many resources are being used in parallel with each other.\n",
    "\n",
    "![](https://snag.gy/MknIN6.jpg)\n",
    "\n",
    "- Running multiple instances to process data.\n",
    "- Data can be subset and solved iteratively.\n",
    "- Sub-solutions can be solved independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='dc'></a>\n",
    "## Divide and Conquer\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/xh2mJA.jpg\">\n",
    "\n",
    "The divide and conquer strategy is a fundamental algorithmic technique for solving a task. Its steps are:\n",
    "\n",
    "1) Split the task into subtasks.\n",
    "2) Solve these subtasks independently.\n",
    "3) Recombine the subtask results into a final result.\n",
    "\n",
    "For a problem to be suitable for the divide and conquer approach, you must be able to break it into smaller independent subtasks. Many processes are suitable for this strategy, but there are also plenty that aren't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='mapreduce'></a>\n",
    "## MapReduce\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/XBgCOs.jpg\">\n",
    "\n",
    "**MapReduce** is a two-phase divide and conquer algorithm initially invented and publicized by Google in 2004. It involves splitting a problem into subtasks and processing these subtasks parallelly. MapReduce has two phases:\n",
    "\n",
    "1) The **mapper** phase.\n",
    "2) The **reducer** phase.\n",
    "\n",
    "In the **mapper phase**, data are split into chunks and the same computation is performed on each, while in the **reducer phase**, data are aggregated back to produce a final result.\n",
    "\n",
    "MapReduce uses a functional programming paradigm. The data processing primitives are mappers and reducers.\n",
    "\n",
    "- **Mappers**: Filter and transform data.\n",
    "- **Reducers**: Aggregate results.\n",
    "\n",
    "The functional paradigm is good for describing how to solve a problem but not for describing data manipulations (e.g., relational joins)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='kv-pairs'></a>\n",
    "## MapReduce: Key-Value Pairs\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/k2FCar.jpg\">\n",
    "\n",
    "Data are passed through the various phases of a **MapReduce pipeline** as key-value pairs.\n",
    "\n",
    "**What Python data structures could be used to implement a key-value pair?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- A **dictionary**.\n",
    "- A **tuple** of two elements.\n",
    "- A **list** of two elements.\n",
    "- A named **tuple**.\n",
    "\n",
    "To understand MapReduce, you need to always keep in mind that data are flowing through a pipeline as key-value pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Guided Practice: Word Count on Paper\n",
    "---\n",
    "\n",
    "Let's perform a simple MapReduce in class. Our task is to find the 10 most common words in the paragraph below.\n",
    "\n",
    "    1:  MapReduce is a programming model for large-scale distributed data processing.\n",
    "    3:  It is inspired by the map function and the reduce function of the functional\n",
    "    4:  programming languages such as Lisp, Haskell, or Python. One of the most\n",
    "    5:  important features of MapReduce is that it allows us to hide the low-level\n",
    "    6:  implementation such as message passing or synchronization from users and\n",
    "    7:  allows to split a problem into many partitions. This is a great way to make\n",
    "    8:  trivial parallelization of data processing without any need for\n",
    "    9:  communication between the partitions.\n",
    "    10: MapReduce became mainstream because of Apache Hadoop, which is an open-\n",
    "    11: source framework that was derived from Google's MapReduce paper.\n",
    "    12: MapReduce allows us to process massive amounts of data in a distributed\n",
    "    13: cluster. In fact, there are many implementations of the MapReduce\n",
    "    14: programming model. Some of them are shown in the following list. It is\n",
    "    15: important to say that MapReduce is not an algorithm; it is just a part\n",
    "    16: of a high performance infrastructure that provides a lightweight\n",
    "    17: way to run a program in a lot of parallel machines.\n",
    "    18:                From: Practical Data Analysis, Hector Cuesta, 2013\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple MapReduce\n",
    "\n",
    "**Instructions:**\n",
    "- Students will perform the mapper function.\n",
    "- Instructor will perform the reducer function.\n",
    "\n",
    "Each student will be assigned one line of text. You'll have to produce a list of key-value pairs `(word, 1)` to provide to the instructor. \n",
    "\n",
    "**Check:** What preprocessing should you perform on your tokens in order to improve the results?\n",
    "\n",
    "Example: The first line will produce this list:\n",
    "\n",
    "    (MapReduce, 1)\n",
    "    (is, 1)\n",
    "    (a, 1)\n",
    "    (programming, 1)\n",
    "    (model, 1)\n",
    "    (for, 1)\n",
    "    (large-scale, 1)\n",
    "    (distributed, 1)\n",
    "    (data, 1)\n",
    "    (processing, 1)\n",
    "\n",
    "Your instructor will then sort the key-value pairs, add up the `1`s for each word, and produce the counts.\n",
    "\n",
    "**Check:** What additional operation did the instructor perform in order to complete the aggregation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> ***Answer**: Ignore punctuation and transform all to lowercase.*\n",
    "\n",
    "---\n",
    "\n",
    "> *Instructor Notes:*\n",
    "*1) If there are more than 18 students, group the students to obtain 18 groups.*\n",
    "*2) If there are less than 18 students, give each student more than one line so that all of the lines are processed.*\n",
    "*3) Make sure that students hand in a list of key-value pairs where the key is the word and the value is `1`.*\n",
    "*4) There's no need to actually perform the count — here are the expected results:*\n",
    ">\n",
    ">        ('of', 10)\n",
    ">        ('a', 9)\n",
    ">        ('is', 8)\n",
    ">        ('the', 8)\n",
    ">        ('mapreduce', 7)\n",
    ">        ('to', 6)\n",
    ">        ('that', 4)\n",
    ">        ('it', 4)\n",
    ">        ('in', 4)\n",
    ">        ('data', 4)\n",
    "\n",
    "---\n",
    "\n",
    "> *The instructor had to shuffle the key-value pairs handed in by the students in order to find the common key and add up the corresponding values.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='combiners'></a>\n",
    "## Combiners\n",
    "---\n",
    "\n",
    "Combiners are intermediate reducers that are performed at the node level in a multi-node architecture.\n",
    "\n",
    "![](https://snag.gy/lFYfoC.jpg)\n",
    "\n",
    "When data are really large, we can distribute them to several mappers running on different machines. Sending a long list of `(word, 1)` pairs to the reducer node isn’t efficient. We can first aggregate at the mapper node level and send the result to the reducer. This is possible because aggregations are associative.\n",
    "\n",
    "**Let's repeat the previous exercise with a small change:**\n",
    "1) Divide the class into three groups. In each group, one student will be the combiner and the others will be the mappers.\n",
    "2) Let's split the text into three parts and give each group one part.\n",
    "3) Mapper students produce a list of `(word, 1)` pairs for each line they receive and hand the list to the combiner.\n",
    "4) Combiner students sort the lists and sum the counts for words that appear in each list.\n",
    "5) Finally, combiner students hand their list of counts to the instructor, who will combine the intermediate sums and produce the final result.\n",
    "\n",
    "**Check:** What changed?\n",
    "\n",
    "Congratulations! You just performed a MapReduce sum.\n",
    "\n",
    "**Check:** Can you think of other aggregation tasks that can be parallelized in this way?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> *Less message passing to the instructor.*\n",
    "\n",
    "---\n",
    "\n",
    "> *Answer:*\n",
    "*- Count, sum, and average.*\n",
    "*- Grep, sort, and inverted index.*\n",
    "*- Graph traversals and some ML algorithms.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"python\"></a>\n",
    "## MapReduce in Python\n",
    "---\n",
    "\n",
    "Now that we've performed a MapReduce by hand, let's try it in Python. Below, you’ll find the code for a simple mapper and reducer that calculate word count.\n",
    "\n",
    "Let's look at them in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='mapper'></a>\n",
    "### `mapper.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-7e116e44b788>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-7e116e44b788>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    print '%s\\t%s' % (word, 1)\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# mapper.py:\n",
    "import sys\n",
    "\n",
    "# Get text from the standard input.\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** What kind of input does `mapper.py` expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** What kind of output does `mapper.py` produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='reducer'></a>\n",
    "### `reducer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# reducer.py:\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# Input comes from STDIN.\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "    \n",
    "    # Try to count if the error is continue.\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # This IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it's passed to the reducer.\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# Don't forget to output the last word if necessary.\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** What kind of input does `reducer.py` expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** What kind of output does `reducer.py` produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='terminal'></a>\n",
    "### Running the Code in Terminal\n",
    "\n",
    "**You can find `mapper.py`, `reducer.py`, and some text input files in the `code` directory.**\n",
    "\n",
    "This code can be run using the following command from your terminal:\n",
    "\n",
    "```bash\n",
    "cat <input-file> | python mapper.py | sort -k1,1 | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** Can you explain what each of the four steps in the pipeline accomplishes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Cat**: Reads the file and streams it line by line.\n",
    "- **Mapper**.\n",
    "- **Sort**: Shuffles the mapper output to sort it by key so that counting is easier.\n",
    "- **Reducer**: Aggregates by word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** Can you figure out how our previous example *could* be represented in the diagram below?\n",
    "![map reduce word count](./assets/images/word_count_dataflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent Practice\n",
    "---\n",
    "\n",
    "Now that you have a basic word counter set up in Python, try to do some of the following:\n",
    "\n",
    "1) Process a much larger text file of your choice.\n",
    "    - For example, use a page from Wikipedia or a blog article. If you're really ambitious, you can take books from Project Gutenberg.\n",
    "2) Explore how the execution time scales with file size.\n",
    "3) Read [this article](http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html) to discover some powerful shell tricks. Learning to use the shell will save you time munging data on your file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "---\n",
    "\n",
    "In this lesson, we've learned about big data and the MapReduce process. MapReduce is an algorithm that works well for aggregations on very large data sets.\n",
    "\n",
    "**Check:** Now that you know how big data works, can you think of some more specific business applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Examples:**\n",
    "\n",
    "- For processing log files to find security breaches.\n",
    "- For processing medical records to assess spending.\n",
    "- For processing news articles to decide on investments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='resources'></a>\n",
    "### Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "- [Top 500 supercomputers](http://www.top500.org/lists/).\n",
    "- [Google MapReduce paper](http://research.google.com/archive/mapreduce.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
