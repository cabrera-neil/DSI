{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Gradient Descent in Scikit-Learn\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "Until now we've been using specific scikit-learn model classes such as `LinearRegression` and `LogisticRegression` to perform regression and classification. While these methods work well on smaller data sets with relatively few features, the process slows down to a crawl once you start getting into \"medium data.\" Plus, these methods take up so much memory that fitting them becomes mind-numbingly slow — especially on a laptop.\n",
    "\n",
    "Luckily, scikit-learn comes with stochastic gradient descent (SGD) solvers for regression and classification:\n",
    "- `SGDRegressor`\n",
    "- `SGDClassifier`\n",
    "\n",
    "Because of its ability to minimize the loss function iteratively on smaller portions of the data, the SGD solvers avoid the intense slowdown other models suffer on large data sets.\n",
    "\n",
    "> **Note:** The gradient descent solvers are flexible and can fit a variety of different model types not covered here. We highly recommend reading their documentation in detail.\n",
    "\n",
    "---\n",
    "\n",
    "### San Francisco Assessor Data\n",
    "\n",
    "This lab uses data on housing prices in San Francisco from the S.F. Assessor's Office — the set is already cleaned up.\n",
    "\n",
    "You can see that the set has 250,000 rows. When expanding this with dummy-coded categorical columns, it can become quite large. Be careful that you don't exceed the memory on your computer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import patsy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load the data.\n",
    "\n",
    "Examine the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = pd.read_csv('datasets/assessor_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Sample down the data.\n",
    "\n",
    "Even though this is already only a sample of the full assessor data set, you should sample the data down further for the sake of speed and your computer's memory.\n",
    "\n",
    "Use the `.sample()` function for Pandas DataFrames to subset this down to < 25,000 rows. \n",
    "\n",
    "Sampling down large data sets is a common procedure. Finding the optimal parameters with larger subsets of the data may change the hyperparameters and results and will get you closer to the best coefficients — but the returns become marginal at a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prop_samp = prop.sample(n=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Regression with stochastic gradient descent.\n",
    "\n",
    "Below are x, y data predicting value (housing price) from the remaining variables. There are ~75,000 rows with 170 columns.\n",
    "\n",
    "\n",
    "The `SGDRegressor` is general and flexible and can be customized with a variety of keyword arguments.\n",
    "\n",
    "**Arguments**\n",
    "- `loss`: `['squared_loss','huber', ...]`\n",
    "    - The `'squared_loss'` corresponds to solving a regression with the least squares loss. This is what you'll probably use, but there are other options. Huber loss is a \"robust\" regression loss.\n",
    "- `penalty`: `['none','l1','l2','elasticnet']`\n",
    "    - This defines the penalty on the regression you'd like to solve. The l1 and l2 are the lasso and ridge, while the elastic net is the combination of both.\n",
    "- `alpha`\n",
    "    - The regularization strength to be used with a chosen penalty. It's the same as in the lasso and ridge.\n",
    "- `l1_ratio`\n",
    "    - The mix of the lasso and ridge penalties when elastic net is chosen as the penalty.\n",
    "- `n_iter`\n",
    "    - The number of training epochs over the data. This is the number of passes that the gradient descent algorithm will make over the data to iteratively fit the weights (defaults to five).\n",
    "\n",
    "`SGDRegressor` is most often used in tandem with grid searching to find the optimal parameters for certain models. \n",
    "\n",
    "**It's up to you how you want to define the model. You should:**\n",
    "\n",
    "1) Choose a target to estimate (this should be continuous).\n",
    "    - Select predictors to use.\n",
    "    - Standardize your predictor matrix.\n",
    "    - Build a stochastic gradient descent solver to fit your model. You'll likely want to perform some kind of grid search to find the optimal parameters for your model.\n",
    "    - Describe the model selected through grid search and compare the performance to the baseline.\n",
    "    - Examine and interpret the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Classification with stochastic gradient descent.\n",
    "\n",
    "The `SGDClassifier` is very similar to the `SGDRegressor`. The main difference is that the loss functions are changed to regression loss functions.\n",
    "\n",
    "**Arguments**\n",
    "- `loss`: `['log', ...]`\n",
    "    - The `'log'` loss corresponds to solving a logistic regression classifier. This is what you'll probably use, but there are other options.\n",
    "- `penalty`: `['none','l1','l2','elasticnet']`\n",
    "    - This defines the penalty on the regression you'd like to solve. The l1 and l2 are the lasso and ridge, while the elastic net is the combination of both.\n",
    "- `alpha`\n",
    "    - The regularization strength to be used with a chosen penalty. It's the same as in the lasso and ridge.\n",
    "- `l1_ratio`\n",
    "    - The mix of the lasso and ridge penalties when elastic net is chosen as the penalty.\n",
    "- `n_iter`\n",
    "    - The number of training epochs over the data. This is the number of passes that the gradient descent algorithm will make over the data to iteratively fit the weights (defaults to five).\n",
    "\n",
    "Like `SGDRegressor`, `SGDClassifier` is most often used in tandem with grid searching to find the optimal parameters for certain models. \n",
    "\n",
    "**It's up to you how you want to define the model. You should:**\n",
    "\n",
    "1) Choose a target to classify (you may need to engineer one from existing variables).\n",
    "    - Calculate the baseline accuracy.\n",
    "    - Select predictors to use.\n",
    "    - Standardize your predictor matrix.\n",
    "    - Build a stochastic gradient descent solver to fit your model. You'll likely want to perform some kind of grid search to find the optimal parameters for your model.\n",
    "    - Describe the model selected through grid search and compare the performance to baseline.\n",
    "    - Examine and interpret the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
