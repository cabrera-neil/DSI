{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Walk Through of Standard EDA Procedure\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "This lesson uses Boston housing data to walk through a basic exploratory data analysis procedure, starting at the beginning with loading the data. \n",
    "\n",
    "Although in many — if not most cases — the EDA procedure will be considerably more involved, this should give you an idea of the basic workflow a data scientist would go through when taking a look at a new data set.\n",
    "\n",
    "**Note:** This lesson is strictly exploratory. We will not be formulating any hypotheses about the data or testing them. In many cases, you may have already formulated a hypothesis before even looking at your data, which could considerably affect your focus and choices in what to investigate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Description of the Boston Housing Data](#data_description)\n",
    "- [Loading the Data](#load_data)\n",
    "- [Describing the Basic Format of the Data and the Columns](#header)\n",
    "- [Dropping Unwanted Columns](#drop)\n",
    "- [Cleaning Corrupted Data](#clean)\n",
    "- [Counting Null Values and Dropping Rows](#drop_nulls)\n",
    "- [Renaming Columns](#rename)\n",
    "- [Describing Summary Statistics for Columns](#describe)\n",
    "- [Investigating Potential Outliers With Box Plots](#boxplots)\n",
    "- [Plotting All Variables Together](#plot_all)\n",
    "- [Standardizing Variables](#standardization)\n",
    "- [Plotting the Standardized Variables Together](#plot_all_rescaled)\n",
    "- [Looking at the Covariance or Correlation Between Variables](#cov_cor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_description'></a>\n",
    "\n",
    "### Description of the Boston Housing Data\n",
    "\n",
    "---\n",
    "\n",
    "The columns of the data set are coded. The corresponding descriptions are:\n",
    "\n",
    "    CRIM: Per capita crime rate by town.\n",
    "    ZN: Proportion of residential land zoned for lots larger than 25,000 sq. ft. \n",
    "    INDUS: Proportion of non-retail business acres per town.\n",
    "    CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise). \n",
    "    NOX: Nitric oxides concentration (parts per 10 million).\n",
    "    RM: Average number of rooms per dwelling.\n",
    "    AGE: Proportion of owner-occupied units built prior to 1940. \n",
    "    DIS: Weighted distances to five Boston employment centers.\n",
    "    RAD: Index of accessibility to radial highways.\n",
    "    TAX: Full-value property tax rate per 10,000 dollars.\n",
    "    PTRATIO: Pupil-teacher ratio by town.\n",
    "    B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town. \n",
    "    LSTAT: Percentage of lower-status population. \n",
    "    MEDV: Median value of owner-occupied homes in 1000s of dollars.\n",
    "    \n",
    "Each row in the data set represents a different suburb of Boston.\n",
    "\n",
    "These descriptions of shortened or coded variables are often called \"codebooks,\" or data dictionaries. They are typically included along with data sets you might find online in a separate file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the packages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_data'></a>\n",
    "\n",
    "### 1. Loading the Data\n",
    "\n",
    "---\n",
    "\n",
    "Import the `.csv` into a `pandas` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_file = './datasets/housing.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='header'></a>\n",
    "\n",
    "### 2. Describing the Basic Format of the Data and the Columns\n",
    "\n",
    "---\n",
    "\n",
    "Use the `.head()` function to examine what the loaded data looks like (as an option, you can also pass in an integer for the number of rows you want to see). This is a good initial step to get a feel for what is contained in the `.csv` and what problems may be present.\n",
    "\n",
    "The `.dtypes` attribute tells you the data type for each of your columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first eight rows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the dtypes of the columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='drop'></a>\n",
    "\n",
    "### 2. Dropping Unwanted Columns\n",
    "\n",
    "---\n",
    "\n",
    "There is a column labeled `Unnamed: 0`, which appears to simply number the rows. We already have the rows' number IDs in the DataFrame's index, so we don't need this column.\n",
    "\n",
    "We can use the built-in `.drop()` function to get rid of this column. When removing a column, we need to specify `axis=1` to the function.\n",
    "\n",
    "For the record, the `.index` attribute holds the row indices. This is the sister attribute to the `.columns` attribute that we work with more often.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the `index` object and the first 20 items in the DataFrame's index \n",
    "# to see that we already have these row numbers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the unneccesary column:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "\n",
    "### 3. Cleaning the Corrupted Columns\n",
    "\n",
    "---\n",
    "\n",
    "You may have noticed that, when we examined the `dtypes` attribute, two of the columns had an \"object\" type, indicating that they were strings. However, we know from the data description above (and we can infer from the data's header) that `DIS` and `RAD` should in fact be numeric.\n",
    "\n",
    "It is pretty common to have numeric columns represented as strings in your data if some of the observations are corrupted. That's why it's important to always check your columns' data types.\n",
    "\n",
    "**3.A What's causing the `DIS` column to be encoded as string? Figure out a way to make sure the column is numeric while preserving its information.**\n",
    "\n",
    "**Tip**: The built-in `.map()` function on a column will apply a function to each of its elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.B What is causing the `RAD` column to be encoded as string? Figure out a way to make sure the column is numeric while preserving its information.**\n",
    "\n",
    "**Tip**: You can put `np.nan` values in place of corrupted observations, which are numeric null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='drop_nulls'></a>\n",
    "\n",
    "### 4. Counting Null Values and Dropping Rows\n",
    "\n",
    "---\n",
    "\n",
    "Having replaced the question marks with `np.nan` values, we know that there are some missing observations for the `RAD` column. \n",
    "\n",
    "When we start to build models with data, null values in observations are (almost) never allowed. It's important to always check how many observations are missing — and in which columns.\n",
    "\n",
    "A handy way to find out how many null values there are per column is with `pandas`.\n",
    "\n",
    "```python\n",
    "boston.isnull().sum()\n",
    "```\n",
    "\n",
    "The built-in `.isull()` function will convert the columns to Boolean `True` and `False` values (returning a new DataFrame); null values are indicated by `True`. \n",
    "\n",
    "Tacked on the end, the `.sum()` function will then sum these Boolean columns, and the total number of null values per column will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the null values.** \n",
    "\n",
    "In this case, let's keep it simple and just drop the rows from the data set that contain null values. If a column has a lot of null values, it often makes more sense to drop the column entirely instead of the individual rows. In this case, however, we will just drop the rows.\n",
    "\n",
    "The `.dropna()` function will drop rows that have _**ANY**_ null values. Use this carefully, as you could drop more rows than you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rename'></a>\n",
    "\n",
    "### 5. Renaming Columns\n",
    "\n",
    "---\n",
    "\n",
    "Oftentimes, it's annoying to have to memorize what codes mean for columns, or reference the codebook whenever we want to know the meaning of a variable. So, it makes sense to rename columns more descriptively.\n",
    "\n",
    "There is more than one method for accomplishing this, but one easy way is to use the `.rename()` function.\n",
    "\n",
    "For reference, here are the column names and their descriptions again:\n",
    "\n",
    "    CRIM: Per capita crime rate by town.\n",
    "    ZN: Proportion of residential land zoned for lots larger than 25,000 sq. ft. \n",
    "    INDUS: Proportion of non-retail business acres per town.\n",
    "    CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise). \n",
    "    NOX: Nitric oxides concentration (parts per 10 million).\n",
    "    RM: Average number of rooms per dwelling.\n",
    "    AGE: Proportion of owner-occupied units built prior to 1940. \n",
    "    DIS: Weighted distances to five Boston employment centers.\n",
    "    RAD: Index of accessibility to radial highways.\n",
    "    TAX: Full-value property tax rate per 10,000 dollars.\n",
    "    PTRATIO: Pupil-teacher ratio by town.\n",
    "    B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town. \n",
    "    LSTAT: Percentage of lower-status population. \n",
    "    MEDV: Median value of owner-occupied homes in 1000s of dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There two popular methods for renaming DataFrame columns:\n",
    "1. Using a _dictionary substitution_, which is useful if you only want to rename a few of the columns. This method uses the `.rename()` function.\n",
    "2. Using a _list replacement_, which is quicker than writing out a dictionary but requires a full list of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List replacement method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='describe'></a>\n",
    "\n",
    "### 6. Describing Summary Statistics for Columns\n",
    "\n",
    "---\n",
    "\n",
    "The `.describe()` function gives summary statistics for each of your variables. What are some, if any, oddities you notice about the variables, based on this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boxplots'></a>\n",
    "\n",
    "### 7. Investigating Potential Outliers With Box Plots\n",
    "\n",
    "---\n",
    "\n",
    "Here, we will use the `seaborn` package to create box plots of the variables we've identified as potentially containing outliers.\n",
    "\n",
    "First, some notes on `seaborn`'s box plot keyword argument options:\n",
    "\n",
    "    `orient`: Can be 'v' or 'h' for vertical and horizontal, respectively.\n",
    "    `fliersize`: The size of the outlier points (in pixels).\n",
    "    `linewidth`: The width of the line surrounding the box plot.\n",
    "    `notch`: Shows the confidence interval for the median (calculated by `seaborn/plt.boxplot`).\n",
    "    `saturation`: Saturates the colors to a specific extent.\n",
    "\n",
    "There are more keyword arguments available, but these are the most relevant for now.   \n",
    "\n",
    "_If you want to check out more, place your cursor in the `boxplot` argument bracket and press `shift+tab` (Press four times repeatedly to bring up detailed documentation)._\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate of crime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent owner occupied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent business zone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black population statistic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot_all'></a>\n",
    "\n",
    "### 8. Plotting All Variables Together\n",
    "\n",
    "---\n",
    "\n",
    "Plot all of the variables in a horizontal box plot with `seaborn`. What, if anything, is wrong with this plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='standardization'></a>\n",
    "\n",
    "### 9. Standardizing Variables\n",
    "\n",
    "---\n",
    "\n",
    "Rescaling variables is common and sometimes essential. For example, when we get to regularizing models, the rescaling procedure becomes a requirement before fitting the model.\n",
    "\n",
    "Here, we'll rescale the variables using a procedure called \"standardization,\" which forces the distribution of each variable to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Standardization is not complicated:\n",
    "\n",
    "    standardized_variable = (variable - mean_of_variable) / std_dev_of_variable\n",
    "    \n",
    "**Note**: Nothing else has changed about the distribution of the variable. It doesn't become normally distributed.\n",
    "\n",
    "**9.A Pull out the rate of crime and plot the distribution.**\n",
    "\n",
    "Also, print out the mean and standard deviation of the original variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.B Standardize the `rate_of_crime` variable. Notice that the new mean is centered at 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.C Plot the original and standardized rate of crime. Notice that nothing changes about the distribution, except for the location and scale.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='plot_all_rescaled'></a>\n",
    "\n",
    "### 10. Plotting the Standardized Variables Together\n",
    "\n",
    "---\n",
    "\n",
    "`pandas` DataFrames make it easy to standardize columns simultaneously. You can standardize data like so:\n",
    "\n",
    "```python\n",
    "boston_stand = (boston - boston.mean()) / boston.std()\n",
    "```\n",
    "\n",
    "Create a standardized version of the data and recreate the box plot. Now you can better examine the differences in the shape of distributions across our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='cov_cor'></a>\n",
    "\n",
    "### 11. Looking at the Covariance or Correlation Between Variables\n",
    "\n",
    "---\n",
    "\n",
    "An easy way to get a feel for linear relationships between variables is with a correlation matrix.\n",
    "\n",
    "Below, we have the formula for the covariance between two variables: `$X$` and `$Y$`.\n",
    "\n",
    "#### 11.A Covariance\n",
    "\n",
    "Given the sample size `$N$`, variables `$X$` and `$Y$`, and means `$\\bar{X}$` and `$\\bar{Y}$`:\n",
    "\n",
    "### $$ \\text{covariance}(X, Y) = \\sum_{i=1}^N \\frac{(X - \\bar{X})(Y - \\bar{Y})}{N}$$\n",
    "\n",
    "The covariance is a measure of \"relatedness\" between variables. It's the sum of deviations from the mean of `$X$`, multiplied by deviations from the mean of `$Y$`, adjusted by the sample size (`$N$`).\n",
    "\n",
    "Code the covariance between `pct_underclass` and `home_median_value` by hand below. Verify that you've gotten the correct result using `np.cov()`. Set the keyword argument `bias=True` in `np.cov()` to have it use the same covariance calculation.\n",
    "\n",
    "**Note**: `np.cov` returns a covariance _matrix_, or, each value's covariance with itself and the other variable in a matrix format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.B Correlation\n",
    "\n",
    "Covariance is not easy to interpret. The values are difficult to read because they are relative to the variance of the variables.\n",
    "\n",
    "A much more common metric — and one that is directly calculable from the covariance — is the correlation.\n",
    "\n",
    "Again, let `$X$` and `$Y$` be our two variables and use the covariance `$cov(X, Y)$` that we calculated above:\n",
    "\n",
    "### $$ \\text{pearson correlation}\\;r = cor(X, Y) =\\frac{cov(X, Y)}{std(X)std(Y)}$$\n",
    "\n",
    "Calculate the correlation between `pct_under` and `med_value` by hand below. Check that it is the same as `np.corrcoef()` with `bias=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.C The correlation matrix\n",
    "\n",
    "We can see the correlation between all numeric variables in our data set by using `pandas` DataFrames' built-in `.corr()` function. Use it below on the Boston data set.\n",
    "\n",
    "It's useful for getting a feel for what is related and what is not, which can help you decide what to investigate further. (Although with a lot of variables, the matrix can be a bit overwhelming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`seaborn` also has an effective way of showing this visually, if colors stick out to you more than decimal values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
