{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Natural Language Processing (NLP) Review Lab\n",
    "\n",
    "_Authors: Joseph Nelson (DC)_ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note: This lab is intended to be completed with the help of an instructor.**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "*Adapted from [NLP Crash Course](http://files.meetup.com/7616132/DC-NLP-2013-09%20Charlie%20Greenbacker.pdf) by Charlie Greenbacker, [Introduction to NLP](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf) by Dan Jurafsky, and Kevin Markham's data school curriculum*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is NLP?\n",
    "\n",
    "- It uses computers to process (analyze, understand, and generate) natural human languages.\n",
    "- Most knowledge created by humans is unstructured text, and computers need a way to make sense of it.\n",
    "- It builds probabilistic models using data about a language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the higher-level task areas?\n",
    "\n",
    "- **Information retrieval**: Finding relevant and similar results.\n",
    "    - [Google](https://www.google.com/).\n",
    "- **Information extraction**: Distilling structured information from unstructured documents.\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en).\n",
    "- **Machine translation**: Translating one language to another.\n",
    "    - [Google Translate](https://translate.google.com/).\n",
    "- **Text simplification**: Preserving the meaning of text but simplifying the grammar and vocabulary.\n",
    "    - [Rewordify](https://rewordify.com/).\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page).\n",
    "- **Predictive text input**: Faster or easier typing.\n",
    "    - [My application](https://justmarkham.shinyapps.io/textprediction/).\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/).\n",
    "- **Sentiment analysis**: Assessing the attitude of the speaker.\n",
    "    - [Hater News](http://haternews.herokuapp.com/).\n",
    "- **Automatic summarization**: Extractive or abstractive summarization.\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0).\n",
    "- **Natural language generation**: Generating text from data.\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052).\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763).\n",
    "- **Speech recognition and generation**: Speech-to-text and text-to-speech.\n",
    "    - [Google's web speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html).\n",
    "    - [Vocalware’s text-to-speech demo](https://www.vocalware.com/index/demo).\n",
    "- **Question answering**: Determining the intent of the question, matching the query with the knowledge base, and evaluating the hypotheses.\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson trivia challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html).\n",
    "    - [The AI behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the lower-level components?\n",
    "\n",
    "- **Tokenization**: Breaking text into tokens (words, sentences, and n-grams).\n",
    "- **Stop word removal**: \"a,\" \"an,\" and \"the.\"\n",
    "- **Stemming and lemmatization**: Root words.\n",
    "- **TF-IDF**: Word importance.\n",
    "- **Parts-of-speech tagging**: Noun, verb, adjective, and adverb.\n",
    "- **Named entity recognition**: Person, organization, and location.\n",
    "- **Spelling correction**: \"New Yrok City.\"\n",
    "- **Word sense disambiguation**: “Buy a mouse.\"\n",
    "- **Segmentation**: \"New York City subway.\"\n",
    "- **Language detection**: “Translate this page.\"\n",
    "- **Machine learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP difficult?\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: Text messages.\n",
    "- **Idioms**: “Throw in the towel.\"\n",
    "- **Newly coined words**: “Retweet.\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters;\" \"Mary and Sue are mothers.\"\n",
    "\n",
    "NLP requires an understanding of **language** and the **world**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading in the Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Corpus = A collection of documents.\n",
    "- Corpora = A plural form of corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_file = './datasets/yelp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A) Subset the reviews to best and worst.\n",
    "\n",
    "- Select only five-star and one-star reviews.\n",
    "- The text will be the features and the stars will be the target.\n",
    "- Create a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** It separates the text into units such as sentences or words.\n",
    "- **Why:** It gives structure to previously unstructured text.\n",
    "- **Notes:** It’s relatively easy to use with English language text but not as easy with some other languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.A) Use `CountVectorizer` to convert the training and testing text data.\n",
    "\n",
    "[`CountVectorizer` documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "- **Lowercase:** Boolean; `True` by default.\n",
    "    - Convert all characters to lowercase before tokenizing.\n",
    "- **ngram_range:** tuple `(min_n, max_n)`.\n",
    "    - The lower and upper boundary of the range of `n` values for different n-grams to be extracted. All values of `n` such that $min_n <= n <= max_n$ will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B) Predict the star rating with the new features from `CountVectorizer`.\n",
    "\n",
    "Validate on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** It removes common words that will likely appear in any text.\n",
    "- **Why:** Stop words don't tell you much about your text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.A) Recreate the features and remove stop words using `CountVectorizer`. \n",
    "\n",
    "- **stop_words:** string `{'english'}`, list, or `None` (default).\n",
    "- If `'english'`, a built-in stop word list for English is used.\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- If `None`, no stop words will be used. `max_df` can be set to a value in the range, [0.7, 1.0), to automatically detect and filter stop words based on intra-corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.B) Validate your model using the features with stop words removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.A) Shrink the maximum number of features and retest the model.\n",
    "\n",
    "- **max_features:** `int` or `None`; `default=None`.\n",
    "- If not `None`, build a vocabulary that only considers the top `max_features` ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B) Change the minimum document frequency for terms and test the model's performance.\n",
    "\n",
    "- **min_df:** Float in range `[0.0, 1.0]` or int; `default=1`.\n",
    "- When building the vocabulary, ignore terms that have a document frequency that is strictly lower than the given threshold. This value is also called the cut-off. If float, the parameter represents a proportion of documents while the integer represents absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Introduction to TextBlob\n",
    "\n",
    "TextBlob: Simplified text processing.\n",
    "\n",
    "### 5.A) Use `TextBlob` to convert the text in the first review in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.B) List the words in the `TextBlob` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.C) List the sentences in the `TextBlob` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Stemming and Lemmatization\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "- **What:** It reduces a word to its base/stem/root form.\n",
    "- **Why:** It often makes sense to treat related words the same way.\n",
    "- **Notes:**\n",
    "    - It uses a simple and fast rule-based approach.\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing).\n",
    "    - Some search engines treat words with the same stem as synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.A) Initialize the `SnowballStemmer` and stem the words in the first review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.B) Use the built-in `lemmatize()` function on the first review's words (parsed by `TextBlob`).\n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "- **What:** It derives the canonical form (lemma) of a word.\n",
    "- **Why:** It can be more effective than stemming.\n",
    "- **Notes:** It uses a dictionary-based approach (slower than stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.C) Write a function that uses `TextBlob` and `lemmatize()` to lemmatize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.D) Provide your function to `CountVectorizer` as the `analyzer` and test the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "- **What:** It computes the \"relative frequency\" with which a word appears in a document compared to its frequency across all documents.\n",
    "- **Why:** It's more useful than \"term frequency\" for identifying \"important\" words in each document (e.g., high frequency in that document, low frequency in other documents).\n",
    "- **Notes:** It’s used for search engine scoring, text summarization, and document clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.A) Build a simple TF-IDF using `CountVectorizer`.\n",
    "\n",
    "- Term frequency can be calculated with a default `CountVectorizer`.\n",
    "- Inverse document frequency can be calculated with `CountVectorizer` and the argument `binary=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More details:** [TF-IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Using TF-IDF to Summarize a Yelp Review\n",
    "\n",
    "> **Note:** Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF.\n",
    "\n",
    "### 8.A) Build a TF-IDF predictor matrix that excludes stop words with `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.B) Write a function to pull out the top five words from a review based on TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.A) Extract sentiment from a review parsed with `TextBlob`.\n",
    "\n",
    "Sentiment polarity ranges from negative one — the most negative — to one — the most positive. A parsed `TextBlob` object has sentiment that can be accessed with:\n",
    "\n",
    "    review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.B) Calculate the sentiment for every review in the full Yelp data set as a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.C) Create a box plot of sentiment based on star rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.D) Print the reviews with the highest and lowest sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. [Bonus] Explore Fun `TextBlob` Features\n",
    "\n",
    "### 10.A) Correct spelling with `.correct()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.B) Perform spell-checking with `.spellcheck()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.C) Extract definitions with `.define()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- NLP is a gigantic field.\n",
    "- Understanding the basics broadens the types of data you can work with.\n",
    "- Simple techniques go a long way.\n",
    "- Use scikit-learn for NLP whenever possible."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
