{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Feature Selection on the Titanic Data Set\n",
    "\n",
    "_Authors: Joseph Nelson (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "In this lab, you'll explore a variety of different feature selection methods in scikit-learn. You'll be using the Titanic data set.\n",
    "\n",
    "You can load the Titanic data as follows:\n",
    "\n",
    "    psql -h dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com -p 5432 -U dsi_student titanic\n",
    "    password: gastudents\n",
    "\n",
    "Or alternatively, load the data set from the local folder:\n",
    "\n",
    "    ./datasets/titanic_train.csv\n",
    "    \n",
    "\n",
    "## Useful Feature Selection Resources\n",
    "\n",
    "---\n",
    "\n",
    "- Michigan State Overview on [Feature Selection](http://www.cse.msu.edu/~cse802/Feature_selection.pdf) and (Bonus) Texas A&M on [Bidirectional Feature Selection](http://research.cs.tamu.edu/prism/lectures/pr/pr_l11.pdf)\n",
    "- Scikit-Learn Documentation on [Feature Selection](http://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "- Side-by-Side Comparison of [Feature Selection Tactics](http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1) Import the data and perform exploratory data analysis (EDA). Engineer any features you think are predictive of survival.\n",
    "\n",
    "We'll be working with the Titanic data set. Go ahead and import it from the data set folder (or query for it as described above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/titanic_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Set up predictor and target matrices\n",
    "\n",
    "- The target should be `Survived`.\n",
    "- The predictor matrix will be all other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Feature Selection\n",
    "\n",
    "Let's use the `SelectKBest` method in scikit-learn to see which are the top five features. Also load the `f_classif` and `chi2` functions, which will be our metrics to evaluate what makes a variable the \"best.\"\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "```\n",
    "\n",
    "- What are the top five features for `X` using `f_classif`?\n",
    "- What are the top five features for `X` using `chi2`?\n",
    "\n",
    "\n",
    "> The F-test is explained variance divided by unexplained variance. High numbers will result if our explained variance (what we know) is much greater than our unexplained variance (what we don't know). The `chi2` goodness of fit is the sum of the difference squared between observed and expected, divided by expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n",
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Recursive Feature Elimination (RFE)\n",
    "\n",
    "Scikit-learn also offers recursive feature elimination as a class named `RFECV`. Use it in combination with a logistic regression model to see what features would be kept with this method.\n",
    "\n",
    "When instantiating the `RFECV`:\n",
    "- `step` indicates what percent of features (or number of features if an integer) to remove at each iteration.\n",
    "- `cv` indicates the number of cross-validation folds to use for evaluating what features are important.\n",
    "\n",
    "Store the columns in a variable called `rfecv_columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Feature elimination using the lasso penalty\n",
    "\n",
    "The L1 penalty is a popular method for feature selection. As the regularization strength increases, more features will be removed.\n",
    "\n",
    "Load the `LogisticRegressionCV` class.\n",
    "\n",
    "1) Standardize your predictor matrix (required for regularization).\n",
    "- Create a logistic regression cross-validator object:\n",
    "    - Set `penalty='l1'` (lasso).\n",
    "    - Set `Cs=100` (search 100 different regularization strengths).\n",
    "    - Set `solver='liblinear'` (required for the lasso penalty).\n",
    "    - Set `cv=10` for 10 cross-validation folds.\n",
    "- Fit on the target and standardized predictors.\n",
    "- Sort the logistic regression coefficients by absolute value. \n",
    "\n",
    "Do the top five correspond to those selected by the F-score and `chi2`?\n",
    "\n",
    "Choose which ones you would keep and store them in a variable called `lr_columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Compare feature sets\n",
    "\n",
    "Use the optimized logistic regression from the previous question on the features selected from different methods. \n",
    "- `kbest_columns`\n",
    "- `rfecv_columns`\n",
    "- `lasso_columns`\n",
    "- `all_columns`\n",
    "\n",
    "**Questions:**\n",
    "- Which scores the highest? (Use cross_val_score.)\n",
    "- Is the difference significant?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) [Bonus] Display the lasso logistic regression coefficients with a bar chart\n",
    "\n",
    "Start from the most negative on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
