{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> **Jupyter slideshow:** This notebook can be displayed as slides. To view it as a slideshow in your browser, type the following in the console:\n",
    "\n",
    "\n",
    "> `> jupyter nbconvert [this_notebook.ipynb] --to slides --post serve`\n",
    "\n",
    "\n",
    "> To toggle off the slideshow cell formatting, click the `CellToolbar` button, then `View --> Cell Toolbar --> None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "_Authors: Dan Wilhelm (LA), Alex Combs (NYC) _\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "*By the end of this lesson, you will be able to:*\n",
    "- Describe Naive Bayes.\n",
    "- Choose a Naive Bayes implementation based on your use case.\n",
    "- Implement a Naive Bayes model through scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Discriminative Models vs. Generative Models](#discriminative-models-vs-generative-models)\n",
    "    - [Example: Discriminative vs. Generative](#discriminative-vs-generative-example)\n",
    "    - [Making a Generative Model (Joint Probability Distribution)](#making-a-generative-model-joint-probability-distribution)\n",
    "    - [The Joint Probability Generative Model is Not Practical](#the-joint-probability-generative-model-is-not-practical)\n",
    "    - [Making a Discriminative Model (Logistic Regression)](#making-a-discriminative-model-logistic-regression)\n",
    "\n",
    "\n",
    "- [A Better Generative Model?](#a-better-generative-model)\n",
    "    - [Bayes' Theorem](#bayes-theorem)\n",
    "    - [Conditional Probability](#conditional-probability)\n",
    "\n",
    "\n",
    "- [A Simple Spam Example](#a-really-simple-spam-example)\n",
    "    - [Problem: Multiple Features Require Joint Probabilities](#problem-multiple-features-require-joint-probabilities)\n",
    "    - [Solution: The Naive Bayes Independence Assumption](#solution-the-naive-bayes-independence-assumption)\n",
    "    - [Spam Application: Multiple Features](#spam-application-multiple-features)\n",
    "\n",
    "\n",
    "- [Production Issues](#production-issues)\n",
    "- [Summary](#summary)\n",
    "- [Implementation in Scikit-Learn](#implementation-in-scikit-learn)\n",
    "- [Guided Practice: Scikit-Learn Implementation](#guided-practice-scikit-learn-implementation)\n",
    "- [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"discriminative-models-vs-generative-models\"></a>\n",
    "## Discriminative Models vs. Generative Models\n",
    "---\n",
    "\n",
    "Logistic regression is a **discriminative model**. \n",
    "\n",
    "+ It's equation, $P\\;(\\;class\\;|\\;features\\;)$, **discriminates** between two classes. In other words, it describes the boundary between the classes.\n",
    "+ From this equation, we can’t generate \"typical\" members of either class — we only know their boundaries.\n",
    "\n",
    "Naive Bayes is a **generative model**. It can **generate** typical members of each class, as we know what a typical member of each class looks like.\n",
    "\n",
    "+ Note: We are still estimating $P\\;(\\;class\\;|\\;features\\;)$ for each class.\n",
    "+ By computing how typical each example is to each class, we can choose the most likely class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"discriminative-vs-generative-example\"></a>\n",
    "### Example: Discriminative vs. Generative\n",
    "\n",
    "Let's build a simple generative model from scratch. Suppose we are attempting to infer whether or not someone is a GA data science student. To do this, we take members of the general population and evaluate three binary features:\n",
    "\n",
    "- **G**: At GA.\n",
    "- **C**: Has computer.\n",
    "- **S**: Has stats book.\n",
    "\n",
    "Our model of a \"person\" is now the presence or absence of each of these three features.\n",
    "\n",
    "#### Data\n",
    "\n",
    "We sample 10 students in each class. We'll assume that the GA/non-GA numbers are good, representative samples of the population for each combination of features. \n",
    "\n",
    "- **GA student**: GCS, GC, GCS, GCS, GC, C, C, GC, GCS, GCS.\n",
    "- **Non-GA student**: none, none, C, GC, C, none, C, C, CS, none.\n",
    "\n",
    "We grabbed 10 examples per class. This is typically the data we have access to in most data science problems — examples of each class of interest. However, to be representative of the overall population, should we have gotten 10 examples from each of the eight categories instead? Which technique would be more accurate? Think about it while we take this toy data and tally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"making-a-generative-model-joint-probability-distribution\"></a>\n",
    "### Making a Generative Model (Joint Probability Distribution)\n",
    "\n",
    "Summarize this data per class (where $\\neg$ indicates negation):\n",
    "\n",
    "_                 | GCS | GC | CS | S | C | None   \n",
    "---               | --- | -- | -- | - | - | -    \n",
    "**Student**       | 5   | 3  | 0  | 0 | 2 | 0     \n",
    "**$\\neg$Student** | 0   | 1  | 1  | 0 | 4 | 4     \n",
    "\n",
    "\n",
    "From these, we'll make a generative model of a student:\n",
    "\n",
    "- $P(\\;Student\\;|\\;GCS\\;) = 5/5 = 1$.\n",
    "- $P(\\;Student\\;|\\;GC\\;) = 3/(3+1) = 3/4$.\n",
    "- $P(\\;Student\\;|\\;CS\\;) = 0$.\n",
    "- $P(\\;Student\\;|\\;S\\;) = NA$.\n",
    "- $P(\\;Student\\;|\\;C\\;) = 2/(4+2) = 1/3$.\n",
    "- $P(\\;Student\\;|\\;none\\;) = 0$.\n",
    "\n",
    "We'll also make a model for a non-student:\n",
    "\n",
    "- $P(\\;\\neg Student\\;|\\;GCS\\;) = 0/5 = 0$.\n",
    "- $P(\\;\\neg Student\\;|\\;GC\\;) = 1/4$.\n",
    "- $P(\\;\\neg Student\\;|\\;CS\\;) = 1$.\n",
    "- $P(\\;\\neg Student\\;|\\;S\\;) = NA$.  \n",
    "- $P(\\;\\neg Student\\;|\\;C\\;) = 4/(4+2) = 2/3$.\n",
    "- $P(\\;\\neg Student\\;|\\;none\\;) = 4/4 = 1$.\n",
    "\n",
    "\n",
    "You may recognize that we just computed the **joint probabilities**. \n",
    "\n",
    "Note: We must store $2^3$ parameters in total — the presence or absence of three features.\n",
    "\n",
    "#### Using the Generative Model\n",
    "\n",
    "+ Suppose we see someone with $GCS$. We would then guess with a confidence of one that they are a GA student. If someone is $GC$, we would guess that they are a GA student with a confidence of $3/4$.\n",
    "\n",
    "+ Suppose we want to generate a sequence of typical GA students. It's easy! With probability $\\frac{1}{1 + 3/4 + 1/3} = \\frac{12}{25}$, generate a $GCS$ person. With probability $\\frac{3/4}{1 + 3/4 + 1/3} = \\frac{9}{25}$, generate a $GC$ person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"the-joint-probability-generative-model-is-not-practical\"></a>\n",
    "### The Joint Probability Generative Model is Not Practical\n",
    "\n",
    "Earlier, we saw how to build a generative model strictly from joint probabilities. However, this method has some major problems.\n",
    "\n",
    "+ The number of parameters stored in the model increases exponentially. For example, if each feature is binary and we have 100 features, we would need $2^{100} \\approx 10^{30}$ parameters for every joint probability.\n",
    "\n",
    "+ We would need _enormous_ amounts of data to ensure we have sufficient training examples to evaluate each joint probability robustly (again, just to emphasize — $2^{100}$ joint probabilities for only $100$ binary features).\n",
    "\n",
    "Although each probability is easy to calculate, the joint probability model is simply not practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"making-a-discriminative-model-logistic-regression\"></a>\n",
    "### Making a Discriminative Model (Logistic Regression)\n",
    "\n",
    "For comparison, let's make a discriminative model using the same data. \n",
    "\n",
    "| _                  | GCS | GC | CS | S | C | None\n",
    "| ---               | --- | -- | -- | - | - | -\n",
    "| **Student**       | 5   | 3  | 0  | 0 | 2 | 0 \n",
    "| **$\\neg$Student** | 0   | 1  | 1  | 0 | 4 | 4\n",
    "\n",
    "We'll eyeball a hyperplane that separates the classes. For simplicity, let's suppose that our hyperplane (and corresponding link function) is the following formula, where $\\sigma$ is the logistic function and $X$ is our feature vector. $G = 1$ if true and $G = -1$ if false:\n",
    "\n",
    "$$P(\\;Student\\;|\\;X\\;) = \\sigma(5G - 2C + S)$$\n",
    "\n",
    "We can see that this model is approximately correct. Recall that $5G - 2C + S > 0$ indicates a probability greater than 0.5. This model makes sense — being seen at GA is a strong indicator of being a GA student. However, having a computer is a less positive signal, as many non-students also have computers.\n",
    "\n",
    "### Using the Discriminative Model\n",
    "\n",
    "+ We can predict the probability that $GC$ is a student by letting $G = 1, C = 1, S = -1$. Because $5G - 2C + S = 2 > 0$, they're likely a student.\n",
    "\n",
    "+ This model is more compact, as we store fewer parameters — $4$ (three plus a bias) instead of $2^3$. (It even scales linearly instead of exponentially.)\n",
    "\n",
    "+ However, we can't generate typical students with any accuracy. (In this case we can attempt to, but with any substantial number of features, typical members would be far too ambiguous.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"a-better-generative-model\"></a>\n",
    "## A Better Generative Model?\n",
    "---\n",
    "\n",
    "There are some reasonable simplifications we can make to build a practical generative model, and they're nearly as simple to calculate.\n",
    "\n",
    "Let's understand why we're doing this. Recall that we’re looking for this: $P\\;(\\;class\\;|\\;features\\;)$. Here, class refers to a category such as \"vertosa,\" \"versicola,\" \"Student,\" etc.\n",
    "\n",
    "+ Computing this directly implies that we have a lot of data for every feature combination, which isn't typically the case. Usually, our training data only ensure we have sufficient examples for each **class**. For example, 10 examples of GA students and 10 examples of non-GA students.\n",
    "\n",
    "+ It would be great if we could flip the conditional probability, as our training data have a lot of examples of each class. The Bayes' Theorem will help us here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"bayes-theorem\"></a>\n",
    "### Bayes' Theorem\n",
    "\n",
    "### $$P\\left(\\;class\\;|\\;features\\;\\right) = \\frac{P\\left(\\;features\\;|\\;class\\;\\right)P\\left(\\;class\\;\\right)}{P(\\;features\\;)} $$\n",
    " \n",
    "Luckily, it's easy to compute these probabilities directly from a data table. \n",
    "\n",
    "+ **$P(\\;class\\;)$**. For example: $P(\\;student\\;) = P(\\;\\neg student\\;) = 1/2$.\n",
    "+ **$P(\\;features\\;)$**. For example, we had five $GCS$ combinations in 20 examples, so $P(\\;GCS\\;) = 5/20 = 1/4$.\n",
    "+ **$P(\\;features\\;|\\;class\\;)$**. For example, for $GCS$ we had five students of 10. $P(\\;GCS\\;|\\;Student\\;) = 1/2$.\n",
    "\n",
    "| _                 | GCS | GC | CS | S | C | None\n",
    "| ---               | --- | -- | -- | - | - | -\n",
    "| **Student**       | 5   | 3  | 0  | 0 | 2 | 0 \n",
    "| **$\\neg$Student** | 0   | 1  | 1  | 0 | 4 | 4\n",
    "\n",
    "\n",
    "Then, what is $P(\\;Student\\;|\\;GCS\\;)$? It's just $\\frac{1/2 * 1/2}{1/4} = 1$ — exactly what we computed above as a joint probability.\n",
    "\n",
    "+ Does this work out for all of our earlier joint probabilities? See if you can understand why.\n",
    "+ Hint: Recall that Bayes' Theorem is just a single conditional probability. The numerator is just the chain rule: $P(\\;A \\cap B\\;) = P(\\;A\\;|\\;B\\;)\\;P(\\;B\\;)$.\n",
    "\n",
    "| _                  | GCS                           | GC                 | ... | \n",
    "| ---                | ---------------------------   | ------------------ | --  | \n",
    "| **Student**        | Student $\\cap$ GCS            | Student $\\cap$ GC  | ... | P(Student) = 1/2\n",
    "| **$\\neg$ Student** | $\\neg$ Student $\\cap$ GCS     | $\\neg$ Student $\\cap$ GC | ... | P($\\neg$ Student) = 1/2\n",
    "| probability         | P(GCS) = 5/20                 | P(GC) = 4/20       |   ...  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"conditional-probability\"></a>\n",
    "### Conditional Probability\n",
    "\n",
    "Let's review and see how the Bayes' Theorem is just a big conditional probability.\n",
    "\n",
    "In general, the **conditional probability** for events $A$ and $B$ is:\n",
    "\n",
    "### $$ P(\\;A\\;|\\;B\\;) = \\frac{P(\\;A \\cap B\\;)}{P(\\;B\\;)} $$\n",
    "\n",
    "Hence (because $\\cap$ is commutative — i.e., $P(A \\cap B) = P(B \\cap A)$):\n",
    "\n",
    "### $$ P(A \\cap B) = P(A\\;|\\;B) \\; P(B) = P(B\\;|\\;A) \\; P(A) $$\n",
    "\n",
    "This is often referred to as the \"chain rule\" of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Bayes' Theorem\n",
    "\n",
    "Substitute the second equation above into the first:\n",
    "\n",
    "### $$P\\left(\\;A\\;|\\;B\\;\\right) = \\frac{P(\\;A \\cap B\\;)}{P(\\;B\\;)} = \\frac{P\\left(\\;B\\;|\\;A\\;\\right)P\\left(\\;A\\;\\right)}{P(\\;B\\;)}$$\n",
    "\n",
    "\n",
    "### $$P\\left(\\;class\\;|\\;features\\;\\right) = \\frac{P\\left(\\;features\\;|\\;class\\;\\right)P\\left(\\;class\\;\\right)}{P(\\;features\\;)} $$\n",
    "\n",
    "As we saw earlier, it's very easy to compute $P(\\;features\\;|\\;class\\;)$ from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"a-really-simple-spam-example\"></a>\n",
    "## A Simple Spam Example\n",
    "---\n",
    "\n",
    "Here's another example. We're trying to predict spam emails. For now, we have one feature: whether or not the email mentions \"guarantee.\"\n",
    "\n",
    "$G$ = Guarantee, $S$ = Is spam.\n",
    "\n",
    " $$P\\left(\\;S\\;|\\;G\\;\\right) = \\frac{P\\left(\\;G\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)}{P(\\;G\\;)} = \\frac{P\\left(\\;G\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)}{P(\\;G\\;|\\;S)P(\\;S\\;) + P(\\;G\\;|\\;\\neg{S})P(\\;\\neg{S}\\;)}$$\n",
    "\n",
    "Earlier, we saw how it's possible to compute $P\\;(\\;G\\;)$ directly. In many books, you'll see it computed in this alternative fashion, which is based on our classes rather than on every combination of features. \n",
    "\n",
    "> The denominator looks complicated, but it actually isn't. Because $G$ is binary (either present or not), then:\n",
    "\n",
    "> $$P(G) = P(G \\cap S) + P(G \\cap \\neg{S})$$\n",
    "\n",
    "> Now, simply expand each term using the chain rule and you’ll get the denominator.\n",
    "\n",
    "Remember that we started with a term $P\\;(\\;features\\;)$ — in general, using this would require the calculation of every combination of features (i.e., we would also need to compute $P\\;(\\;GS\\;)$, $P\\;(\\;GCS\\;)$, etc.). However, by expanding $P\\;(\\;G\\;)$ via the chain rule, we got an expression that depends only on the individual classes about which we have data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"problem-multiple-features-require-joint-probabilities\"></a>\n",
    "### Problem: Multiple Features Require Joint Probabilities\n",
    "\n",
    "In this spam example, we only had one feature: $G$. But, in general, we'll probably use more than one. Really, we want to see some feature vector $X_1, X_2, ..., X_n$:\n",
    "\n",
    "$$P\\left(\\;S\\;|\\;X_1, ..., X_n\\;\\right) = \\frac{P\\left(\\;X_1,  ..., X_n\\;|\\;S\\;\\right)}{P(\\;X_1,  ..., X_n\\;|\\;S) + P(\\;X_1, ..., X_n\\;|\\;\\neg{S})}$$\n",
    "\n",
    "For example, what is the likelihood that something is spam given that the email mentions \"guarantee,\" \"oil,\" \"prince,\" and \"Nigeria,\" but not \"meeting,\" \"colleague,\" and \"dad?\"\n",
    "\n",
    "With a lot of features, calculating the joint probabilities quickly becomes complicated. We would definitely need a lot of data to ensure we have enough feature combinations. If you reason this out, you may quickly realize that we run into the same joint probability problem as before, requiring exponentially more joint probabilities.\n",
    "\n",
    "No matter how diligent we are, we may never collect a single training example that contains the precise combination of feature words we need. Therefore, we would be unable to classify a new email containing a particular combination of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"solution-the-naive-bayes-independence-assumption\"></a>\n",
    "### Solution: The Naive Bayes Independence Assumption\n",
    "\n",
    "We're stuck, because conditional joint probabilities are required for multiple features. This means there are exponentially more probabilities to compute and data to collect.\n",
    "\n",
    "To get around this, let's make an assumption: **All $X_i$ are conditionally independent given $S$** (where $S$ indicates \"is spam\"). This means that, given $S$, no two $X_i$ depend on each other. For example, the words \"Nigeria\" and \"prince\" each independently indicate that an email is spam. So, it's not the complex interaction of words that determines spam; each feature can indicate whether or not an email is spam independently.\n",
    "\n",
    "> Of course, this assumption is rarely (if ever) true. Often, it requires precise reading to tell whether or not an email was written by a native speaker. In this case, it's often not the particular words used, but how they are used in context.\n",
    "\n",
    "Recall that if events $A$ and $B$ are independent, then the probability $P\\;(\\;AB\\;) = P\\;(\\;A\\;)\\;P\\;(\\;B\\;)$. Similarly, if $A$ and $B$ are conditionally independent on $S$, then $P\\;(\\;AB\\;|\\;S\\;) = P\\;(\\;A\\;|\\;S\\;)\\;P\\;(\\;B\\;|\\;S\\;)$.\n",
    "\n",
    "> This formula works out well in general, too:\n",
    "\n",
    "> $$P\\left(\\;X_{1}X_{2} \\dots X_{n}\\;|\\;S\\;\\right) = P\\left(\\;X_{1} |\\;S\\;\\right) * P\\left(\\;X_{2} |\\;S\\;\\right) ... P\\left(\\;X_{n} |\\;S\\;\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To see if this **conditional independence assumption** might simplify the numerator $P(X_1, ..., X_n\\;|\\;S)$ by removing the joint probabilities, let's apply the definition of conditional probability, followed by the application of the independence assumption:\n",
    "\n",
    "$$P\\;(\\;SGM\\;) = P\\;(\\;GM\\;|\\;S\\;)\\;P\\;(\\;S\\;) = P\\;(\\;G\\;|\\;S\\;)\\;P\\;(\\;M\\;|\\;S\\;)\\;P\\;(\\;S\\;)$$\n",
    "\n",
    "\n",
    "> None of these probabilities require us to examine multiple features at once in our data set, making them drastically easier to compute. For example, $P(\\;A\\;|\\;S\\;)$ could indicate only the probability of the word “a” occurring in a spam email.\n",
    "\n",
    "In reality, model parameters/coefficients are unlikely to be independent. But Naive Bayes makes exactly this assumption and often works well despite this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a id=\"spam-application-multiple-features\"></a>\n",
    "### Spam Application: Multiple Features\n",
    "\n",
    "How is this used in practice? Let's combine the Naive Bayes simplification above with our original formula (the denominator is computed the same way as before, combined with our naive assumption):\n",
    "\n",
    " $$P\\left(\\;S\\;|\\;GM\\;\\right) = \\frac{P(\\;SGM\\;)}{P(\\;GM\\;)} = \\frac{P\\left(\\;GM\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)}{P(\\;GM\\;|\\;S)P(\\;S\\;) + P(\\;GM\\;|\\;\\neg{S})P(\\;\\neg{S}\\;)} = \\frac{P\\left(\\;G\\;|\\;S\\;\\right)P\\left(\\;M\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)}{P(\\;G\\;|\\;S)P(\\;M\\;|\\;S)P(\\;S\\;) + P(\\;G\\;|\\;\\neg{S})P(\\;M\\;|\\;\\neg{S})P(\\;\\neg{S}\\;)}$$\n",
    " \n",
    "Typically, we compute this probability for each class (in this case, just $S$ or not $S$), then predict the class with the highest probability. Note that, for all of these, the denominator $P(GM)$ is constant. Hence, this formula is often written as \"proportional\" ($\\propto$), considerably simplifying it. Instead of comparing the exact probabilities, we can simply see how they score relative to each other. So:\n",
    "\n",
    " $$P\\left(\\;S\\;|\\;GM\\;\\right) \\propto P\\left(\\;G\\;|\\;S\\;\\right)P\\left(\\;M\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)$$\n",
    "\n",
    "Remember, don't be scared by the denominator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"production-issues\"></a>\n",
    "## Production Issues\n",
    "---\n",
    "\n",
    "Recall that Naive Bayes is proportional to:\n",
    "\n",
    " $$P\\left(\\;S\\;|\\;GM\\;\\right) \\propto P\\left(\\;G\\;|\\;S\\;\\right)P\\left(\\;M\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)$$\n",
    "\n",
    "Accidentally using a zero probability for any of these could present major problems — the entire probability estimation would be zero.\n",
    "\n",
    "- **New features:** What if a particular feature was never seen in our training data? Instead of using a zero probability, we should use a technique such as [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to estimate a small non-zero probability.\n",
    "\n",
    "- **Underflow:** Probabilities could be very small if some features rarely occur in some classes. Recall that floating point often gives us trouble because of its limited precision — small floats tend toward zero. We can approach this problem by storing the logarithm of each probability, $P_i$, instead of $P_i$ itself:\n",
    "\n",
    "$$log(P_1P_2) = log\\ P_1 + log\\ P_2$$\n",
    "\n",
    "$$e^{log\\ P_1} = P_1$$\n",
    "\n",
    "So: $$P_1P_2 \\dots P_n = e^{log\\ P_1 + ... + log\\ P_n}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "---\n",
    "\n",
    "**Why is the Naive Bayes formula important?** With the independence assumption, we don't need to compute every joint probability distribution. Even if none of the training data contain the words “guarantee” and “millions,” we only need to compute the probability of each word separately: $P(\\;G\\;|\\;S\\;)$ and $P(\\;M\\;|\\;S\\;)$. \n",
    "\n",
    "These calculations can be performed quickly using our training data. The downside is that, if spam is actually determined by some complex interaction between \"guarantee\" and \"millions\" (e.g., only the presence of one but not the other), then the independence assumption does not hold and our model will not have the capacity to predict spam correctly.\n",
    "\n",
    "To make Naive Bayes a classifier, all we have to do is compute the probability of $P(y|X)$ for each class, $y$. In math notation, this is:\n",
    "\n",
    "### $$ P(y \\;|\\; x_1, ..., x_n) \\propto P(y) \\prod_{i=1}^n P(x_i \\;|\\; y) \\\\\n",
    "\\downarrow \\\\\n",
    "\\hat{y} = arg \\; \\underset{y}{max} \\; P(y) \\prod_{i=1}^n P(x_i \\;|\\; y)$$\n",
    "\n",
    "> Recall that $arg\\underset{y}{max}$ means we find the class in the vector of categories, $y$, that gives us the maximum expression value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"implementation-in-scikit-learn\"></a>\n",
    "## Implementation in Scikit-Learn\n",
    "---\n",
    "\n",
    "\n",
    "- [Docs 1](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "- [Docs 2](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "- [Docs 3](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n",
    "\n",
    "<img src=\"./images/naive-bayes.png\">\n",
    "\n",
    "The differences can be summarized as:\n",
    "-    ***BernoulliNB*** is designed for binary/Boolean features.\n",
    "-    The ***multinomial Naive Bayes classifier*** is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as `tf-idf` may also work.\n",
    "-    ***GaussianNB*** is designed for continuous features (that can be scaled between zero and one) and is assumed to be normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"guided-practice-scikit-learn-implementation\"></a>\n",
    "## Guided Practice: Scikit-Learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./datasets/spam_base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.40</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.44</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  0.64  0.64.1  0.1  0.32   0.2   0.3   0.4   0.5   0.6 ...  0.40  \\\n",
       "0  0.21  0.28    0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00   \n",
       "1  0.06  0.00    0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01   \n",
       "2  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00   \n",
       "3  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00   \n",
       "4  0.00  0.00    0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00 ...  0.00   \n",
       "\n",
       "    0.41  0.42  0.778   0.43   0.44  3.756   61   278  1  \n",
       "0  0.132   0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
       "1  0.143   0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
       "2  0.137   0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
       "3  0.135   0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
       "4  0.223   0.0  0.000  0.000  0.000  3.000   15    54  1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **Check**: What do you think is going on with this data set?\n",
    "\n",
    "> Which scikit-learn Naive Bayes implementation should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "feature_set = data.iloc[:, :-1]\n",
    "target = data.iloc[:, -1]\n",
    "\n",
    "classifier1 = naive_bayes.MultinomialNB().fit(feature_set, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79153094, 0.81976113, 0.81413043, 0.7867247 , 0.69749728])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(classifier1, feature_set, target, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **Check**: Is that good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.606086956521739"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1. - np.mean(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "---\n",
    "\n",
    "\n",
    "How does Naive Bayes fit into your toolkit? What are its pros and cons? How do you choose between variants?\n",
    "\n",
    "#### Additional Resources\n",
    "\n",
    "- [An interesting slide](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf) from a Stanford MOOC that has a section on Naive Bayes.\n",
    "- [A much more technical paper](https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf) comparing Naive Bayes to logistics regressions.\n",
    "- [More exposition on Naive Bayes](http://blog.yhat.com/posts/naive-bayes-in-python.html).\n",
    "- [Naive Bayes from scratch](http://machinelearningmastery.com/naive-bayes-classifier-scratch-python/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
