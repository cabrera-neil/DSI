{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Bayesian Statistics\n",
    "\n",
    "_Authors: Matt Brems (DC), Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Review the axioms and properties of probability.\n",
    "- Cover the formula for Bayes' theorem.\n",
    "- Learn the diachronic interpretation of Bayes' theorem.\n",
    "- Gain an intuition for the different components of the formula.\n",
    "- Tackle the Monty Hall problem with Bayesian statistics.\n",
    "- Complete some additional Bayesian statistics problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lesson Guide\n",
    "- [Review of Probability](#review)\n",
    "    - [Axioms of Probability](#axioms)\n",
    "    - [Properties of Probability](#properties)\n",
    "- [Bayes' Theorem](#bayes-rule)\n",
    "    - [The Diachronic Interpretation](#diachronic)\n",
    "- [Frequentist vs. Bayesian Probability](#freq-vs-bayes)\n",
    "- [Bayes' Theorem in Parts](#parts)\n",
    "- [The Monty Hall Problem](#monty-hall)\n",
    "- [Additional Bayesian Statistics Problems](#additional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='review'></a>\n",
    "## Review of Probability\n",
    "\n",
    "---\n",
    "\n",
    "### The Sample Space, Event Space, and Probability Function\n",
    "\n",
    "With $S$ denoting the sample space and $F$ denoting the event space — or space of all possible events — we have a probability function $P$ defined as:\n",
    "\n",
    "### $$ P(S, F) \\rightarrow [0, 1]$$\n",
    "\n",
    "The probability maps all events in the sample space to the interval from zero to one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='axioms'></a>\n",
    "### Axioms of Probability\n",
    "\n",
    "**Non-Negativity**\n",
    "\n",
    "For any event, $A$, the probability of the event must be greater than or equal to zero.\n",
    "\n",
    "### $$ 0 \\le P(A) $$\n",
    "\n",
    "**Unit Measure**\n",
    "\n",
    "The probability of the entire sample space is one.\n",
    "\n",
    "### $$ P(S) = 1 $$\n",
    "\n",
    "**Additivity**\n",
    "\n",
    "For mutually exclusive (or disjoint) events, $E$, the probability of any of the events occurring is equivalent to the sum of their probabilities.\n",
    "\n",
    "### $$ P\\left(\\cup_{i=1}^{\\infty}\\; E_i \\right) = \\sum_{i=1}^{\\infty} P(E_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='properties'></a>\n",
    "### Properties of Probability\n",
    "\n",
    "**The Probability of No Event**\n",
    "\n",
    "The probability of the empty set, denoted $\\emptyset$, is zero.\n",
    "\n",
    "### $$ P\\left(\\emptyset \\right) = 0 $$\n",
    "\n",
    "**The Probability of $A$ or $B$ Occurring (Union)**\n",
    "\n",
    "The probability of event $A$ or event $B$ occurring is equivalent to the sum of their individual probabilities, minus the intersection of their probabilities (the probability that they both occur).\n",
    "\n",
    "### $$ P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$$\n",
    "\n",
    "**Conditional Probability**\n",
    "\n",
    "The probability of an event that is conditional on another event is written using a vertical bar between the two events. The probability of event $A$ occurring _given_ that event $B$ occurs is calculated as:\n",
    "\n",
    "### $$ P(A | B) = \\frac{P(A \\cap B)}{P(B)} $$\n",
    "\n",
    "This represents the probability of both $A$ and $B$ occurring, divided by the probability that $B$ occurs at all.\n",
    "\n",
    "**Joint Probability**\n",
    "\n",
    "The joint probability of two events, $A$ and $B$, is a reformulation of the equation above.\n",
    "\n",
    "### $$ P(A \\cap B) = P(A|B) \\; P(B) $$\n",
    "\n",
    "If we want to know the probability that both $A$ and $B$ happen, we can multiply the probability that $B$ happens by the probability that $A$ occurs if $B$ does.\n",
    "\n",
    "**The Law of Total Probability**\n",
    "\n",
    "Let's say we want to know the probability of the event $B$ occurring across _all_ different events, $A$. For example, let's say that we are a judge presiding over a murder trial. $B$ is the event that the suspect's wallet was found at the scene of the murder. We have many hypotheses or possible scenarios in which the wallet is found at the murder scene, one being that the suspect was actually at the scene of the crime at the time of the murder.\n",
    "\n",
    "These different events, $A$ — our scenarios — are disjoint. The _total probability_ of $B$ is the probability across all of these scenarios that the wallet was found at the murder scene. In other words — regardless of which possible scenario $A$ — what is the overall probability that the wallet was at the murder scene?\n",
    "\n",
    "### $$ P(B) = \\sum_{i=1}^n P(B \\cap A_i) $$\n",
    "\n",
    "![total probability](./assets/images/output_27_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bayes-rule'></a>\n",
    "## Bayes' Theorem\n",
    "\n",
    "---\n",
    "\n",
    "Bayes' theorem relates the probability of $A$ given $B$ to the probability of $B$ given $A$. This rule is critical for performing statistical inference, as we'll see shortly. It's formulated as:\n",
    "\n",
    "### $$ P(A|B) = \\frac{P(B|A)\\;P(A)}{P(B)} $$\n",
    "\n",
    "Let's return to the courtroom example.\n",
    "\n",
    "Say $A$ is the event that the suspect is guilty.\n",
    "\n",
    "$B$ is the event that the suspect's wallet was found at the scene of the crime.\n",
    "\n",
    "Using Bayes' theorem, we phrase this as: The probability that the suspect is guilty given that the suspect's wallet was found at the scene of the crime is equivalent to the probability that the suspect's wallet was found there given that the suspect is guilty, times the probability that the suspect is guilty (without evidence), and divided by the total probability that the wallet is found at the scene of the crime.\n",
    "\n",
    "<a id='diachronic'></a>\n",
    "### The Diachronic Interpretation of Bayes' Theorem\n",
    "\n",
    "We can rewrite the formula for Bayes' theorem in the context of hypotheses and data, like we’ve already been doing with the courtroom example. The diachronic interpretation is for the probability of events _over time_, as in, the probability that an event changes over time as we collect new data.\n",
    "\n",
    "In this case, we have a model or statistic and we’re asking the probability of our model given the data that we’ve observed.\n",
    "\n",
    "### $$P\\left(model\\;|\\;data\\right) = \\frac{P\\left(data\\;|\\;model\\right)}{P(data)}\\; P\\left(model\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='freq-vs-bayes'></a>\n",
    "## Frequentist vs. Bayesian Probability\n",
    "\n",
    "---\n",
    "\n",
    "### Frequentism\n",
    "\n",
    "Frequentists believe that the \"true\" value of a statistic about a population (for example, the mean) is fixed and unknown. We can infer more about this \"true\" distribution by engaging in sampling, testing for effects, and studying relevant parameters of the population.\n",
    "\n",
    "Say we are flipping a coin and want to know the probability of heads. Frequentists formulate the probability of heads as a limit, deriving the true probability of heads from an infinite number of coin flips with that coin.\n",
    "\n",
    "### $$P(\\text{heads}) = \\lim_{\\text{# of coin flips} \\to \\infty} \\frac{\\text{# of heads}}{\\text{# of flips}}$$\n",
    "\n",
    "Alternatively, we can write this more generally as the number of times any event, $A$, occurs given an infinite number of observations/experiments (random samples from the event space).\n",
    "\n",
    "### $$P(A) = \\lim_{\\text{# of experiments} \\to \\infty} \\frac{\\text{# of occurrences of A}}{\\text{# of experiments}} $$\n",
    "\n",
    "### Bayesianism\n",
    "\n",
    "Bayesians believe that data inform us about the distribution of a statistic or event and that, as we receive more data, our view of the distribution can be updated, further confirming or denying our previous beliefs (but never in total certainty).\n",
    "\n",
    "For the coin flip example above, we would write out the probability of heads as our belief in the probability of getting heads given the evidence we have from observing coin flips.\n",
    "\n",
    "### $$ P(\\text{heads}) = \\frac{P(\\text{# of heads observed} \\;|\\; \\text{heads})}{P(\\text{# of heads observed})} P(\\text{heads}) $$\n",
    "\n",
    "Here, we're representing the probability of flipping with:\n",
    "\n",
    "Our **prior** belief (before observing flips) of the probability of flipping heads: $P(\\text{heads})$.\n",
    "\n",
    "The **likelihood** of the data we observe given the chance to flip heads: $P(\\text{# of heads observed} \\;|\\; \\text{heads})$.\n",
    "\n",
    "The **total probability** of observing that many heads in coin flip,s regardless of weighting (or rather, across all coin weightings): $P(\\text{# of heads observed})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='parts'></a>\n",
    "## Bayes' Theorem in Parts\n",
    "---\n",
    "\n",
    "Using the diachronic interpretation of Bayes' theorem, we can describe each part with its label, like in our coin flip example above.\n",
    "\n",
    "### $$P\\left(model\\;|\\;data\\right) = \\frac{P\\left(data\\;|\\;model\\right)}{P(data)}\\; P\\left(model\\right)$$\n",
    "\n",
    "**The Prior**\n",
    "\n",
    "### $$ \\text{prior} = P\\left(model\\right) $$\n",
    "\n",
    "The prior is our belief in the model given no additional information. This model could be as simple as a statistic, such as the mean we're measuring, or a complex regression. \n",
    "\n",
    "**The Likelihood**\n",
    "\n",
    "### $$ \\text{likelihood} = P\\left(data\\;|\\;model\\right) $$\n",
    "\n",
    "The likelihood is the probability of the data we observed occurring given the model. For example, assuming that a coin is biased toward heads with a mean rate of heads of 0.9, what is the likelihood that we observe 10 tails and two heads in 12 coin flips?\n",
    "\n",
    "The likelihood is, in fact, what frequentist statistical methods are measuring. \n",
    "\n",
    "**The Marginal Probability or Total Probability of the Data**\n",
    "\n",
    "### $$ \\text{marginal probability of data} = P(data) $$\n",
    "\n",
    "The marginal probability of the data is the probability that our data are observed regardless of what model we choose or believe in. You divide the likelihood by this value to ensure that we are only talking about our model within the context of the data occurring. We divide by this value to ensure that what we get on the other side is a true probability distribution — more on this later.\n",
    "\n",
    "**The Posterior**\n",
    "\n",
    "### $$ \\text{posterior} = P\\left(model\\;|\\;data\\right) $$\n",
    "\n",
    "The posterior is our _updated_ belief in the model given the new data we have observed. Bayesian statistics are all about updating a prior belief we have about the world with new data, so we're transforming our _prior_ belief into this new _posterior_ belief about the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='monty-hall'></a>\n",
    "\n",
    "## The Monty Hall Problem\n",
    "---\n",
    "\n",
    "The Monty Hall problem is a famous probability problem with an unintuitive solution. Framing it in a Bayesian context makes it much clearer!\n",
    "\n",
    "Open up the [Monty Hall notebook](./monty-hall.ipynb) and tackle the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional'></a>\n",
    "## Additional Bayesian Statistics Problems\n",
    "---\n",
    "\n",
    "As independent practice, you can tackle some more Bayesian statistics problems, including the:\n",
    "- Pregnancy screening problem.\n",
    "- Cookie jar problem.\n",
    "- German tank problem.\n",
    "- Dungeons & Dragons dice problems.\n",
    "- M&M's problem.\n",
    "\n",
    "These problems can be found in [this notebook](bayes-problems.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
