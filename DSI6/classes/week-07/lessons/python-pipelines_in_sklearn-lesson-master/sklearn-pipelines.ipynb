{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Pipelines in Scikit-Learn\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Learn what a scikit-learn pipeline is and the scenarios for which its useful.\n",
    "- Standardize data as part of a pipeline.\n",
    "- Use pipelines with training and testing data.\n",
    "- Practice object-oriented programming and building a custom transformation in scikit-learn.\n",
    "- Put the custom Titanic preprocessor into a pipeline.\n",
    "- Investigate the internals of scikit-learn pipelines.\n",
    "- Practice using the `make_pipeline()` function to easily create pipeline objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction](#intro)\n",
    "- [Loading the Pipeline Objects](#pipe-objects)\n",
    "- [Processing Steps for the Titanic Data](#steps)\n",
    "- [Standardizing Data as Part of a Pipeline](#standardize)\n",
    "- [Pipelines With Training and Testing Data](#pipe-train-test)\n",
    "- [Built-in Transformations and Preprocessing Steps](#built-in)\n",
    "- [Custom Transformations](#custom)\n",
    "- [Putting the Custom `TitanicPreprocessor()` in a Pipeline](#custom-pipe)\n",
    "- [Pipeline Internals](#internals)\n",
    "- [The `make_pipeline()` Convenience Function](#make-pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction to Pipelines\n",
    "\n",
    "---\n",
    "\n",
    "When working with data, the same process is often repeated multiple times and can become tedious to recode. A simple example of this is standardizing data before using regularized regression or other models.\n",
    "\n",
    "Luckily, scikit-learn has “pipelines\" that chain together multiple steps in a data analysis process. By constructing these, you can consolidate all of the steps in a process into a single object.\n",
    "\n",
    "This code-along introduces how to use pipelines and also serves as object-oriented programming practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('./datasets/titanic_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipe-objects'></a>\n",
    "\n",
    "## Loading the Pipeline Objects\n",
    "\n",
    "---\n",
    "\n",
    "From the `sklearn.pipeline` module, we’re going to import `Pipeline` and `make_pipeline`.\n",
    "\n",
    "`Pipeline` is the class object that will hold our data analysis process. The `make_pipeline()` function is a convenience method that takes in a series of estimators or preprocessing steps and returns a `Pipeline` object.\n",
    "\n",
    "We'll start with the more explicit construction using `Pipeline` and then move on to the convenience function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The term \"pipeline\" is jargon for a series of concatenated data transformations. Each stage of a pipeline builds off its predecessor (i.e., the output of a stage is plugged into the input of the next stage) and data flow through the pipeline from beginning to end.\n",
    "\n",
    "\n",
    "![pipeline](./assets/pipeline.png)\n",
    "\n",
    "---\n",
    "\n",
    "Pipelines provide a higher level of abstraction than the individual building blocks of a data science process and are a convenient way to organize analyses.\n",
    "\n",
    "**Let's take a look at the Titanic data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='steps'></a>\n",
    "\n",
    "## Processing Steps for the Titanic Data\n",
    "\n",
    "---\n",
    "\n",
    "There are some preprocessing steps we’ll complete before classifying whether or not passengers survived.\n",
    "\n",
    "1) Remove unwanted columns.\n",
    "    - Convert categorical string or numeric columns to dummy-coded columns.\n",
    "    - Standardize the predictor matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll do this manually and integrate it into the pipeline later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standardize'></a>\n",
    "## Using a Pipeline to Standardize the Data and Fit the Model\n",
    "\n",
    "---\n",
    "\n",
    "Now, we'll split the data up into the `x`, `y` predictor target format, standardize the `x` matrix, and fit a logistic regression model on “survived.”\n",
    "\n",
    "First, split into `x` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `LogisticRegression` and `StandardScaler` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to build a pipeline that can combine the steps above. Below, we create the `StandardScaler` and `LogisticRegression` objects, then combine them into the `Pipeline` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipelines combine both preprocessing and model-building steps into a single object**. \n",
    "\n",
    "Rather than manually building transformations and feeding them into the models, pipelines tie both of these steps together.\n",
    "\n",
    "Furthermore, pipelines are equipped with the methods of the final estimator step:\n",
    "\n",
    "- `fit()` methods.\n",
    "- `predict()` and/or `predict_proba()`.\n",
    "- `score()`.\n",
    "\n",
    "Use the pipeline to fit the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipe-train-test'></a>\n",
    "\n",
    "## Using Pipelines With Training and Testing Data\n",
    "\n",
    "---\n",
    "\n",
    "Next, we'll split up this data into training and testing sets. One of the greatest benefits to using pipelines is that the preprocessing steps retain the “fit” information from the training data, which can then be applied to the testing data.\n",
    "\n",
    "In the pipeline above, for example, the first standardization step is fit on the data we put into it. This means that the `StandardScaler` object takes the mean and standard deviation of the data and performs the procedure with those values.\n",
    "\n",
    "It _also_ means that, were we to predict or score on future data, the standard scaler in the pipeline would use the training data's mean and standard deviation to standardize the testing data. This is what we want! You don't want to standardize the training and testing data to their own means and standard deviations.\n",
    "\n",
    "There are many scenarios in which the testing data are actually data we haven’t collected yet. In these cases, you need to save the standardization procedure you used on the training set to use on this future data.\n",
    "\n",
    "Below, split up into training and testing `x` and `y`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the pipeline with the training data, then score it on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the sake of example, standardize the `x` train and `x` test separately and show how their normalization parameters differ.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='built-in'></a>\n",
    "## Built-In Transformations and Preprocessing Steps\n",
    "\n",
    "---\n",
    "\n",
    "Scikit-learn comes with a wide variety of useful classes for preprocessing your data prior to model fitting that can be put into pipelines.\n",
    "\n",
    "These can be found in the `sklearn.preprocessing` module. Familiarize yourself with these classes if you want to make use of them in your code. \n",
    "\n",
    "They include:\n",
    "\n",
    "**Data Manipulators**\n",
    "\n",
    "- `Binarizer`\n",
    "- `KernelCenterer`\n",
    "- `MaxAbsScaler`\n",
    "- `MinMaxScaler`\n",
    "- `Normalizer`\n",
    "- `OneHotEncoder`\n",
    "- `PolynomialFeatures`\n",
    "- `RobustScaler`\n",
    "- `StandardScaler`\n",
    "\n",
    "**Data Imputation**\n",
    "\n",
    "- `Imputer`\n",
    "\n",
    "**Function Transformer**\n",
    "\n",
    "- `FunctionTransformer`\n",
    "\n",
    "**Label Manipulators**\n",
    "\n",
    "- `LabelBinarizer`\n",
    "- `LabelEncoder`\n",
    "- `MultiLabelBinarizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='custom'></a>\n",
    "## Custom Transformations\n",
    "\n",
    "---\n",
    "\n",
    "It's not always possible to use a built-in transformation class to do what you want. In fact, it's likely that you're going to run into a scenario where you need a customized preprocessing step before model fitting.\n",
    "\n",
    "Let's take our Titanic data, for example. Say we wanted a preprocessor that would remove the columns we didn't want and create the dummy-coded columns before sending the set through to the standardization step.\n",
    "\n",
    "Custom transformer classes start with this template code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to import the template classes to create something that works like a scikit-learn class.\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Our \"TitanicPreprocessor\" is going to do the processing.\n",
    "class TitanicPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, *args):\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on this class:\n",
    "\n",
    "1) We have to load in the `BaseEstimator` and `TransformerMixin` classes for our preprocessor to inherit from in the class definition.\n",
    "    - The two required functions are `fit()` and `transform()`, which will be used to chain the processes together in our pipeline.\n",
    "    - The `*args` argument tells the function to expect an arbitrary number of arguments after whatever arguments were explicitly listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the dummy coding functions we wrote to the class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add a function to remove the unnecessary columns after dummy coding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the `transform()` function to perform these preprocessing steps, returning the new DataFrame.**\n",
    "\n",
    "Also, keep track of the final column names in a class attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='custom-pipe'></a>\n",
    "## Use the Custom `TitanicPreprocessor()` in a Pipeline\n",
    "---\n",
    "\n",
    "We'll put it before the `StandardScaler` in our original pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit on the training data and test on the testing data with the new pipeline. You'll need to create a new `x`, `y` with the original, non-manually preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='internals'></a>\n",
    "## Looking at Pipeline Internals With `.get_params()`.\n",
    "\n",
    "---\n",
    "\n",
    "Use the `.get_params()` function on the pipeline object to extract all of the parameters from the different steps as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pull out the feature names we stored by accessing our preprocessor object from the dictionary, then pulling out the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='make-pipe'></a>\n",
    "## The `make_pipeline()` Convenience Function\n",
    "\n",
    "---\n",
    "\n",
    "`make_pipeline()` does essentially the same thing as `Pipeline`, except you insert your objects as arguments and the function will create the pipeline for you. This means that it will name the steps itself, rather than you having to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
