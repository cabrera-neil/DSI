{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Topic Modeling and Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- See how search engines benefit from topic modeling.\n",
    "- Visualize a simple example of term vectors.\n",
    "- Walk through the coding of a simplified LDA topic model.\n",
    "- Gain an intuition for what LDA is doing.\n",
    "- Understand the pros and cons to LDA topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction: search engines and topic modeling](#intro)\n",
    "- [Term vector model visual example](#term-vector-ex)\n",
    "- [How LDA works](#how-lda-works)\n",
    "    - [Step 1: choose K topics](#step1)\n",
    "    - [Step 2: randomly assign words in documents to topics](#step2)\n",
    "    - [Step 3: word distributions of topics](#step3)\n",
    "    - [Step 4: reassignment of topics](#step4)\n",
    "    - [Step 5: getting words that define topics and topics that define documents](#step5)\n",
    "    - [Major caveat: oversimplified!](#caveat)\n",
    "- [LDA intuition](#lda-intuition)\n",
    "- [LDA challenges](#lda-challenges)\n",
    "- [LDA strengths](#lda-strengths)\n",
    "- [Other models similar to LDA](#other-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction: search engines and topic modeling\n",
    "---\n",
    "\n",
    "Search Engines use a variety of natural language processing techniques to provide an accurate query interface to documents on the internet.  Latent Dirichlet Allocation, which we will be discussing in this lecture, is just one of the many tools employed to improve search engine experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](https://snag.gy/lbsuV2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://snag.gy/YdgxKz.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://snag.gy/Ctr6OL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://snag.gy/ob9Um8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='term-vector-ex'></a>\n",
    "\n",
    "## Term vector model visual example\n",
    "\n",
    "---\n",
    "\n",
    "The code below plots a basic example of differentiating between topics using *vectors*. Specifically, the similarity between vectors can be used to infer the similarity or dissimilarity between topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### from scipy.interpolate import spline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Wedge\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.path import Path\n",
    "import pandas as pd\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = plt.gca()\n",
    "theta1, theta2 = 0, 90\n",
    "radius = .1\n",
    "center = (0, 0)\n",
    "\n",
    "# Main angle guide\n",
    "w1 = Wedge(center, radius, theta1, theta2, fill=False, linestyle='dashed')\n",
    "w1.set_linewidth(1)\n",
    "ax.add_artist(w1)\n",
    "\n",
    "# Vector examples\n",
    "ax.annotate('Dave', (0, 0), (0.5, 0.5),\n",
    "            arrowprops=dict(arrowstyle='<-', color=\"g\"),\n",
    "            xycoords='data', textcoords='axes fraction', color=\"g\")\n",
    "\n",
    "ax.annotate('Bigfoot', (0, 0), (0.35, 0.4),\n",
    "            arrowprops=dict(arrowstyle='<-', color=\"g\"),\n",
    "            xycoords='data', textcoords='axes fraction', color=\"g\")\n",
    "\n",
    "ax.annotate('Carrillo', (0, 0), (0.2, 0.3),\n",
    "            arrowprops=dict(arrowstyle='<-', color=\"g\"),\n",
    "            xycoords='data', textcoords='axes fraction', color=\"g\")\n",
    "\n",
    "ax.annotate('Calrissian', (0, 0), (0.15, 0.63),\n",
    "            arrowprops=dict(arrowstyle='<-', color=\"b\", linewidth=2.5),\n",
    "            xycoords='data', textcoords='axes fraction', color=\"b\")\n",
    "\n",
    "ax.annotate('Lando', (0, 0), (0.05, 0.6),\n",
    "            arrowprops=dict(arrowstyle='<-', color=\"b\", linewidth=2.5),\n",
    "            xycoords='data', textcoords='axes fraction', fontsize=12, color=\"b\")\n",
    "\n",
    "ax.annotate('Spock', (0, 0), (.6, 0.09),\n",
    "            arrowprops=dict(arrowstyle='<-', color=\"r\", linewidth=2.5),\n",
    "            xycoords='data', textcoords='axes fraction', color=\"r\")\n",
    "\n",
    "ax.annotate('Picard', (0, 0), (.7, 0.025),\n",
    "            arrowprops=dict(arrowstyle='<-', linewidth=2.5, color=\"r\"),\n",
    "            xycoords='data', textcoords='axes fraction', color=\"r\")\n",
    "\n",
    "# Remove splines on top and right\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "# ax.spines['bottom'].set_visible(False)\n",
    "# ax.get_xaxis().set_visible(False)\n",
    "# ax.get_yaxis().set_visible(False)\n",
    "ax.set_ylim([0, .25])\n",
    "ax.set_xlim([0, .25])\n",
    "ax.set_xmargin(.001)\n",
    "ax.set_ymargin(.001)\n",
    "ax.set_autoscale_on(False)\n",
    "\n",
    "\n",
    "# x = np.linspace(0, 2*np.pi, 100)\n",
    "# plt.plot(x)\n",
    "plt.title(\"Basic Term Vector Model\", fontsize=25, position=(0.5, 0.8))\n",
    "plt.ylabel(\"Star Wars\", fontsize=18, color=\"b\")\n",
    "plt.xlabel(\"Star Track\", fontsize=18, color=\"r\")\n",
    "\n",
    "\n",
    "plt.margins(0.005, 0.0001)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in our corpus is related to either \"Star Wars\" or \"Star Track\".  We use a vector space model to calculate the distance of each word to each topic.\n",
    "\n",
    "> Topics generated from an LDA model are actually a cluster of word probabilities, not clearly defined labels.  Simplifying word vectors like this, should give you a sense about the intuition of how **words vectors** relate to **topics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='how-lda-works'></a>\n",
    "## How LDA Works\n",
    "\n",
    "---\n",
    "\n",
    "_Abridged Explanation_\n",
    "\n",
    "![](https://snag.gy/aiSFrm.jpg)\n",
    "\n",
    "LDA isn't exactly straightforward when it comes to the math.  But we can explore this piecewise in more general terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "yelp = pd.read_csv('../datasets/yelp.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['len'] = yelp['text'].map(lambda t: len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#very naive method!\n",
    "yelp['words'] = yelp['text'].map(lambda t: len(t.replace('.',' ').split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>len</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>889</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1345</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>419</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny   len  words  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0   889    171  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  1345    274  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0    76     17  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0   419     79  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0   469    102  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "stopwords = stop_words.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split the reviews into good (5 stars) and bad (1 star) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = [list(TextBlob(x.lower()).words) for x in yelp[yelp['stars']==5].text.values[0:25]]\n",
    "bad = [list(TextBlob(x.lower()).words) for x in yelp[yelp['stars']==1].text.values[0:25]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'friend',\n",
       " 'went',\n",
       " 'delicious',\n",
       " 'food',\n",
       " 'garlic',\n",
       " 'knots',\n",
       " 'favorite',\n",
       " 'course',\n",
       " 'wine',\n",
       " 'going',\n",
       " 'alot']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = [[w for w in doc if w not in stopwords] for doc in good]\n",
    "bad = [[w for w in doc if w not in stopwords] for doc in bad]\n",
    "good[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "### Step 1: choose K topics\n",
    "\n",
    "Similar to the KNN algorithm. By setting K we are deciding up front on a preset number of topics to determine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "### Step 2: randomly assign words in documents to topics\n",
    "\n",
    "First let's denote the symbols for the different components:\n",
    "\n",
    "$D$ represents the documents, $d$ is a document.\n",
    "\n",
    "$W$ represents all the words in the documents, $w$ is a word.\n",
    "\n",
    "$Z$ represents the collection of our $K$ topics, and $z$ is one of those topics.\n",
    "\n",
    "We will start off with a completely random assignment of words to topics. Iterate through the documents $D$ and for each word $W$ in each document randomly assign the word to be in one of the $Z$ topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 2676\n"
     ]
    }
   ],
   "source": [
    "# get the documents and all the words:\n",
    "D = good+bad\n",
    "W = [w for wordlist in D for w in wordlist]\n",
    "print(len(D), len(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1410\n"
     ]
    }
   ],
   "source": [
    "# get the UNIQUE words:\n",
    "W_unique = list(np.unique(W))\n",
    "print(len(W_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the topics. Each topic will have a dictionary of counts of occurrances\n",
    "# of each word, which we will set to zero for now:\n",
    "Z = {i:{w:0 for w in W_unique} for i in range(K)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\": 0,\n",
       " \"'ll\": 0,\n",
       " \"'m\": 0,\n",
       " \"'re\": 0,\n",
       " \"'s\": 0,\n",
       " \"'ve\": 0,\n",
       " '1': 0,\n",
       " '1-1/2': 0,\n",
       " '1/2': 0,\n",
       " '1/3': 0,\n",
       " '10': 0,\n",
       " '100': 0,\n",
       " '11': 0,\n",
       " '115': 0,\n",
       " '15': 0,\n",
       " '150': 0,\n",
       " '16th': 0,\n",
       " '1pm': 0,\n",
       " '2': 0,\n",
       " '2.50': 0,\n",
       " '20': 0,\n",
       " '20-38': 0,\n",
       " '20mbs': 0,\n",
       " '25': 0,\n",
       " '2:05': 0,\n",
       " '2nd': 0,\n",
       " '3.79': 0,\n",
       " '3/4': 0,\n",
       " '30': 0,\n",
       " '39': 0,\n",
       " '4': 0,\n",
       " '40': 0,\n",
       " '45': 0,\n",
       " '5': 0,\n",
       " '5-20': 0,\n",
       " '5-6': 0,\n",
       " '5:50': 0,\n",
       " '5:52': 0,\n",
       " '6': 0,\n",
       " '60': 0,\n",
       " '64': 0,\n",
       " '6:02': 0,\n",
       " '6:42': 0,\n",
       " '6pm': 0,\n",
       " '70': 0,\n",
       " '7:30': 0,\n",
       " '7pm': 0,\n",
       " '7th': 0,\n",
       " '8': 0,\n",
       " '80': 0,\n",
       " '8:15': 0,\n",
       " '9': 0,\n",
       " '9:25': 0,\n",
       " 'absolute': 0,\n",
       " 'absolutely': 0,\n",
       " 'abstract': 0,\n",
       " 'absurd': 0,\n",
       " 'accepted': 0,\n",
       " 'accident': 0,\n",
       " 'according': 0,\n",
       " 'action': 0,\n",
       " 'add': 0,\n",
       " 'addition': 0,\n",
       " 'adequate': 0,\n",
       " 'adequately': 0,\n",
       " 'admit': 0,\n",
       " 'adorable': 0,\n",
       " 'adult': 0,\n",
       " 'advertised': 0,\n",
       " 'aged': 0,\n",
       " 'ages': 0,\n",
       " 'ahead': 0,\n",
       " 'ahi': 0,\n",
       " 'aid': 0,\n",
       " 'aj': 0,\n",
       " 'albeit': 0,\n",
       " 'allow': 0,\n",
       " 'alot': 0,\n",
       " 'alphagraphics': 0,\n",
       " 'alright': 0,\n",
       " 'alteration': 0,\n",
       " 'alternatives': 0,\n",
       " 'amazing': 0,\n",
       " 'ample': 0,\n",
       " 'animatronics': 0,\n",
       " 'annoyance': 0,\n",
       " 'annual': 0,\n",
       " 'anti': 0,\n",
       " 'anymore': 0,\n",
       " 'anythings': 0,\n",
       " 'anyways': 0,\n",
       " 'apologized': 0,\n",
       " 'apology': 0,\n",
       " 'appeal': 0,\n",
       " 'appetizer': 0,\n",
       " 'apples': 0,\n",
       " 'appointment': 0,\n",
       " 'area': 0,\n",
       " 'arizona': 0,\n",
       " 'arms': 0,\n",
       " 'arrive': 0,\n",
       " 'arrived': 0,\n",
       " 'arriving': 0,\n",
       " 'art': 0,\n",
       " 'articles': 0,\n",
       " 'artist': 0,\n",
       " 'ask': 0,\n",
       " 'asked': 0,\n",
       " 'asking': 0,\n",
       " 'asparagus': 0,\n",
       " 'ass': 0,\n",
       " 'associate': 0,\n",
       " 'assortment': 0,\n",
       " 'assure': 0,\n",
       " 'ate': 0,\n",
       " 'atm': 0,\n",
       " 'atmosphere': 0,\n",
       " 'attempts': 0,\n",
       " 'attention': 0,\n",
       " 'attentive': 0,\n",
       " 'automatically': 0,\n",
       " 'ave': 0,\n",
       " 'average': 0,\n",
       " 'avoid': 0,\n",
       " 'awaited': 0,\n",
       " 'away': 0,\n",
       " 'awesome': 0,\n",
       " 'awfully': 0,\n",
       " 'az': 0,\n",
       " 'baby': 0,\n",
       " 'bad': 0,\n",
       " 'baguettes': 0,\n",
       " 'baja': 0,\n",
       " 'baked': 0,\n",
       " 'balance': 0,\n",
       " 'ballparks': 0,\n",
       " 'balsamic': 0,\n",
       " 'banana': 0,\n",
       " 'banned': 0,\n",
       " 'bar': 0,\n",
       " 'bartender': 0,\n",
       " 'baseball': 0,\n",
       " 'based': 0,\n",
       " 'basically': 0,\n",
       " 'basil': 0,\n",
       " 'basket': 0,\n",
       " 'bbq': 0,\n",
       " 'beach': 0,\n",
       " 'beat': 0,\n",
       " 'beats': 0,\n",
       " 'beef': 0,\n",
       " 'beer': 0,\n",
       " 'began': 0,\n",
       " 'believe': 0,\n",
       " 'believers': 0,\n",
       " 'belly': 0,\n",
       " 'belt': 0,\n",
       " 'benefit': 0,\n",
       " 'benefits': 0,\n",
       " 'best': 0,\n",
       " 'bet': 0,\n",
       " 'betrayed': 0,\n",
       " 'better': 0,\n",
       " 'bianco': 0,\n",
       " 'big': 0,\n",
       " 'biggest': 0,\n",
       " 'biltmore': 0,\n",
       " 'birthday': 0,\n",
       " 'biscuits': 0,\n",
       " 'bit': 0,\n",
       " 'bitch': 0,\n",
       " 'bite': 0,\n",
       " 'bites': 0,\n",
       " 'blah': 0,\n",
       " 'blend': 0,\n",
       " 'bleu': 0,\n",
       " 'bloody': 0,\n",
       " 'blow': 0,\n",
       " 'bones': 0,\n",
       " 'bonus': 0,\n",
       " 'booklet-style': 0,\n",
       " 'boss': 0,\n",
       " 'bottle': 0,\n",
       " 'bout': 0,\n",
       " 'boutique': 0,\n",
       " 'box': 0,\n",
       " 'boys': 0,\n",
       " 'brag': 0,\n",
       " 'brake': 0,\n",
       " 'brand': 0,\n",
       " 'branding': 0,\n",
       " 'bread': 0,\n",
       " 'breakfast': 0,\n",
       " 'bride': 0,\n",
       " 'bright': 0,\n",
       " 'bring': 0,\n",
       " 'bringing': 0,\n",
       " 'broke': 0,\n",
       " 'brought': 0,\n",
       " 'brunette': 0,\n",
       " 'buck': 0,\n",
       " 'bucks': 0,\n",
       " 'buddy': 0,\n",
       " 'building': 0,\n",
       " 'bulk': 0,\n",
       " 'buns': 0,\n",
       " 'burgers': 0,\n",
       " 'burlington': 0,\n",
       " 'burro': 0,\n",
       " 'burros': 0,\n",
       " 'busy': 0,\n",
       " 'butter': 0,\n",
       " 'butts': 0,\n",
       " 'buy': 0,\n",
       " 'buying/putting': 0,\n",
       " 'ca': 0,\n",
       " 'cake': 0,\n",
       " 'calamari': 0,\n",
       " 'called': 0,\n",
       " 'calzone': 0,\n",
       " 'came': 0,\n",
       " 'campuses': 0,\n",
       " 'candles': 0,\n",
       " 'cans': 0,\n",
       " 'car': 0,\n",
       " 'carafe': 0,\n",
       " 'carbonara': 0,\n",
       " 'card': 0,\n",
       " 'cards': 0,\n",
       " 'cardstock': 0,\n",
       " 'care': 0,\n",
       " 'careful': 0,\n",
       " 'carefully': 0,\n",
       " 'cares': 0,\n",
       " 'cars': 0,\n",
       " 'carton': 0,\n",
       " 'casa': 0,\n",
       " 'case': 0,\n",
       " 'cash': 0,\n",
       " 'catalogs': 0,\n",
       " 'catch': 0,\n",
       " 'catering': 0,\n",
       " 'caught': 0,\n",
       " 'cave': 0,\n",
       " 'cell': 0,\n",
       " 'certain': 0,\n",
       " 'certainly': 0,\n",
       " 'chain': 0,\n",
       " 'changed': 0,\n",
       " 'chaparral': 0,\n",
       " 'charge': 0,\n",
       " 'charged': 0,\n",
       " 'charger': 0,\n",
       " 'charging': 0,\n",
       " 'charming': 0,\n",
       " 'chased': 0,\n",
       " 'cheap': 0,\n",
       " 'check': 0,\n",
       " 'checked': 0,\n",
       " 'cheese': 0,\n",
       " 'chef': 0,\n",
       " 'chefs': 0,\n",
       " 'chewy': 0,\n",
       " 'chick': 0,\n",
       " 'chicken': 0,\n",
       " 'children': 0,\n",
       " 'china': 0,\n",
       " 'chocolate': 0,\n",
       " 'choices': 0,\n",
       " 'choose': 0,\n",
       " 'chopped': 0,\n",
       " 'christ': 0,\n",
       " 'church': 0,\n",
       " 'cibo': 0,\n",
       " 'cinammoness': 0,\n",
       " 'class': 0,\n",
       " 'clean': 0,\n",
       " 'cleaned': 0,\n",
       " 'cleanliness': 0,\n",
       " 'clear': 0,\n",
       " 'clearance': 0,\n",
       " 'close': 0,\n",
       " 'clothing': 0,\n",
       " 'clumsiness': 0,\n",
       " 'clumsy': 0,\n",
       " 'cocktail': 0,\n",
       " 'cocktails': 0,\n",
       " 'coffee': 0,\n",
       " 'cold': 0,\n",
       " 'color': 0,\n",
       " 'colored': 0,\n",
       " 'colors/design': 0,\n",
       " 'come': 0,\n",
       " 'comeback': 0,\n",
       " 'comes': 0,\n",
       " 'comfortable': 0,\n",
       " 'comfy': 0,\n",
       " 'coming': 0,\n",
       " 'comp': 0,\n",
       " 'companions': 0,\n",
       " 'company': 0,\n",
       " 'comped': 0,\n",
       " 'competely': 0,\n",
       " 'complaint': 0,\n",
       " 'complaints': 0,\n",
       " 'complete': 0,\n",
       " 'complimented': 0,\n",
       " 'compliments': 0,\n",
       " 'concert': 0,\n",
       " 'condesa': 0,\n",
       " 'connecting': 0,\n",
       " 'consistency': 0,\n",
       " 'construction': 0,\n",
       " 'consumer': 0,\n",
       " 'containers': 0,\n",
       " 'convenient': 0,\n",
       " 'conversations': 0,\n",
       " 'convert': 0,\n",
       " 'cooked': 0,\n",
       " 'cookie': 0,\n",
       " 'cookies': 0,\n",
       " 'cooking': 0,\n",
       " 'cool': 0,\n",
       " 'copied': 0,\n",
       " 'copypaste': 0,\n",
       " 'cordon': 0,\n",
       " 'cork': 0,\n",
       " 'cost': 0,\n",
       " 'counter': 0,\n",
       " 'coup': 0,\n",
       " 'couple': 0,\n",
       " 'course': 0,\n",
       " 'crabs': 0,\n",
       " 'crackers': 0,\n",
       " 'crafted': 0,\n",
       " 'crammed': 0,\n",
       " 'cramped': 0,\n",
       " 'crap': 0,\n",
       " 'cream': 0,\n",
       " 'creamiest': 0,\n",
       " 'creating': 0,\n",
       " 'creativity': 0,\n",
       " 'cred': 0,\n",
       " 'credit': 0,\n",
       " 'creek': 0,\n",
       " 'crew': 0,\n",
       " 'crisp': 0,\n",
       " 'crispy': 0,\n",
       " 'crowded': 0,\n",
       " 'crust': 0,\n",
       " 'cucina': 0,\n",
       " 'customer': 0,\n",
       " 'customers': 0,\n",
       " 'cut': 0,\n",
       " 'cute': 0,\n",
       " 'cutting': 0,\n",
       " 'dakota': 0,\n",
       " 'damn': 0,\n",
       " 'dance': 0,\n",
       " 'date': 0,\n",
       " 'daughter': 0,\n",
       " 'day': 0,\n",
       " 'days': 0,\n",
       " 'dead': 0,\n",
       " 'dealer': 0,\n",
       " 'deals': 0,\n",
       " 'dealt': 0,\n",
       " 'dean': 0,\n",
       " 'debating': 0,\n",
       " 'decided': 0,\n",
       " 'decision': 0,\n",
       " 'decline': 0,\n",
       " 'decor': 0,\n",
       " 'deep': 0,\n",
       " 'deep-fried': 0,\n",
       " 'definitely': 0,\n",
       " 'deli': 0,\n",
       " 'delicious': 0,\n",
       " 'deliciously': 0,\n",
       " 'delight': 0,\n",
       " 'delivered': 0,\n",
       " 'delivery': 0,\n",
       " 'deluca': 0,\n",
       " 'demand': 0,\n",
       " 'deported': 0,\n",
       " 'dept': 0,\n",
       " 'des': 0,\n",
       " 'desert': 0,\n",
       " 'deserves': 0,\n",
       " 'design': 0,\n",
       " 'designer': 0,\n",
       " 'designing': 0,\n",
       " 'desolate': 0,\n",
       " 'despite': 0,\n",
       " 'dessert': 0,\n",
       " 'desserts': 0,\n",
       " 'destination': 0,\n",
       " 'devoured': 0,\n",
       " 'did': 0,\n",
       " 'diego': 0,\n",
       " 'different': 0,\n",
       " 'differentiation': 0,\n",
       " 'difficult': 0,\n",
       " 'dining': 0,\n",
       " 'dinner': 0,\n",
       " 'disagree': 0,\n",
       " 'disappointed': 0,\n",
       " 'disappointing': 0,\n",
       " 'disclaimer': 0,\n",
       " 'discount': 0,\n",
       " 'disgusting': 0,\n",
       " 'dishes': 0,\n",
       " 'display': 0,\n",
       " 'disrespected': 0,\n",
       " 'distraction': 0,\n",
       " 'does': 0,\n",
       " 'doesnt': 0,\n",
       " 'dog': 0,\n",
       " 'dogfish': 0,\n",
       " 'dogs': 0,\n",
       " 'doing': 0,\n",
       " 'dollar': 0,\n",
       " 'dollars': 0,\n",
       " 'dont': 0,\n",
       " 'door': 0,\n",
       " 'dragon': 0,\n",
       " 'drink': 0,\n",
       " 'drinks': 0,\n",
       " 'drive': 0,\n",
       " 'driving': 0,\n",
       " 'drooling': 0,\n",
       " 'drop': 0,\n",
       " 'duck': 0,\n",
       " 'ducks': 0,\n",
       " 'dumplings': 0,\n",
       " 'earlier': 0,\n",
       " 'early': 0,\n",
       " 'earned': 0,\n",
       " 'easier': 0,\n",
       " 'easy': 0,\n",
       " 'eat': 0,\n",
       " 'eaten': 0,\n",
       " 'eating': 0,\n",
       " 'efax': 0,\n",
       " 'egg': 0,\n",
       " 'eggs': 0,\n",
       " 'elected': 0,\n",
       " 'eloquent': 0,\n",
       " 'elsa': 0,\n",
       " 'embarrassed': 0,\n",
       " 'embraces': 0,\n",
       " 'employee': 0,\n",
       " 'employees': 0,\n",
       " 'endless': 0,\n",
       " 'ends': 0,\n",
       " 'english': 0,\n",
       " 'ensure': 0,\n",
       " 'entire': 0,\n",
       " 'entirely': 0,\n",
       " 'entrees': 0,\n",
       " 'episode': 0,\n",
       " 'evening': 0,\n",
       " 'event': 0,\n",
       " 'events': 0,\n",
       " 'exactly': 0,\n",
       " 'excellent': 0,\n",
       " 'excited': 0,\n",
       " 'excuse': 0,\n",
       " 'exists': 0,\n",
       " 'expect': 0,\n",
       " 'expectation': 0,\n",
       " 'expectations': 0,\n",
       " 'expected': 0,\n",
       " 'expensive': 0,\n",
       " 'experience': 0,\n",
       " 'experiences': 0,\n",
       " 'expert': 0,\n",
       " 'explain': 0,\n",
       " 'explained': 0,\n",
       " 'extra': 0,\n",
       " 'extreme': 0,\n",
       " 'eye': 0,\n",
       " 'fabulous': 0,\n",
       " 'face': 0,\n",
       " 'facebook': 0,\n",
       " 'fact': 0,\n",
       " 'fair': 0,\n",
       " 'fake': 0,\n",
       " 'families': 0,\n",
       " 'family': 0,\n",
       " 'fan': 0,\n",
       " 'fancier': 0,\n",
       " 'fancy': 0,\n",
       " 'far': 0,\n",
       " 'fast-food': 0,\n",
       " 'faster': 0,\n",
       " 'fat': 0,\n",
       " 'fault': 0,\n",
       " 'favor': 0,\n",
       " 'favorite': 0,\n",
       " 'favorites': 0,\n",
       " 'fax': 0,\n",
       " 'faxes': 0,\n",
       " 'features': 0,\n",
       " 'feel': 0,\n",
       " 'feeling': 0,\n",
       " 'feelings': 0,\n",
       " 'fees': 0,\n",
       " 'felt': 0,\n",
       " 'fenced': 0,\n",
       " 'fields': 0,\n",
       " 'filet': 0,\n",
       " 'filling': 0,\n",
       " 'fills': 0,\n",
       " 'final': 0,\n",
       " 'finally': 0,\n",
       " 'finding': 0,\n",
       " 'finds': 0,\n",
       " 'fine': 0,\n",
       " 'finished': 0,\n",
       " 'fish': 0,\n",
       " 'fishy': 0,\n",
       " 'fit': 0,\n",
       " 'fits': 0,\n",
       " 'fitting': 0,\n",
       " 'fixing': 0,\n",
       " 'flat-out': 0,\n",
       " 'flavor': 0,\n",
       " 'flavorful': 0,\n",
       " 'flight': 0,\n",
       " 'folk': 0,\n",
       " 'food': 0,\n",
       " 'food=just': 0,\n",
       " 'foods': 0,\n",
       " 'forced': 0,\n",
       " 'forcing': 0,\n",
       " 'forever': 0,\n",
       " 'forget': 0,\n",
       " 'forgot': 0,\n",
       " 'fortunately': 0,\n",
       " 'frantic': 0,\n",
       " 'fred': 0,\n",
       " 'free': 0,\n",
       " 'french': 0,\n",
       " 'fresh': 0,\n",
       " 'freshest': 0,\n",
       " 'frida': 0,\n",
       " 'friday': 0,\n",
       " 'fried': 0,\n",
       " 'friend': 0,\n",
       " 'friendly': 0,\n",
       " 'friends': 0,\n",
       " 'fruit': 0,\n",
       " 'fucking': 0,\n",
       " 'fun': 0,\n",
       " 'furthermore': 0,\n",
       " 'future': 0,\n",
       " 'gainey': 0,\n",
       " 'garden': 0,\n",
       " 'garlic': 0,\n",
       " 'gatos': 0,\n",
       " 'gaudy': 0,\n",
       " 'gave': 0,\n",
       " 'general': 0,\n",
       " 'getting': 0,\n",
       " 'ghetto': 0,\n",
       " 'giant': 0,\n",
       " 'gift': 0,\n",
       " 'girl': 0,\n",
       " 'girlfriend': 0,\n",
       " 'girls': 0,\n",
       " 'giving': 0,\n",
       " 'glasses': 0,\n",
       " 'god': 0,\n",
       " 'goes': 0,\n",
       " 'going': 0,\n",
       " 'gone': 0,\n",
       " 'good': 0,\n",
       " 'goodness': 0,\n",
       " 'goods': 0,\n",
       " 'goona': 0,\n",
       " 'got': 0,\n",
       " 'gotten': 0,\n",
       " 'gourmet': 0,\n",
       " 'grand': 0,\n",
       " 'grande': 0,\n",
       " 'gravy': 0,\n",
       " 'great': 0,\n",
       " 'green': 0,\n",
       " 'greeted': 0,\n",
       " 'grew': 0,\n",
       " 'griddled': 0,\n",
       " 'grimaldi': 0,\n",
       " 'griping': 0,\n",
       " 'gristle': 0,\n",
       " 'grounds': 0,\n",
       " 'group': 0,\n",
       " 'groupon': 0,\n",
       " 'guard': 0,\n",
       " 'guess': 0,\n",
       " 'guy': 0,\n",
       " 'guys': 0,\n",
       " 'half': 0,\n",
       " 'ham': 0,\n",
       " 'hand': 0,\n",
       " 'handed': 0,\n",
       " 'handmade': 0,\n",
       " 'hang': 0,\n",
       " 'hanging': 0,\n",
       " 'happy': 0,\n",
       " 'hate': 0,\n",
       " 'having': 0,\n",
       " 'head': 0,\n",
       " 'headed': 0,\n",
       " 'heading': 0,\n",
       " 'heads': 0,\n",
       " 'heafty': 0,\n",
       " 'healthy': 0,\n",
       " 'heard': 0,\n",
       " 'heavenly': 0,\n",
       " 'hello': 0,\n",
       " 'help': 0,\n",
       " 'helped': 0,\n",
       " 'helpful': 0,\n",
       " 'helping': 0,\n",
       " 'helps': 0,\n",
       " 'hideous': 0,\n",
       " 'high': 0,\n",
       " 'highland': 0,\n",
       " 'highly': 0,\n",
       " 'hit': 0,\n",
       " 'hmmmmm': 0,\n",
       " 'hmmmmmm': 0,\n",
       " 'hold': 0,\n",
       " 'home': 0,\n",
       " 'homes': 0,\n",
       " 'honestly': 0,\n",
       " 'hong': 0,\n",
       " 'hoping': 0,\n",
       " 'horchata': 0,\n",
       " 'horrendous': 0,\n",
       " 'horrible': 0,\n",
       " 'horse': 0,\n",
       " 'host': 0,\n",
       " 'hostess': 0,\n",
       " 'hot': 0,\n",
       " 'hotel': 0,\n",
       " 'hour': 0,\n",
       " 'hours': 0,\n",
       " 'house': 0,\n",
       " 'hubbys': 0,\n",
       " 'huge': 0,\n",
       " 'hungry': 0,\n",
       " 'hyatt': 0,\n",
       " 'ice': 0,\n",
       " 'idea': 0,\n",
       " 'im': 0,\n",
       " 'important': 0,\n",
       " 'impress': 0,\n",
       " 'incredible': 0,\n",
       " 'incredibly': 0,\n",
       " 'individual': 0,\n",
       " 'inedible': 0,\n",
       " 'inevitable': 0,\n",
       " 'inexpensive': 0,\n",
       " 'informed': 0,\n",
       " 'infront': 0,\n",
       " 'ingredients': 0,\n",
       " 'inside': 0,\n",
       " 'inspected': 0,\n",
       " 'installed': 0,\n",
       " 'instead': 0,\n",
       " 'intercontinental': 0,\n",
       " 'interior': 0,\n",
       " 'internet': 0,\n",
       " 'invalidating': 0,\n",
       " 'invitation': 0,\n",
       " 'invitations': 0,\n",
       " 'issues': 0,\n",
       " 'italian': 0,\n",
       " 'items': 0,\n",
       " 'ixtapa-zihuatenejo': 0,\n",
       " 'jack-in-the-box': 0,\n",
       " 'jason': 0,\n",
       " 'jelly': 0,\n",
       " 'jersey': 0,\n",
       " 'jesus': 0,\n",
       " 'job': 0,\n",
       " 'jobs': 0,\n",
       " 'joe': 0,\n",
       " 'joining': 0,\n",
       " 'joint': 0,\n",
       " 'jon': 0,\n",
       " 'joy': 0,\n",
       " 'just': 0,\n",
       " 'justify': 0,\n",
       " 'kahlo': 0,\n",
       " 'kale': 0,\n",
       " 'kale-aid': 0,\n",
       " 'keeping': 0,\n",
       " 'kept': 0,\n",
       " 'kidding': 0,\n",
       " 'kids': 0,\n",
       " 'kind': 0,\n",
       " 'kinkos': 0,\n",
       " 'kitchen': 0,\n",
       " 'knew': 0,\n",
       " 'knots': 0,\n",
       " 'know': 0,\n",
       " 'knowing': 0,\n",
       " 'known': 0,\n",
       " 'kong': 0,\n",
       " 'kool-aid': 0,\n",
       " 'la': 0,\n",
       " 'lack': 0,\n",
       " 'lady': 0,\n",
       " 'laid': 0,\n",
       " 'lake': 0,\n",
       " 'large': 0,\n",
       " 'later': 0,\n",
       " 'laugh': 0,\n",
       " 'layaway': 0,\n",
       " 'leaped': 0,\n",
       " 'learned': 0,\n",
       " 'leaves': 0,\n",
       " 'left': 0,\n",
       " 'leg': 0,\n",
       " 'let': 0,\n",
       " 'license': 0,\n",
       " 'lie': 0,\n",
       " 'life': 0,\n",
       " 'life-changing': 0,\n",
       " 'light-up': 0,\n",
       " 'like': 0,\n",
       " 'liked': 0,\n",
       " 'limited': 0,\n",
       " 'line': 0,\n",
       " 'list': 0,\n",
       " 'listening': 0,\n",
       " 'litmus': 0,\n",
       " 'little': 0,\n",
       " 'live': 0,\n",
       " 'lived': 0,\n",
       " 'load': 0,\n",
       " 'located': 0,\n",
       " 'locations': 0,\n",
       " 'lolo': 0,\n",
       " 'long': 0,\n",
       " 'longer': 0,\n",
       " 'look': 0,\n",
       " 'looked': 0,\n",
       " 'looking': 0,\n",
       " 'looks': 0,\n",
       " 'los': 0,\n",
       " 'lose': 0,\n",
       " 'lot': 0,\n",
       " 'lots': 0,\n",
       " 'loud': 0,\n",
       " 'love': 0,\n",
       " 'loved': 0,\n",
       " 'lovely': 0,\n",
       " 'low': 0,\n",
       " 'lowdown': 0,\n",
       " 'luck': 0,\n",
       " 'luggage': 0,\n",
       " 'lunch': 0,\n",
       " 'lunches': 0,\n",
       " 'lux': 0,\n",
       " 'lying': 0,\n",
       " 'm': 0,\n",
       " \"m'fin\": 0,\n",
       " 'macaroni': 0,\n",
       " 'maestro': 0,\n",
       " 'mahi': 0,\n",
       " 'mail': 0,\n",
       " 'mailing': 0,\n",
       " 'make': 0,\n",
       " 'makes': 0,\n",
       " 'making': 0,\n",
       " 'man': 0,\n",
       " 'manage': 0,\n",
       " 'management': 0,\n",
       " 'manager': 0,\n",
       " 'managers': 0,\n",
       " 'marvelous': 0,\n",
       " 'mary': 0,\n",
       " 'mashed': 0,\n",
       " 'matter': 0,\n",
       " 'maybe': 0,\n",
       " 'meal': 0,\n",
       " 'meals': 0,\n",
       " 'mean': 0,\n",
       " 'means': 0,\n",
       " 'meat': 0,\n",
       " 'meeting': 0,\n",
       " 'melted': 0,\n",
       " 'member': 0,\n",
       " 'mention': 0,\n",
       " 'mentioned': 0,\n",
       " 'menu': 0,\n",
       " 'menus': 0,\n",
       " 'mexican': 0,\n",
       " 'milk': 0,\n",
       " 'min': 0,\n",
       " 'minute': 0,\n",
       " 'minutes': 0,\n",
       " 'miss': 0,\n",
       " 'missing': 0,\n",
       " 'mistake': 0,\n",
       " 'mistakes': 0,\n",
       " 'mitts': 0,\n",
       " 'mmmm': 0,\n",
       " 'modest': 0,\n",
       " 'mole': 0,\n",
       " 'mom': 0,\n",
       " 'money': 0,\n",
       " 'monotone': 0,\n",
       " 'month': 0,\n",
       " 'months': 0,\n",
       " 'morning': 0,\n",
       " 'mornings': 0,\n",
       " 'mortons': 0,\n",
       " 'mozzarella': 0,\n",
       " 'mushrooms': 0,\n",
       " 'music': 0,\n",
       " 'n': 0,\n",
       " \"n't\": 0,\n",
       " 'na': 0,\n",
       " 'nailed': 0,\n",
       " 'near': 0,\n",
       " 'nearby': 0,\n",
       " 'nearly': 0,\n",
       " 'need': 0,\n",
       " 'needed': 0,\n",
       " 'needing': 0,\n",
       " 'new': 0,\n",
       " 'ni-i-i-ice': 0,\n",
       " 'night': 0,\n",
       " 'no-nonsense': 0,\n",
       " 'nobuo': 0,\n",
       " 'noise=loud': 0,\n",
       " 'noodles': 0,\n",
       " 'normally': 0,\n",
       " 'noted': 0,\n",
       " 'nuts': 0,\n",
       " 'oakville': 0,\n",
       " 'obligations': 0,\n",
       " 'occasion': 0,\n",
       " 'odor': 0,\n",
       " 'offer': 0,\n",
       " 'office': 0,\n",
       " 'oh': 0,\n",
       " 'oh-la-la': 0,\n",
       " 'oil': 0,\n",
       " 'ok': 0,\n",
       " 'okay': 0,\n",
       " 'old': 0,\n",
       " 'oldish': 0,\n",
       " 'one-star': 0,\n",
       " 'ones': 0,\n",
       " 'onion': 0,\n",
       " 'onions': 0,\n",
       " 'online': 0,\n",
       " 'onsite': 0,\n",
       " 'ony': 0,\n",
       " 'open': 0,\n",
       " 'opened': 0,\n",
       " 'opening': 0,\n",
       " 'opinion': 0,\n",
       " 'oprah': 0,\n",
       " 'options': 0,\n",
       " 'order': 0,\n",
       " 'ordered': 0,\n",
       " 'orders': 0,\n",
       " 'ordinary': 0,\n",
       " 'oregano': 0,\n",
       " 'organizes': 0,\n",
       " 'orgy': 0,\n",
       " 'originally': 0,\n",
       " 'out-of-town': 0,\n",
       " 'outdated': 0,\n",
       " 'outrageous': 0,\n",
       " 'outside': 0,\n",
       " 'oven': 0,\n",
       " 'overall': 0,\n",
       " 'overcharged': 0,\n",
       " 'overcooked': 0,\n",
       " 'overlooking': 0,\n",
       " 'overpriced': 0,\n",
       " 'owed': 0,\n",
       " 'owner': 0,\n",
       " 'owners': 0,\n",
       " 'owns': 0,\n",
       " 'packed': 0,\n",
       " 'pads': 0,\n",
       " 'page': 0,\n",
       " 'paid': 0,\n",
       " 'painting': 0,\n",
       " 'paintings': 0,\n",
       " 'pairing': 0,\n",
       " 'palm': 0,\n",
       " 'pan': 0,\n",
       " 'panini': 0,\n",
       " 'par': 0,\n",
       " 'paradise': 0,\n",
       " 'park': 0,\n",
       " 'parking': 0,\n",
       " 'parmesan': 0,\n",
       " 'participating': 0,\n",
       " 'particularly': 0,\n",
       " 'party': 0,\n",
       " 'pass': 0,\n",
       " 'past': 0,\n",
       " 'pasta': 0,\n",
       " 'pastries': 0,\n",
       " 'paths': 0,\n",
       " 'patio': 0,\n",
       " 'patronizing': 0,\n",
       " 'pay': 0,\n",
       " 'pecans': 0,\n",
       " 'people': 0,\n",
       " 'pepper': 0,\n",
       " 'peppers': 0,\n",
       " 'peppery-sweet': 0,\n",
       " 'perfect': 0,\n",
       " 'perfection': 0,\n",
       " 'perfectly': 0,\n",
       " 'perimeters': 0,\n",
       " 'permutations': 0,\n",
       " 'person': 0,\n",
       " 'personal': 0,\n",
       " 'personality': 0,\n",
       " 'perspective': 0,\n",
       " 'petello': 0,\n",
       " 'phenomenal': 0,\n",
       " 'phoenix': 0,\n",
       " 'phone': 0,\n",
       " 'piccola': 0,\n",
       " 'picked': 0,\n",
       " 'pickup': 0,\n",
       " 'picnic': 0,\n",
       " 'piece': 0,\n",
       " 'pieces': 0,\n",
       " 'pinkeye': 0,\n",
       " 'pissed': 0,\n",
       " 'pittsburgh': 0,\n",
       " 'pizza': 0,\n",
       " 'pizzas': 0,\n",
       " 'pizzeria': 0,\n",
       " 'place': 0,\n",
       " 'placed': 0,\n",
       " 'places': 0,\n",
       " 'placing': 0,\n",
       " 'plain': 0,\n",
       " 'plan': 0,\n",
       " 'plant': 0,\n",
       " 'plate': 0,\n",
       " 'play': 0,\n",
       " 'pleasant': 0,\n",
       " 'pleasantly': 0,\n",
       " 'pleasure': 0,\n",
       " 'pm': 0,\n",
       " 'poblano': 0,\n",
       " 'point': 0,\n",
       " 'poise': 0,\n",
       " 'police': 0,\n",
       " 'poopy-pick': 0,\n",
       " 'poorer': 0,\n",
       " 'popping': 0,\n",
       " 'pork': 0,\n",
       " 'portobello': 0,\n",
       " 'positive': 0,\n",
       " 'post': 0,\n",
       " 'potatoes': 0,\n",
       " 'premises': 0,\n",
       " 'prepared': 0,\n",
       " 'presents': 0,\n",
       " 'pretentious': 0,\n",
       " 'pretty': 0,\n",
       " 'price': 0,\n",
       " 'priced': 0,\n",
       " 'prices': 0,\n",
       " 'pricing': 0,\n",
       " 'principle': 0,\n",
       " 'print': 0,\n",
       " 'printing': 0,\n",
       " 'probably': 0,\n",
       " 'problem': 0,\n",
       " 'professional': 0,\n",
       " 'programs': 0,\n",
       " 'promotes': 0,\n",
       " 'providing': 0,\n",
       " 'pull': 0,\n",
       " 'purveyor': 0,\n",
       " 'quality': 0,\n",
       " 'quandary': 0,\n",
       " 'quesadillas': 0,\n",
       " 'question': 0,\n",
       " 'quick': 0,\n",
       " 'quickly': 0,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wife', 4],\n",
       " ['took', 3],\n",
       " ['birthday', 0],\n",
       " ['breakfast', 1],\n",
       " ['excellent', 0],\n",
       " ['weather', 0],\n",
       " ['perfect', 2],\n",
       " ['sitting', 4],\n",
       " ['outside', 4],\n",
       " ['overlooking', 0],\n",
       " ['grounds', 1],\n",
       " ['absolute', 4],\n",
       " ['pleasure', 3],\n",
       " ['waitress', 0],\n",
       " ['excellent', 2],\n",
       " ['food', 0],\n",
       " ['arrived', 1],\n",
       " ['quickly', 2],\n",
       " ['semi-busy', 2],\n",
       " ['saturday', 0],\n",
       " ['morning', 4],\n",
       " ['looked', 3],\n",
       " ['like', 4],\n",
       " ['place', 2],\n",
       " ['fills', 3],\n",
       " ['pretty', 1],\n",
       " ['quickly', 2],\n",
       " ['earlier', 4],\n",
       " ['better', 3],\n",
       " ['favor', 1],\n",
       " ['bloody', 2],\n",
       " ['mary', 3],\n",
       " ['phenomenal', 2],\n",
       " ['simply', 1],\n",
       " ['best', 3],\n",
       " [\"'ve\", 2],\n",
       " [\"'m\", 1],\n",
       " ['pretty', 0],\n",
       " ['sure', 4],\n",
       " ['use', 3],\n",
       " ['ingredients', 2],\n",
       " ['garden', 1],\n",
       " ['blend', 2],\n",
       " ['fresh', 3],\n",
       " ['order', 0],\n",
       " ['amazing', 3],\n",
       " ['menu', 0],\n",
       " ['looks', 0],\n",
       " ['excellent', 4],\n",
       " ['white', 2],\n",
       " ['truffle', 3],\n",
       " ['scrambled', 0],\n",
       " ['eggs', 3],\n",
       " ['vegetable', 2],\n",
       " ['skillet', 4],\n",
       " ['tasty', 2],\n",
       " ['delicious', 0],\n",
       " ['came', 0],\n",
       " ['2', 2],\n",
       " ['pieces', 0],\n",
       " ['griddled', 0],\n",
       " ['bread', 2],\n",
       " ['amazing', 2],\n",
       " ['absolutely', 2],\n",
       " ['meal', 3],\n",
       " ['complete', 4],\n",
       " ['best', 3],\n",
       " ['toast', 2],\n",
       " [\"'ve\", 0],\n",
       " ['ca', 1],\n",
       " [\"n't\", 1],\n",
       " ['wait', 3]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# go through all the words in all the documents, and randomly assign each\n",
    "# one to one of the topics:\n",
    "D_z = [[[w, np.random.choice(range(K))] for w in d] for d in D]\n",
    "D_z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the topic dictionaries:\n",
    "for d in D_z:\n",
    "    for w, z in d:  \n",
    "        Z[z][w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "### Step 3: word distributions of topics\n",
    "\n",
    "Now that we have randomly assigned words to topics, we have the word distributions of different topics. These distributions indicate the probability of a word being in a topic. We can write out the probability of some given word being selected given a choice of one of our topics:\n",
    "\n",
    "### $$ P(\\text{word } w \\;|\\; \\text{topic } z) $$\n",
    "\n",
    "We also have the probability of *topic* occurance given a specific document, which is the proportion of words in a document that are assigned to a given topic.\n",
    "\n",
    "### $$ P(\\text{topic } z \\;|\\; \\text{document } d) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p(z|d): proportion of words in document d assigned to topic z\n",
    "def topic_given_doc(z_ind, d):\n",
    "    return np.sum([w_z[1] == z_ind for w_z in d])/float(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p(w|z): proportion of words in topic z that are this word\n",
    "def word_given_topic(w, z):\n",
    "    total_words = np.sum(list(z.values()))\n",
    "    if total_words == 0:\n",
    "        return 0.\n",
    "    else:\n",
    "        return z[w]/float(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20833333333333334"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_given_doc(3, D_z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_given_topic('wife', Z[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "### Step 4: reassignment of topics\n",
    "\n",
    "Now we will go through an iterative procedure. We will walk through the words in the documents many times. At each step we will calculate which topic a word should be reassigned to.\n",
    "\n",
    "First we neet to calculate the probability associated with each topic:\n",
    "\n",
    "### $$ P(\\text{topic } z \\;|\\; \\text{document } d) \\cdot P(\\text{word } w \\;|\\; \\text{topic } z) $$\n",
    "\n",
    "Which is the probability that the topic generated our current word. Below we can calculate the probability of a word in the first document being generated from each of our topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0 P: 0.0\n",
      "topic: 1 P: 0.0\n",
      "topic: 2 P: 0.0\n",
      "topic: 3 P: 0.0\n",
      "topic: 4 P: 0.00028825995807127885\n"
     ]
    }
   ],
   "source": [
    "for z_ind in range(K):\n",
    "    print('topic:', z_ind, 'P:', topic_given_doc(z_ind, D_z[0]) * word_given_topic('wife', Z[z_ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a function that will sample from the topics according to these probabilities to assign a new topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_topic(Z, document, word):\n",
    "    probs = []\n",
    "    for i in range(len(Z)):\n",
    "        probs.append(topic_given_doc(i, document) * word_given_topic(word, Z[i]))\n",
    "    if np.sum(probs) == 0:\n",
    "        return -1\n",
    "    return np.random.choice(range(len(Z)), p=np.array(probs)/np.sum(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_topic(Z, D_z[0], 'wife')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put it all together, we can write a a function that will iterate over all the words/documents a specified number of times and do the reassignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: SLOW!\n",
    "def topic_iterator(Z, D_z, iters=20):\n",
    "    for it in range(iters):\n",
    "        print('iter:', it)\n",
    "        for d_ind in range(len(D_z)):\n",
    "            for w_ind in range(len(D_z[d_ind])):\n",
    "                old_topic = D_z[d_ind][w_ind][1]\n",
    "                word = D_z[d_ind][w_ind][0]\n",
    "                new_topic = sample_topic(Z, D_z[d_ind], word)\n",
    "                if new_topic != -1:\n",
    "                    Z[old_topic][word] = max(0, Z[old_topic][word]-1)\n",
    "                    Z[new_topic][word] = Z[new_topic][word]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0\n",
      "iter: 1\n",
      "iter: 2\n",
      "iter: 3\n",
      "iter: 4\n",
      "iter: 5\n",
      "iter: 6\n",
      "iter: 7\n",
      "iter: 8\n",
      "iter: 9\n",
      "iter: 10\n",
      "iter: 11\n",
      "iter: 12\n",
      "iter: 13\n",
      "iter: 14\n",
      "iter: 15\n",
      "iter: 16\n",
      "iter: 17\n",
      "iter: 18\n",
      "iter: 19\n"
     ]
    }
   ],
   "source": [
    "# WARNING: SLOW!\n",
    "topic_iterator(Z, D_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "### Step 5: getting words that define topics and topics that define documents\n",
    "\n",
    "To get the topic distribution for a document we can use the function that calculated the probability of a topic given a document from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_dist(document, K=5):\n",
    "    for z in range(K):\n",
    "        print('topic:', z, 'P:', topic_given_doc(z, document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it out for the first 3 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document: 0\n",
      "topic: 0 P: 0.2361111111111111\n",
      "topic: 1 P: 0.1388888888888889\n",
      "topic: 2 P: 0.2638888888888889\n",
      "topic: 3 P: 0.20833333333333334\n",
      "topic: 4 P: 0.1527777777777778\n",
      "----------------------------------------\n",
      "document: 1\n",
      "topic: 0 P: 0.26\n",
      "topic: 1 P: 0.25\n",
      "topic: 2 P: 0.16\n",
      "topic: 3 P: 0.14\n",
      "topic: 4 P: 0.19\n",
      "----------------------------------------\n",
      "document: 2\n",
      "topic: 0 P: 0.22727272727272727\n",
      "topic: 1 P: 0.13636363636363635\n",
      "topic: 2 P: 0.09090909090909091\n",
      "topic: 3 P: 0.25\n",
      "topic: 4 P: 0.29545454545454547\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for d, doc in enumerate(D_z[0:3]):\n",
    "    print('document:', d)\n",
    "    topic_dist(doc)\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the words that define topics we can just calculate the words that have greatest probability of occuring for a given topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_top_words(z, top=5):\n",
    "    word_sum = np.sum(list(z.values()))\n",
    "    word_counts = sorted(z.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_words = [[w, c/float(word_sum)] for w, c in word_counts[0:top]]\n",
    "    for w, p in top_words:\n",
    "        print(w, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "n't 0.14451754385964913\n",
      "place 0.06491228070175438\n",
      "'s 0.06030701754385965\n",
      "like 0.04780701754385965\n",
      "really 0.03618421052631579\n",
      "----------------------------------------\n",
      "topic: 1\n",
      "'ve 0.06719563600297546\n",
      "food 0.051078601537317136\n",
      "try 0.03074634267294818\n",
      "staff 0.029506570790974462\n",
      "customer 0.026779072650632285\n",
      "----------------------------------------\n",
      "topic: 2\n",
      "good 0.06478149100257069\n",
      "just 0.04498714652956298\n",
      "sandwich 0.0390745501285347\n",
      "'s 0.03496143958868895\n",
      "'m 0.03470437017994859\n",
      "----------------------------------------\n",
      "topic: 3\n",
      "time 0.0672353236327145\n",
      "service 0.05845459106874059\n",
      "bad 0.05268439538384345\n",
      "way 0.039136979427997994\n",
      "people 0.037380832915203215\n",
      "----------------------------------------\n",
      "topic: 4\n",
      "best 0.058557486312782674\n",
      "chicken 0.04356105689121638\n",
      "pizza 0.03594382289930969\n",
      "said 0.03499166865032135\n",
      "did 0.03023089740537967\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for z_ind in range(K):\n",
    "    print('topic:', z_ind)\n",
    "    topic_top_words(Z[z_ind])\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='caveat'></a>\n",
    "### Major caveat: oversimplified!\n",
    "\n",
    "This is a dumb, oversimplified version of LDA. We've completely ignored priors and didn't talk about the Dirichlet distribution at all. That being said, this is the general breakdown of whats going on. We perform a procedure where we assign words to topics based on how likely a topic would have generated that word. \n",
    "\n",
    "> **Note:** More legitimate implementations of LDA have hyperparameters such as alpha. Alpha is a scaler that helps minimize an error term.  Thankfully, most LDA models that are implented will set this automatically and it's usually, 95% of the time a fine solution.  To really get a strong handle of the math behind this model, there are whitepapers you can read.  Also, having a strong handle on Bayesian statistic is a must to really grasp this model at it's lowest levels.  We are not going there today!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lda-intuition'></a>\n",
    "## LDA Intuition\n",
    "\n",
    "---\n",
    " \n",
    "As we iterate through each word in our corpus and (re)assign it to a topic:\n",
    "\n",
    "1. Words become more common in topics where they are already common.\n",
    "1. Topics will become more common in documents where they are already common.\n",
    "\n",
    "**Remember:**:\n",
    "- Words are assigned to topics randomly at first\n",
    "- As words are found to be consistently distributed within topics, the model achieves a sort of equilibrium based on the distribution of words accross all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lda-challenges'></a>\n",
    "## LDA challenges\n",
    "\n",
    "---\n",
    "\n",
    "1. **There's a bit of entropy to topics.**\n",
    "There can be between 1-10% shift in what is generated in LDA models.  You may not get the same thing 2x!\n",
    "\n",
    "1. **Can be very difficult to assess.**\n",
    "If you have a large corpus, with many topics (>10), it's damn near impossible to visualize the distribution of documents to topics.\n",
    "\n",
    "1. **Preprocessing is heavy.**\n",
    "To get the most out of LDA, cleaning stopwords, and specific language can be a challenging task.  Sometimes it's difficult to avoid the noise involved with this model.\n",
    "\n",
    "1. **SME is necessary for accurate topic assessment.**\n",
    "The more straight forward your text is, the less subject matter expertise is required.  A more advanced use of LDA would involve assessing documents with lots of idiomtic language. Knowing what topics are found, can be subjective.\n",
    "\n",
    "1.  **Determining what topics mean is tricky.**\n",
    "A collection of world probabiltiies generally isn't very intuitive.  You could take the first word and use that as your topic \"label\".  Hence, more subject matter expertise is required.\n",
    "\n",
    "1. **LDA is unsupervised.**\n",
    "It's not possible to know what is \"correct\".  The repsonse topics are generated. LDA is known as a \"generative\" model. \n",
    "\n",
    "1. **Tuning your LDA model can be tough.\"**\n",
    "It's possible to tune for the parameter **K** *number of topics*, but it's not necessarily a very straightforward way to improve your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lda-strengths'></a>\n",
    "## LDA strengths\n",
    "\n",
    "---\n",
    "\n",
    "1. **It can be very strong performer in production.**\n",
    "After you build the model, it can easily be used \"online\".\n",
    "> \"Online\" training allows you to update your model with more training data without having to refit all your data.  Only new data can be fit globally.\n",
    "\n",
    "1.  **It's easy to get a quick sense of what a large body of text is broadly \"about\", without having to read all of it.** Rather than reading 12k PDF's on corproate policies, you could extract the text, and run LDA to see what generalities it finds.\n",
    "1.  **Easily classify / tag documents by topic.**\n",
    "1.  **It can \"just work\" out of the box.**  However, your mileage will vary depending on your preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='other-models'></a>\n",
    "## Other models similar to LDA\n",
    "\n",
    "---\n",
    "\n",
    "- Topics Over Time\n",
    "- Dynamic Topic Modeling\n",
    "- Hierarchical LDA\n",
    "- Pachinko Allocation \n",
    "\n",
    "A cool new LDA model to look out for:\n",
    "\n",
    "- [LDA2Vec](http://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
