{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Preprocessing Libraries in Scikit-Learn\n",
    "\n",
    "_Authors: Richard Harris (CHI)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Get an overview of preprocessing modules in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Intro to Preprocessing Modules in Scikit-Learn](#intro-to-preprocessing-modules-in-sklearn)\n",
    "\t- [What's Happening With `fit`, `transform`, and `fit_transform`?](#whats-happening-with-fit-transform-and-fittransform)\n",
    "- [Different Modules in Scikit-Learn](#different-modules-in-sklearn)\n",
    "\t- [Binarizer](#binarizer)\n",
    "\t- [FunctionTransformer](#functiontransformer)\n",
    "\t- [Imputer](#imputer)\n",
    "\t- [LabelBinarizer](#labelbinarizer)\n",
    "\t- [PolynomialFeatures](#polynomialfeatures)\n",
    "\t- [Scalers](#scalers)\n",
    "- [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro-to-preprocessing-modules-in-sklearn\"></a>\n",
    "## Intro to Preprocessing Modules in Scikit-Learn\n",
    "\n",
    "As we've seen, creating custom transformers for every one of our columns can be a little time consuming. Luckily, scikit-learn provides a number of libraries that will preprocess your data and streamline the work. We've used a few of them as well.\n",
    "\n",
    "For modeling techniques in scikit-learn, all libraries come standard with the following methods:\n",
    "\n",
    "- `fit()`\n",
    "- `predict()`\n",
    "- `score()`\n",
    "\n",
    "Within the preprocessing libraries, we see another set of standard behavior:\n",
    "\n",
    "- `fit()`\n",
    "- `transform()`\n",
    "- `fit_transform()`\n",
    "\n",
    "This standard interface makes it easy to combine transformations within pipelines and let scikit-learn do much of our work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"whats-happening-with-fit-transform-and-fittransform\"></a>\n",
    "### What's Happening with `fit`, `transform`, and `fit_transform`?\n",
    "\n",
    "**fit()** will plan out the steps necessary for transformation but not apply them. For example, if we use `StandardScaler()` on a column _inside_ of our instantiated object, the following instructions are written:\n",
    "\n",
    "1) Here is the mean we will apply: *x*.\n",
    "2) Here is the standard deviation we will apply: *y*.\n",
    "3) When transform is called, take each value, subtract the mean we have stored, divide it by the standard deviation we have stored and return those transformed values.\n",
    "\n",
    "In other words, it lets us apply the same standards across multiple sets of data.\n",
    "\n",
    "**transform()** just carries out the steps that have been stored during fitting. \n",
    "\n",
    "**fit_transform()** accomplishes both of these steps in one go â€” it's what we do when we just want to transform a column right out of the gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"different-modules-in-sklearn\"></a>\n",
    "## Different Modules in Scikit-Learn\n",
    "\n",
    "We'll talk through each of the different libraries below with some sample code. Documentation and other details can be found in the scikit-learn API reference [here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing). Libraries are presented in alphabetical order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"binarizer\"></a>\n",
    "### Binarizer\n",
    "\n",
    "Binarizer codes data as '1' or '0' according to a particular threshold. This program will code your data '1' if the data point is above that threshold and '0' if the data point is below or equal to that threshold.\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "bin = Binarizer(20)\n",
    "print bin.fit_transform(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"functiontransformer\"></a>\n",
    "### FunctionTransformer\n",
    "\n",
    "This processor lets us take a function and apply it to the input as if it were a scikit-learn object. It can be helpful to look at this in terms of an example: \n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def get_first_column(numpy_array):\n",
    "    return numpy_array[:,0]\n",
    "\n",
    "ft = FunctionTransformer(get_first_column)\n",
    "ft.fit_transform(data)\n",
    "```\n",
    "\n",
    "This will let you reproducibly extract the first column out of any NumPy array to which you fit the transformer. This processor is useful for custom scalers, data extraction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"imputer\"></a>\n",
    "### Imputer\n",
    "\n",
    "This processor provides you with a scikit-learn implementation of `.fillna()` in Pandas but within scikit-learn. Your choices for imputation are the average value, median value, or most frequent value. If you want to add a more specific imputation, you may want to write a function and use `FunctionTransformer` instead.\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "impute = Imputer(strategy='median')\n",
    "impute.fit_transform(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"labelbinarizer\"></a>\n",
    "### LabelBinarizer\n",
    "\n",
    "LabelBinarizer transforms a set of classes into binary features (yes or no). You can think of this as working similar to `pd.get_dummies()`:\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "data = np.array([1, 3, 7, 2, 4])\n",
    "\n",
    "preprocessing.LabelBinarizer().fit_transform(data)\n",
    "\n",
    "array([[1, 0, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 0, 0, 1, 0]])\n",
    "\n",
    "```\n",
    "\n",
    "This creates a column for each potential value (in numeric order) and then encodes it using 0s and 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"polynomialfeatures\"></a>\n",
    "### PolynomialFeatures\n",
    "\n",
    "This preprocessor creates polynomial terms and, optionally, interaction terms from your input. By default it will also create a _bias_ term. The following example shows a case without the bias term:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "pf = PolynomialFeatures(3, include_bias=False)\n",
    "pf.fit_transform(data.reshape(-1, 1)) \n",
    "\n",
    "array([[   1.,    1.,    1.],\n",
    "       [   2.,    4.,    8.],\n",
    "       [   3.,    9.,   27.],\n",
    "       [   4.,   16.,   64.],\n",
    "       [   5.,   25.,  125.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scalers\"></a>\n",
    "### Scalers\n",
    "\n",
    "The following scikit-learn scalers all rescale data according to certain methods but are used the same way in each case:\n",
    "\n",
    "- **MinMaxScaler**: Scales the data using the max and min values so that they fit between 0 and 1.\n",
    "- **StandardScaler**: Scales the data so that they have a mean of 0 and a variance of 1.\n",
    "- **RobustScaler**: Scales the data similarly to StandardScaler but makes use of the median and scales, employing the interquartile range to avoid issues with large outliers.\n",
    "\n",
    "Other scalers exist, but these are the most commonly used and easiest to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "\n",
    "These tools can make it easy to perform common tasks as part of a reproducible data pipeline. Every transformation here can be reapplied to new data as they come in. It's not necessarily the most user-friendly system for iterating during model generation, but it can certainly come in handy when creating a _data pipeline_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
