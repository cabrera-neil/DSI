{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Sentiment Analysis and Naive Bayes\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "In the sentiment analysis lesson, we used a predefined dictionary of positive and negative valences for words. This  lab inverts that process: You'll find which words are most likely to appear in positive or negative reviews by using the rotten versus fresh binary label.\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "A common practical way to do this is with the Naive Bayes algorithm. Naive Bayes classifiers are covered in more depth in another lecture â€” for this lab, you'll just be leveraging the scikit-learn implementation.\n",
    "\n",
    "Given a feature, $x_i$, and target, $y_i$, Naive Bayes classifiers solve for $P(x_i \\;|\\; y_i)$. In other words, they solve for the probability of a feature/predictor _given_ that the target is one.\n",
    "\n",
    "We'll use this to figure out which words are more likely to appear when the target is one (\"fresh\") versus zero (\"rotten\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1) Load the packages and movie data.\n",
    "\n",
    "Perform any necessary cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# We are using the BernoulliNB version of Naive Bayes, which assumes that predictors are binary encoded.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt = pd.read_csv('./datasets/rt_critics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2) Create a predictor matrix of words in the quotes with `CountVectorizer`.\n",
    "\n",
    "It's up to you to select an n-gram range. **Make sure that `binary=True`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3) Split the data into training and testing sets.\n",
    "\n",
    "You should keep 25 percent of the data in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4) Build a `BernoulliNB` model predicting fresh versus rotten from the word occurrences.\n",
    "\n",
    "The model should only be built (and cross-validated) on the training data.\n",
    "\n",
    "Cross-validate the score and compare it to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5) Pull out the probability of words given \"fresh.\"\n",
    "\n",
    "The `.feature_log_prob_` attribute of the Naive Bayes model contains the log probabilities of a feature appearing given a target class.\n",
    "\n",
    "The rows correspond to the class of the target and the columns correspond to the features. The first row is the zero, \"rotten\" class and the second row is the one, \"fresh\" class.\n",
    "\n",
    "#### 5.A) Pull out the log probabilities and convert them to probabilities for fresh and rotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.B) Make a DataFrame with the probabilities and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.C) Create a column that is the difference between the probability of the appearance of fresh and rotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.D) Look at the most likely words for fresh and rotten reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6) Examine how your model performs on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7) Look at the top 10 movies and reviews likely to be fresh and the top 10 likely to be rotten.\n",
    "\n",
    "You can fit the model on the full set of data for this.\n",
    "\n",
    "> **Note:** While it's good at classifying, Naive Bayes is known to be somewhat bad at providing accurate predicted probabilities (beyond getting it on the correct side of 50 percent). It's a good classifier but a bad estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8) Find the movies with at least 10 reviews that are most likely to be fresh or rotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
