{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Conjugacy and the Beta-Binomial Model\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the concept of conjugacy and conjugate priors in Bayesian statistics.\n",
    "- Set up an example of the beta-binomial model using a subscription probability example.\n",
    "- Understand binomial likelihood and where it fits in Bayes’ theorem.\n",
    "- Calculate the maximum likelihood estimate (MLE).\n",
    "- Understand when and why the MLE point estimate is insufficient.\n",
    "- Use the beta-binomial model to build our example in a Bayesian framework.\n",
    "- Understand and visualize the beta distribution.\n",
    "- Learn about the beta and gamma functions and where they fit in the beta distribution calculation.\n",
    "- Mathematically derive the conjugacy relationship between the prior and posterior of the beta-binomial model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction](#intro)\n",
    "- [Review: The Binomial Distribution Probability Mass Function (PMF)](#pmf)\n",
    "- [Modeling the $p$ Parameter Given Counts of Successes and Failures](#p)\n",
    "- [The Binomial Likelihood](#likelihood)\n",
    "- [The Maximum Likelihood Estimate (MLE) for $p$](#mle)\n",
    "    - [The Likelihood Function](#likelihood-func)\n",
    "    - [When the MLE Doesn't Make Sense](#nonsense)\n",
    "- [Bayesian Modeling of the Parameter $p$ and the Beta Distribution](#beta)\n",
    "- [The Beta PDF and the Beta Function](#beta-pdf)\n",
    "- [The Gamma Function](#gamma)\n",
    "- [Defining the Beta Function in Terms of the Gamma Function](#beta-gamma)\n",
    "- [Putting It All Together: The Beta as a Conjugate Prior to the Binomial Likelihood](#beta-conjugate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "**Conjugacy** and conjugate priors are important concepts in Bayesian statistics. The essential idea is that the *posterior* distribution is guaranteed to have the same form as the *prior* distribution when the prior is a conjugate prior to the likelihood function.\n",
    "\n",
    "There are many conjugate priors and posteriors. They’re extremely useful because they make the prior-posterior update algebraically solvable. When there is no conjugate prior, sampling techniques such as Markov chain Monte Carlo are often necessary.\n",
    "\n",
    "This lecture covers the most classic conjugate prior scenario: the beta-binomial model. Binomial models are appropriate for binary events. The prior distribution on the probability of a binary event is a beta distribution, which is conjugate to the binomial likelihood. Therefore, we’re guaranteed to get a posterior distribution that is also a beta distribution.\n",
    "\n",
    "If none of this makes sense right now, don't worry! We’ll be walking through this in great detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pmf'></a>\n",
    "## Review: The Binomial Distribution Probability Mass Function\n",
    "---\n",
    "\n",
    "Recall that the number of success trials in $n$ trials is modeled with the binomial distribution. The binomial distribution has the probability mass function:\n",
    "\n",
    "### $$ P(k \\;|\\; n, p) = \\binom{n}{k} p^k (1 - p)^{(n-k)} $$\n",
    "\n",
    "$k$ is the number of successes.\n",
    "\n",
    "$n$ is the number of total trials.\n",
    "\n",
    "$p$ is the probability of success for each trial.\n",
    "\n",
    "**We can plot the probability mass function for a given $n$ and $p$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we change the probability of success, $p$ (or the total trials, $n$), we can see that the probability mass function changes — values of $k$ have different probabilities or likelihoods of occurring.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='p'></a>\n",
    "## Modeling the $p$ Parameter Given Counts of Successes and Failures\n",
    "---\n",
    "\n",
    "Let's reframe this. \n",
    "\n",
    "Say that we were measuring visitors to our site and whether or not they chose to subscribe to our newsletter. We redefine $n$, $k$, and $p$ accordingly:\n",
    "\n",
    "### $$ \\begin{aligned} n &= \\text{number of visitors to our website} \\\\\n",
    "k &= \\text{number of visitors who subscribed} \\\\\n",
    "p &= \\text{probability of a visitor subscribing (unknown)} \\end{aligned}$$\n",
    "\n",
    "Remember, now we’re _measuring_ $k$ subscribers out of the $n$ visitors. We can consider the measurement of subscribers to be our data.\n",
    "\n",
    "At this point, we want to make an inference about the $p$ parameter — our probability of a visitor subscribing. We can talk about this in terms of Bayes' theorem:\n",
    "\n",
    "### $$ P(p \\;|\\; data) = \\frac{ P(data \\;|\\; p) }{ P(data) } P(p) $$\n",
    "\n",
    "Or equivalently:\n",
    "\n",
    "### $$ P(p \\;|\\; n,k) = \\frac{ P(n,k \\;|\\; p) }{ P(n, k) } P(p) $$\n",
    "\n",
    "Where we have:\n",
    "\n",
    "### $$ \\begin{aligned} \n",
    "P(p \\;|\\; n,k) &= \\text{posterior} \\\\\n",
    "P(n,k \\;|\\; p) &= \\text{likelihood} \\\\\n",
    "P(n,k) &= \\text{marginal probability of the data} \\\\\n",
    "P(p) &= \\text{prior} \n",
    "\\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='likelihood'></a>\n",
    "## The Binomial Likelihood\n",
    "---\n",
    "\n",
    "Let's start with the likelihood. The likelihood represents the probability of observing $k$ successes out of $n$ trials _given a probability of success, $p$._\n",
    "\n",
    "This $p$ can be fixed — say at $p = 0.3$ — in which case we would evaluate the likelihood at exactly that point. We could also represent $p$ as a distribution over a range of possible $p$ values, evaluating the likelihood of what $p$ could be for all of our different hypotheses.\n",
    "\n",
    "Let's start with a fixed value, $p = 0.3$. How do we evaluate the likelihood? As it turns out, the likelihood function is the same as the PMF we wrote above, as this function is literally used to evaluate, \"What is the probability of $k$ successes given $n$ trials and $p$ probability of success?\" This is what we formulated as the likelihood in the numerator.\n",
    "\n",
    "**We can use the binomial object initialized with $p = 0.3$ and $n = 25$ to find the likelihood value for a given $k$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mle'></a>\n",
    "## The Maximum Likelihood Estimate (MLE) for $p$\n",
    "---\n",
    "\n",
    "If we were to _just_ focus on the likelihood part of Bayes' theorem, we could ask, “What is the value of the $p$ parameter that maximizes the value of the likelihood function?\" This is precisely what we do in frequentist statistics to find the point estimate of a parameter. \n",
    "\n",
    "Remember that frequentists have no interest in the prior or posterior beliefs about the probability of the parameter's value. Frequentists state that there is no probability associated with a parameter (such as our probability of subscription). Instead, there is one _true_ probability of subscription if we were to measure the entire population. \n",
    "\n",
    "Because we only take a sample of people, we may measure a probability of subscription that deviates from that true probability. Remember: In frequentist statistics, the data have a probability, rather than the parameter.\n",
    "\n",
    "**For the binomial distribution, we can easily calculate the value for subscription rate $p$ that makes our observed data the most likely: It’s going to be the fraction of successes that we measured in our data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='likelihood-func'></a>\n",
    "### The Likelihood Function\n",
    "\n",
    "We can also derive the MLE more formally. Our scenario here is simple, but for distributions and models that aren’t so simple, this becomes necessary.\n",
    "\n",
    "**First, define the likelihood function $L$ (which is the same as the PMF):**\n",
    "\n",
    "### $$ L(n, k \\;|\\; p) = \\binom{n}{k} p^k (1 - p)^{(n-k)} $$\n",
    "\n",
    "**Take the logarithm of this to get the log likelihood:**\n",
    "\n",
    "### $$ LL(n, k \\;|\\; p) = ln\\binom{n}{k} + k \\cdot ln(p) + (n - k) \\cdot ln(1 - p) $$\n",
    "\n",
    "The log likelihood has nice properties. It allows the computer to perform computations with very small probabilities multiplied together. It also gets rid of our exponents, which makes the derivative easier to find.\n",
    "\n",
    "**Now, take the derivative of the log likelihood with respect to $p$ and set it to zero.** This will find the value of $p$ that maximizes the log likelihood (the likelihood function is convex):\n",
    "\n",
    "### $$ \\begin{aligned}\n",
    "\\frac{\\partial}{\\partial p} LL(n, k \\;|\\; p) &= 0 \\\\\n",
    "\\frac{k}{p} - \\frac{(n-k)}{(1-p)} &= 0 \\\\\n",
    "\\frac{(n-k)}{(1-p)} &= \\frac{k}{p} \\\\\n",
    "pn - pk &= k - pk \\\\\n",
    "pn - pk + pk &= k \\\\\n",
    "p &= \\frac{k}{n} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "**This distills down to what we calculated before: The fraction of users who subscribed is the maximum likelihood estimate for the subscription rate.**\n",
    "\n",
    "<a id='nonsense'></a>\n",
    "### When the MLE Doesn’t Make Sense\n",
    "\n",
    "Now, say we had $n = 5$ visitors to the site and, to our surprise, all of them subscribed ($k = 5$). Using the MLE for $p$, we would conclude that $p = 1.0$: A person has a 100-percent probability of subscribing when they reach our site.\n",
    "\n",
    "This, of course, is a flawed conclusion. We’ve only measured five people! \n",
    "\n",
    "> **Note:** If we took the frequentist route, we would ask, “What is the probability that we measured this parameter $p = 1.0$ by chance when in fact the true rate is (some predetermined null hypothesis value) $H0_p = 0.3$?\" This would be our p value (a.k.a., alpha or type I error), and, with such insufficient data, we would almost certainly fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='beta'></a>\n",
    "\n",
    "## Bayesian Modeling of the Parameter $p$ and the Beta Distribution\n",
    "---\n",
    "\n",
    "What if we took a Bayesian rather than frequentist approach?\n",
    "\n",
    "Instead of thinking about the *data* as having a probability, we think of the *parameter*, $p$, as having a probability. In other words, different values of $p$ have different _likelihoods_. We will represent our beliefs about likely values of $p$ with our prior distribution.\n",
    "\n",
    "**The distribution that represents _a distribution of probabilities_ is the beta distribution. The beta distribution is parameterized by two values, $\\alpha$ and $\\beta$.**\n",
    "\n",
    "###  $$ Beta(\\alpha, \\beta) =\n",
    "\\begin{cases}\n",
    "\\alpha &= \\text{number of successes + 1} \\\\\n",
    "\\beta &= \\text{number of failures + 1}\n",
    "\\end{cases} $$\n",
    "\n",
    "**We can plot the beta distribution for the scenario in which we measured $k = 5$ out of $n = 5$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see from this distribution that our probability with the highest likelihood is one. But remember, other probabilities are also likely.**  Because of our low sample size, $n$, many values other than $p = 1.0$ have reasonable likelihoods.\n",
    "\n",
    "**What if we measured 20 subscriptions out of 20 visitors?** Plot this scenario below to see how the beta distribution changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='beta-pdf'></a>\n",
    "## The Beta PDF and the Beta Function\n",
    "---\n",
    "You’re probably wondering how the beta distribution is defined. Formally, we define the probability density function of the beta distribution as:\n",
    "\n",
    "### $$ PDF_{Beta}(x) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\int_0^1 u^{\\alpha-1} (1-u)^{\\beta-1}\\, du} $$\n",
    "\n",
    "Here, $x$ falls in the range `[0, 1]` and $u$ represents the values in that range over which we integrate.\n",
    "\n",
    "In the denominator, we are integrating over the possible probabilities. The denominator of the PDF is actually called the beta function, not to be confused with the beta _distribution_. \n",
    "\n",
    "If this looks familiar to the equation for the binomial likelihood above, that's because it is! In the numerator, we essentially have the binomial likelihood equation, only with the shape parameters $\\alpha$ and $\\beta$ instead of our $k$ and $n$. In the denominator, we're integrating the binomial likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gamma'></a>\n",
    "## The Gamma Function\n",
    "---\n",
    "\n",
    "There is another way to write the beta distribution using the gamma function. \n",
    "\n",
    "The gamma function is defined as:\n",
    "\n",
    "###  $$ \\Gamma(z) =\n",
    "\\begin{cases}\n",
    "(z - 1)! &= \\text{when z is a positive integer} \\\\\n",
    "\\int_0^{\\infty} x^{z-1} e^{-x} dx &= \\text{when z is a complex real number}\n",
    "\\end{cases} $$\n",
    "\n",
    "**We can plot out a gamma distribution below for $z = 10$ using SciPy's gamma object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='beta-gamma'></a>\n",
    "## Defining the Beta Function Using the Gamma Function\n",
    "---\n",
    "\n",
    "The gamma function is a generalization of the factorial function. The beta _function_ can also be written in terms of the gamma function:\n",
    "\n",
    "### $$ Beta(\\alpha, \\beta) = \\frac{ \\Gamma (\\alpha) \\Gamma (\\beta) }{\\Gamma (\\alpha + \\beta) } = \\int_0^1 u^{\\alpha-1} (1-u)^{\\beta-1}\\, du $$\n",
    "\n",
    "At this point, we can rewrite the beta _distribution_, or probability density function, like so:\n",
    "\n",
    "### $$ PDF_{Beta}(x) = \\frac{\\Gamma (\\alpha + \\beta) }{ \\Gamma (\\alpha) \\Gamma (\\beta) }x^{\\alpha-1}(1-x)^{\\beta-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='beta-conjugate'></a>\n",
    "## Putting It All Together: The Beta as a Conjugate Prior to the Binomial Likelihood\n",
    "---\n",
    "\n",
    "Remember, our beta distribution is what we’re going to be using as our _prior_ over the probability of subscription, $p$. In other words, we have some distribution of beliefs about which subscription rates are most likely represented by a beta distribution.\n",
    "\n",
    "**Recall the set up of this problem in terms of Bayes’ theorem:**\n",
    "\n",
    "### $$ P(p \\;|\\; n,k) = \\frac{ P(n,k \\;|\\; p) }{ P(k, n) } P(p) $$\n",
    "\n",
    "**For now, let's ignore the normalizing constant — the marginal probability of the data, $k,n$. We can say that the unnormalized posterior is:**\n",
    "\n",
    "### $$ P(p \\;|\\; n,k) \\propto P(n,k \\;|\\; p) \\cdot P(p) $$\n",
    "\n",
    "**And, we can put our binomial likelihood and the beta posterior in where we had the placeholders:**\n",
    "\n",
    "### $$ P(p \\;|\\; n,k) \\propto \\binom{n}{k} p^k (1 - p)^{(n-k)} \\cdot \\frac{\\Gamma (\\alpha + \\beta) }{ \\Gamma (\\alpha) \\Gamma (\\beta) }p^{\\alpha-1}(1-p)^{\\beta-1} $$\n",
    "\n",
    "**Let's now define a constant $c$ as:**\n",
    "\n",
    "### $$ c = \\binom{n}{k} \\cdot \\frac{\\Gamma (\\alpha + \\beta) }{ \\Gamma (\\alpha) \\Gamma (\\beta) } $$\n",
    "\n",
    "**Now, our formula for the unnormalized posterior is:**\n",
    "\n",
    "### $$ \\begin{aligned}\n",
    "P(p \\;|\\; n,k) &\\propto c \\cdot p^k (1 - p)^{(n-k)} \\cdot p^{\\alpha-1}(1-p)^{\\beta-1} \\\\\n",
    "P(p \\;|\\; n,k) &\\propto c \\cdot p^{(k + \\alpha - 1)} (1-p)^{(n - k + \\beta - 1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**If we define a new alpha and beta:**\n",
    "\n",
    "### $$ \\begin{aligned}\n",
    "\\alpha_{posterior} &= k + \\alpha_{prior} \\\\\n",
    "\\beta_{posterior} &= n - k + \\beta_{prior}\n",
    "\\end{aligned} $$\n",
    "\n",
    "**We can see that the posterior distribution can in fact be parameterized as a beta distribution.** The constant term $c$ will be handled when we put the marginal likelihood back in and normalize the posterior distribution to be a proper probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
