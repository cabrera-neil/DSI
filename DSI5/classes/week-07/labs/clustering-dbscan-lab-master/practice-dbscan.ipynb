{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# DBSCAN Practice\n",
    "\n",
    "_Authors: Joseph Nelson (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "Now that you're familiar with how DBSCAN works, let's practice it in scikit-learn.\n",
    "\n",
    "We'll start out working with the [NHL data](https://github.com/josephnelson93/GA-DSI/blob/master/NHL_Data_GA.csv). We're going to investigate clustering teams based on their counting stats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this glossary of hockey terms as a \n",
    "# reference guide for what the columns indicate:\n",
    "    \n",
    "    # Statistic \tDefinition\n",
    "# TOI \tTime On Ice\n",
    "# GF20 \tGoals For while player is on ice per 20 minutes of ice time.\n",
    "# GA20 \tGoals Against while player is on ice per 20 minutes of ice time.\n",
    "# GF% \tGF% = Goals For / (Goals For + Goals Against)\n",
    "# TMGF20 \tWeighted (by TOI together) average of all teammates GF20.\n",
    "# TMGA20 \tWeighted (by TOI together) average of all teammates GA20.\n",
    "# TMGF% \tWeighted (by TOI together) average of all teammates GF%.\n",
    "# OPPGF20 \tWeighted (by TOI against each other) average of all opponents GF20.\n",
    "# OPPGA20 \tWeighted (by TOI against each other) average of all opponents GA20.\n",
    "# OPPGF% \tWeighted (by TOI against each other) average of all opponents GF%.\n",
    "# HARO \tHockey Analysis Rating - Offense\n",
    "# HARD \tHockey Analysis Rating - Defense\n",
    "# HART \tHockey Analysis Rating - Total (average of HARO and HARD)\n",
    "# HARO+ \tHockey Analysis Rating - Offense - Enhanced (experimental)\n",
    "# HARD+ \tHockey Analysis Rating - Defense - Enhanced (Experimental)\n",
    "# HART+ \tHockey Analysis Rating - Total - Enhanced (Experimental, average of HARO+ and HARD+)\n",
    "# SF20 \tShots For while player is on ice per 20 minutes of ice time.\n",
    "# SA20 \tShots Against while player is on ice per 20 minutes of ice time.\n",
    "# SF% \tSF% = Shots For / (Shots For + Shots Against)\n",
    "# TMSF20 \tWeighted (by TOI together) average of all teammates SF20.\n",
    "# TMSA20 \tWeighted (by TOI together) average of all teammates SA20.\n",
    "# TMSF% \tWeighted (by TOI together) average of all teammates SF%.\n",
    "# OppSF20 \tWeighted (by TOI against each other) average of all opponents SF20.\n",
    "# OppSA20 \tWeighted (by TOI against each other) average of all opponents SA20.\n",
    "# OppSFor% \tWeighted (by TOI against each other) average of all opponents SF%.\n",
    "# ShotHARO \tHockey Analysis Shot Rating - Offense\n",
    "# ShotHARD \tHockey Analysis Shot Rating - Defense\n",
    "# ShotHART \tHockey Analysis Shot Rating - Total (average of HARO and HARD)\n",
    "# ShotHARO+ \tHockey Analysis Shot Rating - Offense - Enhanced (experimental)\n",
    "# ShotHARD+ \tHockey Analysis Shot Rating - Defense - Enhanced (Experimental)\n",
    "# ShotHART+ \tHockey Analysis Shot Rating - Total - Enhanced (Experimental, average of HARO+ and HARD+)\n",
    "# CorF20 \tCorsi For while player is on ice per 20 minutes of ice time.\n",
    "# CorA20 \tCorsi Against while player is on ice per 20 minutes of ice time.\n",
    "# CorF% \tCorF% = Corsi For / (Corsi For + Corsi Against)\n",
    "# TMCorF20 \tWeighted (by TOI together) average of all teammates CorF20.\n",
    "# TMCorA20 \tWeighted (by TOI together) average of all teammates CorA20.\n",
    "# TMCorF% \tWeighted (by TOI together) average of all teammates CorF%.\n",
    "# OppCorF20 \tWeighted (by TOI against each other) average of all opponents CorF20.\n",
    "# OppCorA20 \tWeighted (by TOI against each other) average of all opponents CorA20.\n",
    "# OppCorF% \tWeighted (by TOI against each other) average of all opponents CorF%.\n",
    "# CorHARO \tHockey Analysis Corsi Rating - Offense\n",
    "# CorHARD \tHockey Analysis Corsi Rating - Defense\n",
    "# CorHART \tHockey Analysis Corsi Rating - Total (average of HARO and HARD)\n",
    "# CorHARO+ \tHockey Analysis Corsi Rating - Offense - Enhanced (experimental)\n",
    "# CorHARD+ \tHockey Analysis Corsi Rating - Defense - Enhanced (Experimental)\n",
    "# CorHART+ \tHockey Analysis Corsi Rating - Total - Enhanced (Experimental, average of HARO+ and HARD+)\n",
    "# FenF20 \tFenwick For while player is on ice per 20 minutes of ice time.\n",
    "# FenA20 \tFenwick Against while player is on ice per 20 minutes of ice time.\n",
    "# FenF% \tFenF% = Fenwick For / (Fenwick For + Fenwick Against)\n",
    "# TMFenF20 \tWeighted (by TOI together) average of all teammates FenF20.\n",
    "# TMFenA20 \tWeighted (by TOI together) average of all teammates FenA20.\n",
    "# TMFenF% \tWeighted (by TOI together) average of all teammates FenF%.\n",
    "# OppFenF20 \tWeighted (by TOI against each other) average of all opponents FenF20.\n",
    "# OppFenA20 \tWeighted (by TOI against each other) average of all opponents FenA20.\n",
    "# OppFenF% \tWeighted (by TOI against each other) average of all opponents FenF%.\n",
    "# FenHARO \tHockey Analysis Fenwick Rating - Offense\n",
    "# FenHARD \tHockey Analysis Fenwick Rating - Defense\n",
    "# FenHART \tHockey Analysis Fenwick Rating - Total (average of HARO and HARD)\n",
    "# FenHARO+ \tHockey Analysis Fenwick Rating - Offense - Enhanced (experimental)\n",
    "# FenHARD+ \tHockey Analysis Fenwick Rating - Defense - Enhanced (Experimental)\n",
    "# FenHART+ \tHockey Analysis Fenwick Rating - Total - Enhanced (Experimental, average of HARO+ and HARD+)\n",
    "# Corsi \tCorsi = Shots + Missed Shots + Blocked Shots\n",
    "# Fenwick \tFenwick = Shots + Missed Shots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load our data and perform any basic cleaning and/or exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nhl = pd.read_csv('./datasets/nhl.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Set up an `X` matrix to perform clustering with DBSCAN.\n",
    "\n",
    "Let's cluster on all features except `team` and `rank`.\n",
    "\n",
    "Make `rank` our `y` vector, which we can use to perform cluster validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Scatterplot EDA.\n",
    "\n",
    "Create two scatterplots. At least one axis in one of the plots should represent points (goals for and goals against). Do the scatterplots give us a general idea of how many clusters we should expect to extract with a clustering algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Scale our data.\n",
    "\n",
    "Standardize the data and compare at least one of the scatterplots for the scaled data to the unscaled data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Fit a DBSCAN clusterer.\n",
    "\n",
    "Remember to pass an `eps` and `min_samples` of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Check out the assigned cluster labels.\n",
    "\n",
    "Use the `.labels_` command on our DBSCAN class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Evaluate the DBSCAN clusters.\n",
    "\n",
    "**7.A) Check the silhouette score.**\n",
    "\n",
    "How are the clusters?\n",
    "\n",
    "If you're up for a challenge, see how you can adjust our `eps` and `min_points` to improve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.B) Check the homogeneity, completeness, and v measure against the stored rank `y`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Plot the clusters.\n",
    "\n",
    "You can choose any two variables for the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Fit DBSCAN on an easier data set.\n",
    "\n",
    "Import the `make_circles()` function from `sklearn.datasets`. You can use this to create some fake clusters that will perform well with DBSCAN.\n",
    "\n",
    "Create some `X` and `y` using the function. Here is some sample code:\n",
    "```python\n",
    "from sklearn.datasets import make_circles\n",
    "circles_X, circles_y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n",
    "```\n",
    "\n",
    "**9.A) Plot the fake circles data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.B) Scale the data and fit DBSCAN on it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.C) Evaluate DBSCAN visually with silhouette and the metrics against the true `y`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
