{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Sentiment Analysis of Movie Reviews With SpaCy and VADER\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the goal of basic sentiment analysis.\n",
    "- Calculate sentiment scores manually using a movie reviews data set and scores tagged by word.\n",
    "- Practice using the spaCy parser to distill part-of-speech tags from text.\n",
    "- Fit a model using sentiment and grammar features.\n",
    "- Use the VADER sentiment analyzer to get more accurate sentiment scores and compare the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction to Sentiment Analysis](#intro)\n",
    "- [Load the Word Sentiment Data Set](#load-sen)\n",
    "    - [Engineer Objectivity and Positive Difference Scores](#adj-scores)\n",
    "    - [Put Scores in a Parts-of-Speech Dictionary](#pos-dict)\n",
    "- [Load the Rotten Tomatoes Review Data Set](#rt-reviews)\n",
    "    - [Restrict Reviews to Valid Lengths and Ratings](#subset)\n",
    "- [Import SpaCy](#spacy)\n",
    "    - [Parse All of the Quotes Using SpaCy's Multithreaded Parser](#multi)\n",
    "- [Parts-of-Speech Features](#pos-features)\n",
    "- [Assign Sentiment Scores](#assign)\n",
    "- [Print Out the Most Positive and Negative Reviews](#print-most)\n",
    "- [Print Out the Most Objective and Subjective Reviews](#print-most-obj)\n",
    "- [Build a Model to Classify Fresh vs. Rotten With the Sentiment and Grammar Features](#model)\n",
    "- [Use the VADER Library to Get Better Sentiment Scores](#vader)\n",
    "    - [Build a Model Using the VADER Sentiment Features](#vader-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction to Sentiment Analysis\n",
    "---\n",
    "\n",
    "Sentiment analysis is one of the most popular topics in NLP. Most commonly, it's the quantification of text into valence and subjectivity scores.\n",
    "\n",
    "First, we'll load in a data set of precoded sentiment scores for positivity and negativity of words. These words are also tagged with their part of speech in the sentence. We can use these valence scores to evaluate the sentiment of Rotten Tomatoes movie reviews. Many packages, such as TextBlob, come prepackaged with sentiment scores for words after parsing text. But doing the sentiment parsing manually will show you how it can be done without any \"magic.\"\n",
    "\n",
    "We'll also explore a more advanced sentiment analysis library in Python: [VADER](https://github.com/cjhutto/vaderSentiment). We can parse the sentiment of the movie reviews using this package and compare it to our more basic method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load-sen'></a>\n",
    "\n",
    "## Load the Word Sentiment Data Set\n",
    "---\n",
    "\n",
    "Below, we’ll load in some pre-tagged positive and negative valence scores for a dictionary of words. Each row of the data set contains the part of speech, the word, and the positive and negative scores for the word. A word may appear more than once if it can be classified with different parts-of-speech tags. \n",
    "\n",
    "These scores are designed so we can also derive the *objectivity score* of the word from the positive and negative scores.\n",
    "\n",
    "Objectivity is calculated as: \n",
    "\n",
    "    1. - (positive_score + negative_score)\n",
    "\n",
    "If a score has a zero positive and negative score, it’s completely objective. If a score has, for example, a 0.5 positive and 0.5 negative score, it may not be any more positive than negative, but we can tell that it's subjective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen = pd.read_csv('./datasets/sentiment_words_simple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "\n",
    "sen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make the parts-of-speech tags uppercase (this will come in handy when we use spaCy).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='adj-scores'></a>\n",
    "\n",
    "### Engineer Objectivity and Positive Difference Scores\n",
    "\n",
    "Because subjective versus objective is embedded in the positive and negative scores, we should extract this and convert the scores into relative difference scores.\n",
    "\n",
    "**Calculate two new scores:**\n",
    "\n",
    "    objectivity = 1. - (pos_score + neg_score)\n",
    "    pos_vs_neg = pos_score - neg_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>objectivity</th>\n",
       "      <th>pos_vs_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>.22-caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>.22-calibre</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>.22_caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>.22_calibre</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>.38-caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos         word  pos_score  neg_score  objectivity  pos_vs_neg\n",
       "0  ADJ  .22-caliber        0.0        0.0          1.0         0.0\n",
       "1  ADJ  .22-calibre        0.0        0.0          1.0         0.0\n",
       "2  ADJ  .22_caliber        0.0        0.0          1.0         0.0\n",
       "3  ADJ  .22_calibre        0.0        0.0          1.0         0.0\n",
       "4  ADJ  .38-caliber        0.0        0.0          1.0         0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pos-dict'></a>\n",
    "\n",
    "### Put Scores in a Parts-of-Speech Dictionary\n",
    "\n",
    "The dictionary format of the data will be much easier to index using our parsing functions later on. Create a dictionary where the keys are the four parts-of-speech tags:\n",
    "\n",
    "    ADJ\n",
    "    NOUN\n",
    "    VERB\n",
    "    ADV\n",
    "\n",
    "For each key, store a dictionary that contains all of the words for that part of speech with their objectivity and positive versus negative scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rt-reviews'></a>\n",
    "\n",
    "## Load the Rotten Tomatoes Reviews Data Set\n",
    "\n",
    "---\n",
    "\n",
    "This data set has:\n",
    "    \n",
    "    critic: Critic's name.\n",
    "    fresh: Fresh vs. rotten rating.\n",
    "    imdb: Code for IMDB.\n",
    "    publication: Where the review was published.\n",
    "    quote: The review snippet.\n",
    "    review_date: Date of review.\n",
    "    rtid: Rotten Tomatoes ID.\n",
    "    title: Name of movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt = pd.read_csv('./datasets/rt_critics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='subset'></a>\n",
    "\n",
    "### Restrict Data to Reviews With Valid Ratings and Reviews More Than 10 Words Long\n",
    "\n",
    "Additionally, clean up the reviews, making a column with the case and punctuation removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='spacy'></a>\n",
    "\n",
    "## Import SpaCy\n",
    "\n",
    "---\n",
    "\n",
    "The spaCy package is the current gold standard for parsing the grammatical structure of text (aside from neural network architectures). We're going to use it to find the parts-of-speech tags for the review words. \n",
    "\n",
    "Once we’ve parsed the tags with spaCy, we can assign objectivity and valence scores by finding the match in our sentiment data set.\n",
    "\n",
    "The code for loading spaCy is:\n",
    "\n",
    "- ```pip install spacy```\n",
    "- ```Python -m spacy download en```  \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** For OSx**  \n",
    "If you have problems with \"unknown locale UTF\" (or similar), open the file ~.bash_profile (e.g. with ```nano ~.bash_profile``` from terminal) and add these two lines:\n",
    "\n",
    "\n",
    "```export LC_ALL=en_US.UTF-8```  \n",
    "```export LANG=en_US.UTF-8```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OSx only, you have the problem above run also this:\n",
    "!export LC_ALL=en_US.UTF-8\n",
    "!export LANG=en_US.UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse a single quote.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "tmp = en_nlp(rt.qt.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the parts-of-speech tags for each word in the quote.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multi'></a>\n",
    "### Parse All of the Quotes Using SpaCy's Multithreaded Parser\n",
    "\n",
    "Parsing a lot of text can take awhile. Luckily, spaCy comes with multithreading functionality to speed up the process. Below is code that will parse the quotes across multiple threads and assign them to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_quotes = []\n",
    "for i, parsed in enumerate(en_nlp.pipe(rt.qt.values, batch_size=50, n_threads=4)):\n",
    "    assert parsed.is_parsed\n",
    "    if (i % 1000) == 0:\n",
    "        print i\n",
    "    parsed_quotes.append(parsed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pos-features'></a>\n",
    "\n",
    "## Create Features With Parts-of-Speech Proportions\n",
    "\n",
    "---\n",
    "\n",
    "With our spaCy-parsed reviews, we have a lot of feature engineering potential before we even get to sentiment. Something simple we could do is calculate the proportion of words in the quote that have each part-of-speech tag. We can try using these as predictors later.\n",
    "\n",
    "**Find all of the unique part-of-speech categories in the reviews.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the proportion columns for each part of speech.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterate through the reviews and calculate the proportions of each part-of-speech tag.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assign'></a>\n",
    "\n",
    "## Assign Sentiment Scores\n",
    "---\n",
    "\n",
    "We'll now use the parsed reviews and the sentiment data set to assign the average objectivity and positive versus negative scores.\n",
    "\n",
    "If a word can't be found in the data set, we can ignore it. If a review has no words that match something in our data set, we can assign overall neutral scores of `objectivity = 1` and `pos_vs_neg = 0`.\n",
    "\n",
    "There are problems with this approach, but for now we can keep it simple and see if things improve when we use the VADER analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='print-most'></a>\n",
    "## Print Out the Most Positive and Negative Reviews\n",
    "---\n",
    "\n",
    "Now that we have the average valence for reviews, try printing out the top 10 most positive and top 10 most negative reviews to visually verify that our approach makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='print-most-obj'></a>\n",
    "\n",
    "## Print Out the Most Objective and Subjective Reviews\n",
    "---\n",
    "\n",
    "Complete the same task as above but sort by objectivity. What kind of differences do you notice between the two? Does our approach appear to capture meaningful subjectivity and objectivity in the reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model'></a>\n",
    "\n",
    "## Build a Model to Classify Fresh vs. Rotten With the Sentiment and Grammar Features\n",
    "\n",
    "---\n",
    "\n",
    "Let's use the features we've created to construct a logistic regression that will predict whether a review is fresh or rotten. \n",
    "\n",
    "Don't forget to check the baseline score and remember that it's a good practice to standardize your predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vader'></a>\n",
    "\n",
    "## Use the VADER Library to Get Better Sentiment Scores\n",
    "---\n",
    "\n",
    "The [VADER](https://github.com/cjhutto/vaderSentiment) package for Python is a more advanced way to calculate positivity, negativity, and objectivity in our reviews. Its GitHub page describes VADER as:\n",
    "\n",
    "> VADER (valence aware dictionary and sentiment reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains.\n",
    "\n",
    "You will likely need to install VADER with `pip` or `conda`. Instructions can be found on the GitHub page. Once it is installed, you can load the `SentimentIntensityAnalyzer` and parse text.\n",
    "\n",
    "**Parse a couple of quotes with the `SentimentIntensityAnalyzer` and print out the dictionary of scores using `analyzer.polarity_scores`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pip install vaderSentiment.\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores look more legitimate. VADER polarity score dictionaries have four elements: `neg`, `pos`, `neu`, and `compound`. The compound score is a single metric that represents the overall valence.\n",
    "\n",
    "**Calculate the four scores for each review and save them as features in the DataFrame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vader-model'></a>\n",
    "\n",
    "### Fit a Model Using the VADER Sentiment Features\n",
    "\n",
    "Does this model perform better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vader-top'></a>\n",
    "\n",
    "### Print Out the Top Most Negative, Positive, Neutral, and Subjective Features by VADER Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
