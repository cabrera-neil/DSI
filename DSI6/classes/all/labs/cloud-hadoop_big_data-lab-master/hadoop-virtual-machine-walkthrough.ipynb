{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Hadoop Lab\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "### Learning Objectives\n",
    "*By the end of this lab, you will be able to:*\n",
    "- Install a local virtual machine running Apache Hadoop.\n",
    "- Navigate the Hadoop-distributed file system (HDFS).\n",
    "\n",
    "### Pre-Work\n",
    "*Before this lesson, you will need to:*\n",
    "- Review the installation of the virtual machine.\n",
    "- Download the virtual machine image before the lab (see instructions below).\n",
    "- Note: Data assets for this lesson are included in the virtual machine.\n",
    "\n",
    "> Instructors: This week requires some additional preparation. *First*, you'll need to assist students with the use of the virtual machine to run the first few lessons and labs. *Second*, you'll need to check with your local team about buying or accessing AWS credits for your students to use EC2 and EMR services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction](#intro)\n",
    "- [Installing the Virtual Machine (VM)](#vm)\n",
    "    - [Import the VM in VirtualBox](#import-vm)\n",
    "- [Launch the VM](#launch)\n",
    "- [Start the Big Data Tools](#start)\n",
    "- [Hadoop](#hadoop)\n",
    "- [YARN](#yarn)\n",
    "- [Exploring HDFS From the Command Line](#guided-practice)\n",
    "    - [Exercise 1](#ex1)\n",
    "- [Exploring HDFS From the Web Interface](#guided-practice2)\n",
    "    - [Exercise 2](#ex2)\n",
    "- [Hadoop Word Count](#guided-practice3)\n",
    "    - [Exercise 3](#ex3)\n",
    "- [Hadoop Streaming Word Count](#guided-practice4)\n",
    "    - [Exercise 4](#ex4)\n",
    "- [Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "In this lab, we will explore Hadoop, a common implementation of the MapReduce framework. We'll do this using a virtual machine (i.e., a simulated computer running on a host computer (our laptops)).\n",
    "\n",
    "This lab will guide you through the installation and configuration of the virtual environment. The environment includes a virtual machine that runs on your computer and comes packaged with useful software, including:\n",
    "\n",
    "- Hadoop.\n",
    "- Hive.\n",
    "- Hue.\n",
    "- Spark.\n",
    "- Python with many useful packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"vm\"></a>\n",
    "## Installing the Virtual Machine\n",
    "---\n",
    "\n",
    "The first step in our journey is to start a local virtual machine that we'll use throughout the week.\n",
    "\n",
    "In order to simplify the process, we've made this machine available as a VirtualBox file at [this Dropbox location](https://www.dropbox.com/sh/ktjhecqklpvwcce/AADZBLKS6KQJL3hUt10eQiqSa?dl=0). \n",
    "\n",
    "From now on, we'll assume that you've already installed [VirtualBox](https://www.virtualbox.org/) on your computer. If you haven't, please install it immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='import-vm'></a>\n",
    "### Import the VM in VirtualBox\n",
    "\n",
    "Oracle VM VirtualBox is a free, open-source hypervisor for x86 computers from Oracle Corporation. It was initially developed by innotek GmbH, which was acquired by Sun Microsystems in 2008 (which was in turn acquired by Oracle in 2010).\n",
    "\n",
    "VirtualBox may be installed on a number of host operating systems, including Linux, OS X, Windows, Solaris, and OpenSolaris. It supports the creation and management of guest virtual machines running versions and derivations of Windows, Linux, BSD, OS/2, Solaris, Haiku, OSx86, and others.\n",
    "\n",
    "For some guest operating systems, a \"Guest Additions\" package of device drivers and system applications is available, which typically improves performance — especially of graphics.\n",
    "\n",
    "Once you've downloaded it, import it in VirtualBox.\n",
    "\n",
    "![](./assets/images/virtualbox.png)\n",
    "\n",
    "![](./assets/images/import.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='launch'></a>\n",
    "## Launch the VM\n",
    "---\n",
    "\n",
    "The VM is launched by pressing the green launch arrow. This will open a terminal window where you’ll see a lot of text. Finally, you'll be prompted to log in. Do not log in here. Instead, connect via `ssh` from a terminal window by typing:\n",
    "    \n",
    "    ssh vagrant@10.211.55.101\n",
    "    password: vagrant\n",
    "\n",
    "![](./assets/images/launch.png)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='start'></a>\n",
    "## Start the Big Data Tools\n",
    "---\n",
    "\n",
    "Once you're logged in, type:\n",
    "\n",
    "    $ bigdata_start.sh\n",
    "\n",
    "This will start the following services:\n",
    "\n",
    "- Hadoop.\n",
    "- HDFS.\n",
    "- YARN.\n",
    "- Hive server.\n",
    "- Hue.\n",
    "- Jupyter Notebook.\n",
    "\n",
    "You may be asked for the password, \"vagrant,\" a few times — just type it in.\n",
    "\n",
    "Let's have a look at some of the services available in this virtual machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hadoop'></a>\n",
    "## Hadoop\n",
    "\n",
    "---\n",
    "\n",
    "Apache Hadoop is an open-source software framework for the distributed storage and processing of very large data sets on computer clusters built from commodity hardware. All the modules in Hadoop are designed with the fundamental assumption that hardware failures are common and should be automatically handled by the framework.\n",
    "\n",
    "The core of Apache Hadoop consists of a storage part, known as the **Hadoop-distributed file system (HDFS)**, and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster.\n",
    "\n",
    "### HDFS\n",
    "\n",
    "The HDFS is a distributed, scalable, and portable file system written in Java to support the Hadoop framework. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yarn'></a>\n",
    "## YARN\n",
    "---\n",
    "\n",
    "YARN is a resource-management platform responsible for managing computing resources in clusters and using them to schedule users' applications. The fundamental idea of YARN is to split the functionalities of resource management and job scheduling/monitoring into separate daemons. It's goal is to have a global resource manager (RM) and per-application application master (AM).\n",
    "\n",
    "The resource manager and node manager form the data computation framework. The resource manager is the ultimate authority that arbitrates resources among all the applications in the system. The node manager is the per-machine framework agent that's responsible for containers, monitoring resource usage (CPU, memory, disk, network), and reporting the results to the resource manager/scheduler.\n",
    "\n",
    "The YARN resource manager offers a web interface that is accessible on our VM at this address:\n",
    "\n",
    "http://10.211.55.101:8088/cluster\n",
    "\n",
    "Type that in your browser and you should see a screen like this:\n",
    "\n",
    "![](./assets/images/yarn.png)\n",
    "\n",
    "This will be useful for checking the advancement status when we run a Hadoop job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Exploring HDFS From the Command Line\n",
    "---\n",
    "\n",
    "Hadoop offers a command line interface for navigating the HDFS. The full documentation can be found here:\n",
    "\n",
    "http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "We've pre-loaded our machine with a few data sets. Let's explore them by typing this command:\n",
    "\n",
    "    $ hadoop fs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex1'></a>\n",
    "### Exercise 1\n",
    "Explore HDFS and describe the content of each folder it contains. You'll need to use a combination of commands, such as:\n",
    "\n",
    "    - ls\n",
    "    - cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice2\"></a>\n",
    "## Exploring HDFS From the Web Interface\n",
    "---\n",
    "\n",
    "Hadoop also offers a web interface for navigating and managing HDFS. It can be found at this address:\n",
    "\n",
    "http://10.211.55.101:50070\n",
    "\n",
    "It looks like this:\n",
    "\n",
    "![](./assets/images/hdfsweb.png)\n",
    "\n",
    "<a id='ex2'></a>\n",
    "### Exercise 2\n",
    "Find out how you can navigate the HDFS from the web interface. Is the content listed similar to what you were finding with the command line?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer: No. The web interface displays the content of the root folder, \n",
    "# while the Hadoop fs command automatically goes into the /users/hadoop folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice3\"></a>\n",
    "## Hadoop Word Count\n",
    "\n",
    "Let's create a short file and get its word count using Hadoop:\n",
    "\n",
    "    $ hadoop fs -mkdir wordcount-input\n",
    "    \n",
    "    $ echo \"hello dear world hello\" | hadoop fs -put - wordcount-input/hello.txt\n",
    "\n",
    "<a id='ex3'></a>\n",
    "### Exercise 3:\n",
    "\n",
    "Run the word count with the following command:\n",
    "\n",
    "    $ hadoop jar /usr/local/lib/hadoop-2.7.2/share/hadoop/mapreduce/hadoop*example*.jar \\\n",
    "                  wordcount wordcount-input wordcount-output\n",
    "\n",
    "\n",
    "![](./assets/images/hdwcshell.png)\n",
    "\n",
    "![](./assets/images/hdwcyarn.png)\n",
    "\n",
    "Check the results by typing:\n",
    "\n",
    "    $ hadoop fs -cat wordcount-output/part*\n",
    "    \n",
    "You should see:\n",
    "\n",
    "    dear   1\n",
    "    hello  2\n",
    "    world  1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice4\"></a>\n",
    "## Hadoop Streaming Word Count\n",
    "---\n",
    "\n",
    "Hadoop also offers a streaming interface. This streaming interface will process the data as a stream, one piece at a time, but it must to be told what to do with each piece of data. This is somewhat similar to what we did with MapReduce from the shell in a previous class. \n",
    "\n",
    "Let's use the same Python scripts to run a Hadoop streaming MapReduce. We have pre-copied those scripts to your VM home folder so they're easy to access.\n",
    "\n",
    "First, let's copy some data to the HDFS. The data folder contains a folder called `project_gutenberg`. Let's copy that to Hadoop:\n",
    "\n",
    "    $ hadoop fs -copyFromLocal data/project_gutenberg project_gutenberg\n",
    "    $ hadoop fs -copyFromLocal scripts scripts\n",
    "\n",
    "Go ahead and check that it's there:\n",
    "\n",
    "http://10.211.55.101:50070/explorer.html#/user/vagrant\n",
    "\n",
    "Great! Now, we should pipe all the data contained in that folder through our scripts with Hadoop streaming.\n",
    "Let's make sure that the scripts work by using the shell pipes we learned in the last lecture.\n",
    "\n",
    "    $ cat data/project_gutenberg/pg84.txt | python scripts/mapper.py | sort -k1,1 | python scripts/reducer.py \n",
    "\n",
    "Great! They still work. Now, let's do Hadoop streaming MapReduce:\n",
    "\n",
    "    $ export STREAMING_JAR=/usr/local/lib/hadoop-2.7.2/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\n",
    "    \n",
    "    $ hadoop jar $STREAMING_JAR  \\\n",
    "      -file /home/vagrant/scripts/mapper.py   \\\n",
    "      -mapper /home/vagrant/scripts/mapper.py \\\n",
    "      -file /home/vagrant/scripts/reducer.py  \\\n",
    "      -reducer /home/vagrant/scripts/reducer.py \\\n",
    "      -input /user/vagrant/project_gutenberg/* \\\n",
    "      -output /user/vagrant/output_gutenberg\n",
    "\n",
    "\n",
    "Check the status of your MapReduce job here:\n",
    "\n",
    "http://10.211.55.101:8088/cluster/apps\n",
    "\n",
    "Check your results in the HDFS explorer:\n",
    "\n",
    "http://10.211.55.101:50070/explorer.html#/user/vagrant/output_gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex4'></a>\n",
    "### Exercise 4\n",
    "\n",
    "Congratulations! You've learned how to use a local virtual machine running Hadoop and how to submit MapReduce job flows to it.\n",
    "\n",
    "Now, perform the MapReduce word count on the Project Gutenberg data using the Hadoop jar from the last exercise. You should get the list words with the counts as output. You can also save that list to a file and open it in Pandas to sort the words by frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "## Additional Resources\n",
    "---\n",
    "\n",
    "- [Hadoop](http://hadoop.apache.org/)\n",
    "- [Hadoop Command Line](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html)\n",
    "- [YARN](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)\n",
    "- [Hadoop Streaming Tutorial](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/)\n",
    "- [Hadoop Streaming Document](https://hadoop.apache.org/docs/r1.2.1/streaming.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
